var documenterSearchIndex = {"docs":
[{"location":"theta/#Theta-Method-(STM,-OTM,-DSTM,-DOTM)","page":"Theta","title":"Theta Method (STM, OTM, DSTM, DOTM)","text":"tip: Formula Interface is the Recommended Approach\nUse ThetaSpec with @formula for a declarative interface that works with panel data, grouped fitting, and model comparison. The base (array) API is shown near the end.\n\nThe Theta method decomposes a series into theta lines (one capturing long-run trend, one capturing short-run curvature) and recombines their forecasts. Durbyn implements the four variants discussed by Fiorucci et al. (2016):\n\n:STM - Simple Theta (theta = 2 fixed, alpha optimized)\n:OTM - Optimized Theta (theta and alpha optimized)\n:DSTM - Dynamic Simple Theta (theta = 2, regression coefficients updated each step)\n:DOTM - Dynamic Optimized Theta (dynamic regression + optimized theta, alpha)\n\nSeasonality is handled with additive or multiplicative decomposition prior to fitting (auto-detected unless specified).","category":"section"},{"location":"theta/#Variant-Selection-Guide","page":"Theta","title":"Variant Selection Guide","text":"Variant Best for Trade-off\nSTM Quick baseline forecasts Fast but less flexible (fixed θ=2)\nOTM Series where θ≠2 improves fit Better accuracy, slightly slower\nDSTM Non-stationary trend patterns Adapts to changing trends\nDOTM Complex series with evolving dynamics Most flexible, best for longer series\n\nUse theta() with no model argument to let Durbyn auto-select the best variant via in-sample MSE.\n\n","category":"section"},{"location":"theta/#Formula-Interface-(primary-usage)","page":"Theta","title":"Formula Interface (primary usage)","text":"using Durbyn, Durbyn.Grammar\n\ndata = (sales = [120, 135, 148, 152, 141, 158, 170, 165, 180, 195],)\n\n# Auto-select among STM/OTM/DSTM/DOTM via in-sample MSE\nspec = ThetaSpec(@formula(sales = theta()))\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"theta/#Dynamic-Optimised-Theta-with-explicit-options","page":"Theta","title":"Dynamic Optimised Theta with explicit options","text":"using Durbyn, Durbyn.Grammar\n\ndata = (demand = collect(1.0:100.0),)\n\nspec = ThetaSpec(@formula(demand = theta(\n    model = :DOTM,\n    decomposition = \"multiplicative\",\n    nmse = 5,              # optimise on 1-5 step SSE\n    theta_param = nothing, # optimise theta\n    alpha = nothing        # optimise alpha\n)))\n\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"theta/#Panel-data-/-grouped-fitting","page":"Theta","title":"Panel data / grouped fitting","text":"using Durbyn, Durbyn.TableOps, Durbyn.ModelSpecs, Durbyn.Grammar\n\n# stacked table with :series column\npanel = PanelData(tbl; groupby = :series, date = :date, m = 12)\nspec = ThetaSpec(@formula(value = theta(model = :OTM, decomposition = \"additive\")))\nfitted = fit(spec, panel)\nfc = forecast(fitted, h = 12)\n\nKey options (all optional): model, alpha, theta_param, decomposition (\"multiplicative\" or \"additive\"), nmse (1–30 step MSE objective).\n\n","category":"section"},{"location":"theta/#Base-API-(array-interface)","page":"Theta","title":"Base API (array interface)","text":"using Durbyn\n\ny = collect(1.0:50.0) .+ randn(50)\n\n# Fit a specific variant\nfit_otm = theta(y, 12; model_type = OTM, nmse = 3)   # Optimised Theta\n\n# Let Durbyn choose the best variant by MSE\nfit_auto = auto_theta(y, 12)\n\nfc = forecast(fit_auto; h = 8)  # returns Forecast with mean & intervals\n\nThetaFit exposes model_type, alpha, theta, initial_level, mse, fitted values, residuals, and decomposition metadata.\n\n","category":"section"},{"location":"theta/#Methodology-(Fiorucci-et-al.,-2016)","page":"Theta","title":"Methodology (Fiorucci et al., 2016)","text":"The classic Theta line with second-difference scaling theta:\n\nZ_t(theta) = theta Y_t + (1-theta)(A_n + B_n t) qquad\nA_n = overlineY - tfracn+12B_nquad\nB_n = frac6n^2-1left(frac2nsum_t=1^n tY_t - frac1+nnsum_t=1^n Y_tright)\n\nRecomposition for theta_1 = 0 theta_2 = theta gives\n\nY_t = left(1-tfrac1thetaright)(A_n + B_n t) + tfrac1theta Z_t(theta)\n\nForecasts combine regression and the SES extrapolation of the theta line:\n\nwidehatY_n+hmid n =\nleft(1-tfrac1thetaright)biglA_n + B_n(n+h)bigr\n + tfrac1thetawidetildeZ_n+hmid n(theta)\n\nwith SES recursion and closed form for h=1:\n\nell_t = alpha Y_t + (1-alpha)ell_t-1 qquad ell_0 = ell_0^*theta\n\nwidetildeZ_n+1mid n(theta) =\ntheta ell_n + (1-theta)left\n  A_nbigl(1-(1-alpha)^nbigr) +\n  B_nBigl(n+bigl(1-tfrac1alphabigr)bigl(1-(1-alpha)^nbigr)Bigr)\nright\n\nOptimised Theta (state-space form):\n\nY_t = mu_t + varepsilon_t qquad\nmu_t = ell_t-1 + left(1-tfrac1thetaright)left(1-alpha)^t-1A_n + tfrac1-(1-alpha)^talphaB_nright qquad\nell_t = alpha Y_t + (1-alpha)ell_t-1\n\nThe h-step-ahead forecast from origin n:\n\nwidehatY_n+hmid n = ell_n + left(1-tfrac1thetaright)left(1-alpha)^n A_n + left(h-1) + tfrac1-(1-alpha)^n+1alpharightB_nright\n\nDynamic variants update regression coefficients each period:\n\nwidehatY_t+1mid t = ell_t + left(1-tfrac1thetaright)left(1-alpha)^t A_t + tfrac1-(1-alpha)^t+1alphaB_tright\n\nA_t = overlineY_t - tfract+12 B_t qquad\nB_t = tfrac1t+1left(t-2)B_t-1 + tfrac6t(Y_t - overlineY_t-1)right qquad\noverlineY_t = tfrac1tleft(t-1)overlineY_t-1 + Y_tright\n\nParameters are estimated by SSE / Gaussian likelihood:\n\n(widehatell_0 widehatalpha widehattheta) = argmin_ell_0alphatheta sum_t=1^n (Y_t - mu_t)^2\nqquad\nl = -tfracn2log(mathrmSSEn) - tfracn2(1+log 2pi)","category":"section"},{"location":"theta/#Prediction-intervals","page":"Theta","title":"Prediction intervals","text":"The conditional variance for h-step-ahead forecasts:\n\nmathrmVarleftY_n+h mid Y_1ldotsY_nright = left1 + (h-1)alpha^2rightsigma^2\n\nyielding prediction intervals:\n\nwidehatY_n+hmid n pm z_1-a2sqrtleft1 + (h-1)alpha^2rightsigma^2","category":"section"},{"location":"theta/#Connection-to-SES-with-drift","page":"Theta","title":"Connection to SES with drift","text":"The mapping to SES with drift (Theorem 2 in Fiorucci et al.) is b = (1-tfrac1theta)B_n and ell^**_0 = ell_0 + (1-tfrac1theta)A_n.\n\n","category":"section"},{"location":"theta/#References","page":"Theta","title":"References","text":"Fiorucci, J. A., Pellegrini, T. R., Louzada, F., Petropoulos, F., & Koehler, A. B. (2016). Models for optimising the theta method and their relationship to state space models. International Journal of Forecasting, 32(4), 1151–1161.\nAssimakopoulos, V., & Nikolopoulos, K. (2000). The theta model: a decomposition  approach to forecasting. International Journal of Forecasting, 16(4), 521-530.","category":"section"},{"location":"bats/#BATS:-Box-Cox,-ARMA-errors,-Trend,-Seasonal-Models","page":"BATS","title":"BATS: Box-Cox, ARMA errors, Trend, Seasonal Models","text":"The BATS framework extends exponential smoothing to accommodate multiple, possibly long seasonal cycles together with Box–Cox variance stabilization and ARMA error correction. It was introduced by De Livera, Hyndman & Snyder (2011) as part of the innovation-state-space family and is the method implemented by Durbyn’s [bats] function.\n\nThis page summarizes the core equations, highlights limitations (and why TBATS was proposed in the paper), and shows how to use the Julia interface.\n\n","category":"section"},{"location":"bats/#1.-Box–Cox-transformation","page":"BATS","title":"1. Box–Cox transformation","text":"Each BATS model may apply a Box–Cox transformation to the observed series, which stabilizes variance prior to modeling:\n\ny_t^(omega) =\nbegincases\ndfracy_t^omega - 1omega  omega neq 0 \nln y_t  omega = 0 \nendcases\n\nThe parameter omega (often denoted lambda in code) is estimated within the automated model search when the user permits Box–Cox transforms.\n\n","category":"section"},{"location":"bats/#2.-BATS-state-space-formulation","page":"BATS","title":"2. BATS state-space formulation","text":"After optional transformation, BATS is written in innovations form with an ARMA error process.","category":"section"},{"location":"bats/#Observation-equation","page":"BATS","title":"Observation equation","text":"y_t^(omega) = ell_t-1 + phi b_t-1 + sum_i s_it-1 + d_t\n\nwhere ell_t is the level, b_t the trend, phi the damping parameter, s_it the seasonal state for seasonal period m_i, and d_t the ARMA error term.","category":"section"},{"location":"bats/#State-equations","page":"BATS","title":"State equations","text":"Level and trend:\n\nell_t = ell_t-1 + phi b_t-1 + alpha d_t qquad\nb_t = phi b_t-1 + beta d_t\n\nAdditive seasonality for each seasonal block i (normalized form):\n\ns_it = -sum_j=1^m_i-1 s_it-j + gamma_i d_t","category":"section"},{"location":"bats/#ARMA-error-component","page":"BATS","title":"ARMA error component","text":"d_t = varepsilon_t + sum_k=1^p varphi_k varepsilon_t-k\n      + sum_ell=1^q theta_ell d_t-ell\nqquad varepsilon_t sim mathcalN(0 sigma^2)\n\nCombining these pieces yields the descriptor BATS(ω, {p,q}, φ, {m₁,…,m_J}) that Durbyn prints for each fitted model.\n\n","category":"section"},{"location":"bats/#3.-Limitations-and-relation-to-TBATS","page":"BATS","title":"3. Limitations and relation to TBATS","text":"In the original paper, TBATS was introduced to address several BATS limitations:\n\nSeasonal periods must be integers, and each requires storing m_i state components, which becomes expensive for very long cycles.\nNon-integer or dual-calendar seasonalities (e.g., Hijri and Gregorian) cannot be represented exactly.\n\nTBATS replaces the seasonal states with Fourier (trigonometric) terms to overcome those issues. Durbyn provides both BATS and TBATS implementations. The bats function corresponds strictly to the BATS formulation above for integer seasonal periods, while tbats supports non-integer periods and more efficient handling of long seasonal cycles via Fourier representation.\n\nnote: TBATS documentation\nFor detailed information about TBATS, including non-integer seasonal periods, dual calendar effects, and computational efficiency, see the TBATS documentation.\n\n","category":"section"},{"location":"bats/#4.-Usage-in-Durbyn","page":"BATS","title":"4. Usage in Durbyn","text":"Durbyn provides two interfaces for BATS: the classic API with direct function calls and the grammar interface for declarative model specification.","category":"section"},{"location":"bats/#Grammar-Interface-(Recommended)","page":"BATS","title":"Grammar Interface (Recommended)","text":"The grammar interface provides a unified, declarative way to specify BATS models using @formula and BatsSpec:\n\nusing Durbyn\nusing Durbyn.ModelSpecs\n\n# Create sample data\ndata = (sales = randn(120) .+ 10,)\n\n# Basic BATS with defaults (automatic component selection)\nspec = BatsSpec(@formula(sales = bats()))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# BATS with monthly seasonality\nspec = BatsSpec(@formula(sales = bats(seasonal_periods=12)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# BATS with multiple seasonal periods (e.g., hourly data with daily and weekly)\nspec = BatsSpec(@formula(sales = bats(seasonal_periods=[24, 168])))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# BATS with specific component selection\nspec = BatsSpec(@formula(sales = bats(\n    seasonal_periods=12,\n    use_box_cox=true,\n    use_trend=true,\n    use_damped_trend=false,\n    use_arma_errors=true\n)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# Additional options at fit time\nfitted = fit(spec, data, bc_lower=0.0, bc_upper=1.5, biasadj=true)\n\nPanel Data Support:\n\n# Create panel data (Tables.jl compatible)\ntbl = (\n    date = repeat(1:120, 3),\n    product = repeat([\"A\", \"B\", \"C\"], inner=120),\n    sales = randn(360) .+ 10\n)\n\n# Fit BATS to each product separately\nspec = BatsSpec(@formula(sales = bats(seasonal_periods=12)))\nfitted = fit(spec, tbl, groupby = :product)\nfc = forecast(fitted, h = 12)\n\nModel Comparison:\n\nmodels = model(\n    BatsSpec(@formula(sales = bats(seasonal_periods=12))),\n    ArimaSpec(@formula(sales = p() + q() + P() + Q())),\n    EtsSpec(@formula(sales = e(\"Z\") + t(\"Z\") + s(\"Z\"))),\n    names = [\"bats\", \"arima\", \"ets\"]\n)\n\nfitted = fit(models, data)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"bats/#Classic-API","page":"BATS","title":"Classic API","text":"For direct usage without the grammar interface:\n\nusing Durbyn\n\n# Hourly demand with weekly (24*7) and yearly (24*365) seasonality\nm = [168, 8760]\nfit = bats(load, m; use_box_cox = true, use_arma_errors = true)\n\nprintln(string(fit))\nfc = forecast(fit; h = 168)","category":"section"},{"location":"bats/#Formula-Interface-(Direct)","page":"BATS","title":"Formula Interface (Direct)","text":"You can also use the formula interface directly without BatsSpec:\n\nusing Durbyn\n\ndata = (sales = randn(120) .+ 10,)\nformula = @formula(sales = bats(seasonal_periods=12))\nfit = bats(formula, data)  # Works with Tables.jl compatible data\nfc = forecast(fit, h = 12)","category":"section"},{"location":"bats/#Key-keyword-arguments","page":"BATS","title":"Key keyword arguments","text":"seasonal_periods: Int or Vector{Int} specifying seasonal period(s)\nuse_box_cox, use_trend, use_damped_trend: Bool, Vector{Bool}, or nothing to try both options; the best combination is chosen using AIC.\nuse_arma_errors: toggles fitting an ARMA(p, q) model to the residuals via [auto_arima]; if the selected ARMA orders are zero, the pure exponential-smoothing state-space model is retained.\nbc_lower, bc_upper: bounds for the Box–Cox search when enabled.\nbiasadj: apply bias correction during inverse Box–Cox transformation.\nmodel: pass a previous BATSModel to refit the same structure to new data without re-running the full model selection process.\n\nThe convenience method bats(y, m::Int; kwargs...) simply wraps the vector interface ([m]), making single-season calls ergonomic.\n\n","category":"section"},{"location":"bats/#5.-Reference","page":"BATS","title":"5. Reference","text":"De Livera, A.M., Hyndman, R.J., & Snyder, R.D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. Journal of the American Statistical Association, 106(496), 1513–1527.","category":"section"},{"location":"tableops/#Table-Operations","page":"Table Operations","title":"Table Operations","text":"The TableOps module provides a comprehensive set of data manipulation functions for working with Tables.jl-compatible data sources. These functions enable common data wrangling tasks like filtering, grouping, pivoting, and summarizing data.","category":"section"},{"location":"tableops/#Overview","page":"Table Operations","title":"Overview","text":"TableOps is inspired by popular data manipulation libraries like dplyr and tidyr (R) and pandas (Python), but designed specifically for Julia's Tables.jl ecosystem. All functions work seamlessly with any Tables.jl-compatible data source, including:\n\nNamedTuples\nDataFrames\nCSV.File objects\nArrow.Table objects\nAnd many others","category":"section"},{"location":"tableops/#Time-Series-and-Panel-Data","page":"Table Operations","title":"Time Series and Panel Data","text":"Durbyn.jl is a forecasting package, and time series data manipulation is at its core. The TableOps module is designed to work seamlessly with PanelData, a specialized data structure for handling multiple time series (panel/longitudinal data).\n\nThe PanelData interface follows the tidy forecasting workflow from Hyndman & Athanasopoulos (2021), providing a structured six-step approach:\n\nData Preparation — Load, reshape, and clean data using TableOps\nVisualization — Explore patterns with plot() and glimpse()\nModel Specification — Define models using the formula interface (@formula)\nModel Training — Fit models with fit(), producing fitted model objects\nPerformance Evaluation — Assess accuracy with accuracy() and diagnostics\nForecasting — Generate predictions with forecast(), returning tidy forecast tables","category":"section"},{"location":"tableops/#What-is-PanelData?","page":"Table Operations","title":"What is PanelData?","text":"PanelData wraps your tabular data with metadata that defines:\n\nGrouping columns: Which columns identify individual time series (e.g., :series_id, :store, :product)\nDate column: Which column contains the time index\nSeasonal period (m): The number of observations per seasonal cycle (e.g., 12 for monthly data with yearly seasonality)\nFrequency: The time frequency (:daily, :weekly, :monthly, :quarterly, :yearly)\nTarget column: The variable to forecast","category":"section"},{"location":"tableops/#Why-PanelData-Matters-for-Forecasting","page":"Table Operations","title":"Why PanelData Matters for Forecasting","text":"When working with forecasting, you typically need to:\n\nProcess multiple series independently - Each series may have different scales, patterns, and missing values\nPreserve time ordering - Operations should respect the temporal structure\nHandle missing time points - Gaps in time series need special treatment\nCompute group-relative features - Features like \"deviation from series mean\" require within-group calculations\n\nPanelData enables all of this by automatically applying operations within each group while preserving the panel structure.","category":"section"},{"location":"tableops/#Quick-Example","page":"Table Operations","title":"Quick Example","text":"using Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\n# Create panel data with multiple time series\ndata = (\n    series = [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n    date = [1, 2, 3, 4, 1, 2, 3, 4],\n    value = [100, 110, 105, 120, 500, 520, 510, 540]\n)\n\n# Wrap in PanelData - this defines the panel structure\npanel = PanelData(data; groupby=:series, date=:date, m=12)\n\n# Now TableOps functions automatically work within each series\n# Compute series-relative features\nresult = mutate(panel,\n    series_mean = d -> fill(mean(d.value), length(d.value)),\n    deviation = d -> d.value .- mean(d.value),\n    pct_change = d -> [missing; diff(d.value) ./ d.value[1:end-1] .* 100]\n)\n\n# Fill missing values within each series (not across series!)\nfilled = fill_missing(result, :pct_change; direction=:down)","category":"section"},{"location":"tableops/#When-to-Use-PanelData-vs-Regular-Tables","page":"Table Operations","title":"When to Use PanelData vs Regular Tables","text":"Use Case Data Type Why\nSingle time series Regular table No grouping needed\nMultiple independent series PanelData Operations apply per-series\nCross-sectional data Regular table No time structure\nPanel/longitudinal data PanelData Groups + time structure\nForecasting preparation PanelData Preserves metadata for models\n\nFor detailed PanelData operations, see the PanelData Operations section below.\n\n","category":"section"},{"location":"tableops/#Getting-Started","page":"Table Operations","title":"Getting Started","text":"using CSV\nusing Downloads\nusing Tables\nusing Durbyn\nusing Durbyn.TableOps\nusing Durbyn.Grammar\nusing Durbyn.ModelSpecs\n\n# Download example retail data\nlocal_path = Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/refs/heads/main/Data/retail.csv\")\nretail = CSV.File(local_path)\ntbl = Tables.columntable(retail)\n\n# Preview the data\nglimpse(tbl)","category":"section"},{"location":"tableops/#Function-Reference","page":"Table Operations","title":"Function Reference","text":"","category":"section"},{"location":"tableops/#Quick-Reference-Table","page":"Table Operations","title":"Quick Reference Table","text":"Category Function Description\nPreview glimpse Quick data preview with types and samples\nSelect select Select and rename columns\n rename Rename columns (keep all)\n all_of Select columns by name vector\n everything Select all columns\nFilter query Filter rows by predicate\n distinct Remove duplicate rows\nSort arrange Sort rows by columns\nTransform mutate Add or modify columns\n across Apply functions across multiple columns\nGroup groupby Group data by columns\n ungroup Remove grouping\n summarise Aggregate grouped data\nReshape pivot_longer Wide to long format\n pivot_wider Long to wide format\nCombine bind_rows Stack tables vertically\nJoin inner_join Keep only matching rows\n left_join Keep all left rows\n right_join Keep all right rows\n full_join Keep all rows from both\n semi_join Filter left by right keys\n anti_join Filter left by missing right keys\nString separate Split column into multiple\n unite Combine columns into one\nMissing fill_missing Fill missing values\n complete Complete missing combinations\n\n","category":"section"},{"location":"tableops/#Core-Functions","page":"Table Operations","title":"Core Functions","text":"","category":"section"},{"location":"tableops/#glimpse-Quick-Data-Preview","page":"Table Operations","title":"glimpse - Quick Data Preview","text":"Get a compact summary of your data, showing column names, types, and sample values.\n\nusing Durbyn.TableOps\n\ntbl = (date = [\"2024-01\", \"2024-02\", \"2024-03\"],\n       A = [100, 110, 120],\n       B = [200, 220, 240],\n       C = [300, 330, 360])\n\nglimpse(tbl)\n# Table glimpse\n#   Rows: 3\n#   Columns: 4\n#   date                :: String  [\"2024-01\", \"2024-02\", \"2024-03\"]\n#   A                   :: Int64   [100, 110, 120]\n#   B                   :: Int64   [200, 220, 240]\n#   C                   :: Int64   [300, 330, 360]\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\nmaxrows (keyword, default: 5) - Maximum number of sample values to show\nio (keyword, default: stdout) - Output stream\n\n","category":"section"},{"location":"tableops/#select-Choose-and-Rename-Columns","page":"Table Operations","title":"select - Choose and Rename Columns","text":"Select specific columns from your data, optionally renaming them.\n\nusing Durbyn.TableOps\n\ntbl = (id = [1, 2, 3],\n       name = [\"Shler\", \"Rivka\", \"Dilan\"],\n       age = [25, 30, 35],\n       salary = [50000, 60000, 70000])\n\n# Select specific columns\nselect(tbl, :name, :age)\n# Output: (name = [\"Shler\", \"Rivka\", \"Dilan\"], age = [25, 30, 35])\n\n# Rename while selecting\nselect(tbl, :employee => :name, :years => :age)\n# Output: (employee = [\"Shler\", \"Rivka\", \"Dilan\"], years = [25, 30, 35])\n\n# Mix selection and renaming\nselect(tbl, :id, :employee_name => :name)\n# Output: (id = [1, 2, 3], employee_name = [\"Shler\", \"Rivka\", \"Dilan\"])\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\nspecs... - Column specifications as Symbols or Pair{Symbol,Symbol} for renaming\n\n","category":"section"},{"location":"tableops/#rename-Rename-Columns","page":"Table Operations","title":"rename - Rename Columns","text":"Rename columns while keeping all columns in the table.\n\nusing Durbyn.TableOps\n\ntbl = (a = [1, 2, 3], b = [4, 5, 6], c = [7, 8, 9])\n\n# Rename single column\nrename(tbl, :x => :a)\n# Output: (x = [1, 2, 3], b = [4, 5, 6], c = [7, 8, 9])\n\n# Rename multiple columns\nrename(tbl, :x => :a, :y => :b)\n# Output: (x = [1, 2, 3], y = [4, 5, 6], c = [7, 8, 9])\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\nspecs... - Rename specifications as Pair{Symbol,Symbol}: :new_name => :old_name\n\nNote: Unlike select, rename keeps all columns - it only changes names of specified columns.\n\n","category":"section"},{"location":"tableops/#query-Filter-Rows","page":"Table Operations","title":"query - Filter Rows","text":"Filter rows based on custom conditions using a predicate function.\n\nusing Durbyn.TableOps\n\ntbl = (product = [\"A\", \"B\", \"C\", \"D\", \"E\"],\n       price = [10, 25, 15, 30, 20],\n       quantity = [100, 50, 75, 25, 60])\n\n# Filter rows where price > 15\nquery(tbl, row -> row.price > 15)\n# Output: (product = [\"B\", \"D\", \"E\"], price = [25, 30, 20], quantity = [50, 25, 60])\n\n# Multiple conditions\nquery(tbl, row -> row.price > 15 && row.quantity > 30)\n# Output: (product = [\"B\", \"E\"], price = [25, 20], quantity = [50, 60])\n\n# Using `in` for categorical filtering\nquery(tbl, row -> row.product in [\"A\", \"C\", \"E\"])\n# Output: (product = [\"A\", \"C\", \"E\"], price = [10, 15, 20], quantity = [100, 75, 60])\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\npredicate - A function that takes a row (as NamedTuple) and returns Bool\n\nHandling Missing Values: The predicate must return exactly true or false. If your data contains missing values and comparisons might return missing, use coalesce:\n\n# This throws an error if price contains missing:\nquery(tbl, row -> row.price > 15)\n\n# Use coalesce to treat missing as false:\nquery(tbl, row -> coalesce(row.price > 15, false))\n\n","category":"section"},{"location":"tableops/#distinct-Remove-Duplicate-Rows","page":"Table Operations","title":"distinct - Remove Duplicate Rows","text":"Remove duplicate rows based on specified columns.\n\nusing Durbyn.TableOps\n\ntbl = (a = [1, 1, 2, 2, 3],\n       b = [1, 1, 2, 3, 3],\n       c = [10, 20, 30, 40, 50])\n\n# Distinct by all columns (removes exact duplicate rows)\ndistinct(tbl)\n# Output: (a = [1, 1, 2, 2, 3], b = [1, 1, 2, 3, 3], c = [10, 20, 30, 40, 50])\n# (no duplicates in this case)\n\n# Distinct by specific column - keeps only specified columns\ndistinct(tbl, :a)\n# Output: (a = [1, 2, 3],)\n\n# Distinct by specific column but keep all columns\ndistinct(tbl, :a; keep_all=true)\n# Output: (a = [1, 2, 3], b = [1, 2, 3], c = [10, 30, 50])\n# (keeps first occurrence of each unique value)\n\n# Distinct by multiple columns\ndistinct(tbl, :a, :b)\n# Output: (a = [1, 2, 2, 3], b = [1, 2, 3, 3])\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\ncols... - Column names to consider for uniqueness (if empty, uses all columns)\nkeep_all (keyword, default: false) - If true, keep all columns; if false, only keep specified columns\n\n","category":"section"},{"location":"tableops/#arrange-Sort-Data","page":"Table Operations","title":"arrange - Sort Data","text":"Sort rows by one or more columns in ascending or descending order.\n\nusing Durbyn.TableOps\n\ntbl = (name = [\"Shler\", \"Rivka\", \"Dilan\", \"Moshe\"],\n       department = [\"Sales\", \"IT\", \"Sales\", \"IT\"],\n       salary = [60000, 70000, 55000, 75000])\n\n# Sort by salary (ascending)\narrange(tbl, :salary)\n# Output: (name = [\"Dilan\", \"Shler\", \"Rivka\", \"Moshe\"],\n#          department = [\"Sales\", \"Sales\", \"IT\", \"IT\"],\n#          salary = [55000, 60000, 70000, 75000])\n\n# Sort by salary (descending)\narrange(tbl, :salary => :desc)\n# Output: (name = [\"Moshe\", \"Rivka\", \"Shler\", \"Dilan\"],\n#          department = [\"IT\", \"IT\", \"Sales\", \"Sales\"],\n#          salary = [75000, 70000, 60000, 55000])\n\n# Multi-column sort: department ascending, then salary descending\narrange(tbl, :department, :salary => :desc)\n# Output: (name = [\"Moshe\", \"Rivka\", \"Shler\", \"Dilan\"],\n#          department = [\"IT\", \"IT\", \"Sales\", \"Sales\"],\n#          salary = [75000, 70000, 60000, 55000])\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\ncols... - Column specifications: Symbol for ascending, Pair (:col => :desc) for descending\nrev (keyword, default: false) - If true, reverse the entire final sort order\n\nDescending indicators: :desc, :descending, :reverse, or false\n\n","category":"section"},{"location":"tableops/#mutate-Add-or-Modify-Columns","page":"Table Operations","title":"mutate - Add or Modify Columns","text":"Create new columns or modify existing ones based on computations.\n\nusing Durbyn.TableOps\n\ntbl = (product = [\"A\", \"B\", \"C\"],\n       price = [10.0, 20.0, 15.0],\n       quantity = [100, 50, 75])\n\n# Add a new column\nmutate(tbl, revenue = data -> data.price .* data.quantity)\n# Output: (product = [\"A\", \"B\", \"C\"],\n#          price = [10.0, 20.0, 15.0],\n#          quantity = [100, 50, 75],\n#          revenue = [1000.0, 1000.0, 1125.0])\n\n# Add multiple columns\nmutate(tbl,\n    revenue = data -> data.price .* data.quantity,\n    discounted_price = data -> data.price .* 0.9)\n\n# Modify existing column\nmutate(tbl, price = data -> data.price .* 1.1)  # 10% price increase\n\n# Reference previously created columns (sequential evaluation)\nmutate(tbl,\n    revenue = data -> data.price .* data.quantity,\n    revenue_per_unit = data -> data.revenue ./ data.quantity)  # Uses newly created revenue\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\nkwargs... - Named arguments where name is column name and value is either:\nA function data -> Vector that computes the column\nA vector of values (must match row count)\n\n","category":"section"},{"location":"tableops/#groupby-Group-Data","page":"Table Operations","title":"groupby - Group Data","text":"Group rows by unique combinations of values in specified columns.\n\nusing Durbyn.TableOps\n\ntbl = (department = [\"Sales\", \"IT\", \"Sales\", \"IT\", \"Sales\"],\n       employee = [\"Shler\", \"Rivka\", \"Dilan\", \"Moshe\", \"Jwan\"],\n       salary = [60000, 70000, 55000, 75000, 65000])\n\n# Group by department\ngt = groupby(tbl, :department)\n# Output: GroupedTable(2 groups by department)\n\nglimpse(gt)\n# GroupedTable glimpse\n#   Groups: 2\n#   Key columns: department\n#   Rows: 5 (avg 2.5, min 2, max 3)\n#   Group 1: (department = \"IT\",) (2 rows)\n#     ...\n#   Group 2: (department = \"Sales\",) (3 rows)\n#     ...\n\n# Group by multiple columns\nsales_data = (region = [\"North\", \"South\", \"North\", \"South\"],\n              product = [\"A\", \"A\", \"B\", \"B\"],\n              revenue = [1000, 1500, 2000, 2500])\n\ngt = groupby(sales_data, :region, :product)\n# Output: GroupedTable(4 groups by region, product)\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\ncols... - One or more column names (as Symbols) to group by\n\nReturns: A GroupedTable object for use with summarise or ungroup\n\n","category":"section"},{"location":"tableops/#ungroup-Remove-Grouping","page":"Table Operations","title":"ungroup - Remove Grouping","text":"Remove grouping from a GroupedTable, returning the underlying data.\n\nusing Durbyn.TableOps\n\ntbl = (category = [\"A\", \"B\", \"A\", \"B\"],\n       value = [1, 2, 3, 4])\n\ngt = groupby(tbl, :category)\n# Output: GroupedTable(2 groups by category)\n\n# Remove grouping\nresult = ungroup(gt)\n# Output: (category = [\"A\", \"B\", \"A\", \"B\"], value = [1, 2, 3, 4])\n\nParameters:\n\ngt - A GroupedTable created by groupby\n\nReturns: The original NamedTuple data without grouping\n\n","category":"section"},{"location":"tableops/#summarise-/-summarize-Aggregate-Data","page":"Table Operations","title":"summarise / summarize - Aggregate Data","text":"Compute summary statistics for each group in a GroupedTable.\n\nusing Durbyn.TableOps\nusing Statistics\n\ntbl = (department = [\"Sales\", \"IT\", \"Sales\", \"IT\", \"Sales\"],\n       employee = [\"Shler\", \"Rivka\", \"Dilan\", \"Moshe\", \"Jwan\"],\n       salary = [60000, 70000, 55000, 75000, 65000])\n\ngt = groupby(tbl, :department)\n\n# Compute mean salary per department\nstbl = summarise(gt, avg_salary = :salary => mean)\n# Output: (department = [\"IT\", \"Sales\"], avg_salary = [72500.0, 60000.0])\n\n# Multiple summary statistics\nsummarise(gt,\n    avg_salary = :salary => mean,\n    min_salary = :salary => minimum,\n    max_salary = :salary => maximum,\n    count = data -> length(data.salary))\n# Output: (department = [\"IT\", \"Sales\"],\n#          avg_salary = [72500.0, 60000.0],\n#          min_salary = [70000, 55000],\n#          max_salary = [75000, 65000],\n#          count = [2, 3])\n\n# Multi-column aggregation\nsummarise(gt,\n    salary_range = (:salary,) => col -> maximum(col) - minimum(col))\n\nParameters:\n\ngt - A GroupedTable created by groupby\nkwargs... - Named summary specifications where each value can be:\n:column => function - Apply function to a specific column\n(:col1, :col2) => function - Apply function to multiple columns\ndata -> scalar - Function taking the entire group data\n\nNote: summarize is an alias for summarise (American English spelling).\n\n","category":"section"},{"location":"tableops/#Column-Selection-Helpers","page":"Table Operations","title":"Column Selection Helpers","text":"","category":"section"},{"location":"tableops/#all_of-Select-Columns-by-Name-Vector","page":"Table Operations","title":"all_of - Select Columns by Name Vector","text":"Select columns using a vector of column names. Useful when column names are stored in a variable.\n\nusing Durbyn.TableOps\n\ntbl = (a = [1, 2], b = [3, 4], c = [5, 6], d = [7, 8])\n\n# Select columns from a vector\ncols_to_select = [:a, :c]\nselect(tbl, all_of(cols_to_select))\n# Output: (a = [1, 2], c = [5, 6])\n\n# Useful for programmatic column selection\nnumeric_cols = [:a, :b]\nselect(tbl, all_of(numeric_cols))\n# Output: (a = [1, 2], b = [3, 4])\n\nParameters:\n\ncols - A vector of column names (as Symbols or Strings)\n\n","category":"section"},{"location":"tableops/#everything-Select-All-Columns","page":"Table Operations","title":"everything - Select All Columns","text":"Select all columns. Useful for reordering columns or combining with other selections.\n\nusing Durbyn.TableOps\n\ntbl = (a = [1, 2], b = [3, 4], c = [5, 6])\n\n# Select all columns\nselect(tbl, everything())\n# Output: (a = [1, 2], b = [3, 4], c = [5, 6])\n\n# Reorder: put :c first, then all others\nselect(tbl, :c, everything())\n# Output: (c = [5, 6], a = [1, 2], b = [3, 4])\n\n# Reorder: put :b and :c first\nselect(tbl, :b, :c, everything())\n# Output: (b = [3, 4], c = [5, 6], a = [1, 2])\n\nNote: When combining with other selectors, columns are deduplicated (each column appears only once).\n\n","category":"section"},{"location":"tableops/#across-Apply-Functions-Across-Columns","page":"Table Operations","title":"across - Apply Functions Across Columns","text":"Apply one or more functions across multiple columns. Used with mutate or summarise.\n\nusing Durbyn.TableOps\nusing Statistics\n\n# With summarise\ntbl = (group = [\"A\", \"A\", \"B\", \"B\"],\n       x = [1.0, 2.0, 3.0, 4.0],\n       y = [10.0, 20.0, 30.0, 40.0])\n\ngt = groupby(tbl, :group)\n\n# Apply mean to multiple columns\nsummarise(gt, across([:x, :y], :mean => mean))\n# Output: (group = [\"A\", \"B\"], x_mean = [1.5, 3.5], y_mean = [15.0, 35.0])\n\n# Multiple functions\nsummarise(gt, across([:x, :y], :mean => mean, :sum => sum))\n# Output: (group = [\"A\", \"B\"],\n#          x_mean = [1.5, 3.5], x_sum = [3.0, 7.0],\n#          y_mean = [15.0, 35.0], y_sum = [30.0, 70.0])\n\n# With everything() - applies to all non-grouping columns\nsummarise(gt, across(everything(), :mean => mean))\n# Output: (group = [\"A\", \"B\"], x_mean = [1.5, 3.5], y_mean = [15.0, 35.0])\n\n# With mutate\ntbl2 = (a = [1.0, 2.0, 3.0], b = [4.0, 5.0, 6.0])\nmutate(tbl2, across([:a, :b], :squared => x -> x .^ 2))\n# Output: (a = [1.0, 2.0, 3.0], b = [4.0, 5.0, 6.0],\n#          a_squared = [1.0, 4.0, 9.0], b_squared = [16.0, 25.0, 36.0])\n\nParameters:\n\ncols - Column specification: vector of symbols, all_of(...), or everything()\nfns... - One or more Pair{Symbol, Function}: :name => function\n\nOutput column naming: {original_column}_{function_name}\n\n","category":"section"},{"location":"tableops/#Reshape-Functions","page":"Table Operations","title":"Reshape Functions","text":"","category":"section"},{"location":"tableops/#pivot_longer-Wide-to-Long-Format","page":"Table Operations","title":"pivot_longer - Wide to Long Format","text":"Transform data from wide format to long format by pivoting columns into rows.\n\nusing Durbyn.TableOps\n\n# Wide format data\nwide = (date = [\"2024-01\", \"2024-02\", \"2024-03\"],\n        A = [100, 110, 120],\n        B = [200, 220, 240],\n        C = [300, 330, 360])\n\n# Convert to long format\nlong = pivot_longer(wide, id_cols=:date, names_to=:series, values_to=:value)\n# Output: (date = [\"2024-01\", \"2024-01\", \"2024-01\", \"2024-02\", ...],\n#          series = [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n#          value = [100, 200, 300, 110, 220, 330, 120, 240, 360])\n\nglimpse(long)\n# Table glimpse\n#   Rows: 9\n#   Columns: 3\n#   date    :: String  [\"2024-01\", \"2024-01\", \"2024-01\", \"2024-02\", \"2024-02\", ...]\n#   series  :: String  [\"A\", \"B\", \"C\", \"A\", \"B\", ...]\n#   value   :: Int64   [100, 200, 300, 110, 220, ...]\n\n# Specify which columns to pivot\npivot_longer(wide, id_cols=:date, value_cols=[:A, :B], names_to=:series, values_to=:value)\n# Only pivots A and B columns, C is excluded\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\nid_cols (keyword) - Column(s) to keep as identifiers (not pivoted)\nvalue_cols (keyword) - Column(s) to pivot (if empty, all non-id columns)\nnames_to (keyword, default: :variable) - Name for the column containing original column names\nvalues_to (keyword, default: :value) - Name for the column containing values\n\nColumn Selection Logic:\n\nIf both id_cols and value_cols are empty: all columns become value columns\nIf only id_cols is provided: all other columns become value columns\nIf only value_cols is provided: all other columns become id columns\nIf both are provided: unspecified columns are added to id_cols (not dropped)\n\n","category":"section"},{"location":"tableops/#pivot_wider-Long-to-Wide-Format","page":"Table Operations","title":"pivot_wider - Long to Wide Format","text":"Transform data from long format to wide format by spreading rows into columns.\n\nusing Durbyn.TableOps\n\n# Long format data\nlong = (date = [\"2024-01\", \"2024-01\", \"2024-01\", \"2024-02\", \"2024-02\", \"2024-02\"],\n        series = [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        value = [100, 200, 300, 110, 220, 330])\n\n# Convert to wide format\nwide = pivot_wider(long, names_from=:series, values_from=:value, id_cols=:date)\n# Output: (date = [\"2024-01\", \"2024-02\"],\n#          A = [100, 110],\n#          B = [200, 220],\n#          C = [300, 330])\n\n# Sort column names alphabetically\npivot_wider(long, names_from=:series, values_from=:value,\n            id_cols=:date, sort_names=true)\n\n# Handle missing combinations with custom fill value\nincomplete = (id = [1, 1, 2], category = [\"A\", \"B\", \"A\"], val = [10, 20, 30])\npivot_wider(incomplete, names_from=:category, values_from=:val, fill=0)\n# Output: (id = [1, 2], A = [10, 30], B = [20, 0])\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\nnames_from - Column containing values to become new column names\nvalues_from - Column containing values to populate new columns\nid_cols (keyword) - Column(s) that uniquely identify each row\nfill (keyword, default: missing) - Value for missing combinations\nsort_names (keyword, default: false) - Sort new column names alphabetically\n\n","category":"section"},{"location":"tableops/#Combine-Functions","page":"Table Operations","title":"Combine Functions","text":"","category":"section"},{"location":"tableops/#bind_rows-Stack-Tables-Vertically","page":"Table Operations","title":"bind_rows - Stack Tables Vertically","text":"Combine multiple tables by stacking rows. Handles mismatched columns by filling with missing.\n\nusing Durbyn.TableOps\n\n# Tables with same columns\ntbl1 = (a = [1, 2], b = [3, 4])\ntbl2 = (a = [5, 6], b = [7, 8])\n\nbind_rows(tbl1, tbl2)\n# Output: (a = [1, 2, 5, 6], b = [3, 4, 7, 8])\n\n# Tables with different columns\ntbl3 = (a = [1, 2], b = [3, 4])\ntbl4 = (a = [5, 6], c = [7, 8])\n\nbind_rows(tbl3, tbl4)\n# Output: (a = [1, 2, 5, 6],\n#          b = Union{Missing, Int64}[3, 4, missing, missing],\n#          c = Union{Missing, Int64}[missing, missing, 7, 8])\n\n# Multiple tables\ntbl5 = (x = [1], y = [2])\ntbl6 = (x = [3], y = [4])\ntbl7 = (x = [5], y = [6])\n\nbind_rows(tbl5, tbl6, tbl7)\n# Output: (x = [1, 3, 5], y = [2, 4, 6])\n\nParameters:\n\ntables... - Two or more Tables.jl-compatible data sources\n\nNote: Column order is determined by the order columns first appear across all tables.\n\n","category":"section"},{"location":"tableops/#Join-Functions","page":"Table Operations","title":"Join Functions","text":"Join functions combine two tables based on matching key columns. TableOps provides six types of joins to handle different use cases.","category":"section"},{"location":"tableops/#Join-Types-Overview","page":"Table Operations","title":"Join Types Overview","text":"Join Type Keeps Use Case\ninner_join Rows matching in both tables Find common records\nleft_join All left rows + matching right Enrich left data\nright_join All right rows + matching left Enrich right data\nfull_join All rows from both tables Complete union\nsemi_join Left rows with match (no right columns) Filter by existence\nanti_join Left rows without match Find missing records\n\nNote: Joins use Julia's isequal semantics for key matching: missing matches missing and NaN matches NaN. This differs from SQL where NULL never equals NULL.","category":"section"},{"location":"tableops/#by-Parameter-Specification","page":"Table Operations","title":"by Parameter Specification","text":"All join functions accept a by parameter to specify join keys:\n\n# Auto-detect common columns\ninner_join(left, right)\n\n# Single column (same name in both)\ninner_join(left, right, by=:id)\n\n# Multiple columns (same names)\ninner_join(left, right, by=[:id, :date])\n\n# Different column names\ninner_join(left, right, by=:left_id => :right_id)\n\n# Multiple different column names\ninner_join(left, right, by=[:id => :key, :date => :timestamp])\n\n","category":"section"},{"location":"tableops/#inner_join-Keep-Matching-Rows","page":"Table Operations","title":"inner_join - Keep Matching Rows","text":"Return only rows where keys exist in both tables.\n\nusing Durbyn.TableOps\n\nleft = (id = [1, 2, 3], x = [10, 20, 30])\nright = (id = [2, 3, 4], y = [200, 300, 400])\n\ninner_join(left, right, by=:id)\n# Output: (id = [2, 3], x = [20, 30], y = [200, 300])\n# Only ids 2 and 3 are in both tables\n\nParameters:\n\nleft - Left table\nright - Right table\nby (keyword) - Join key specification\nsuffix (keyword, default: (\"_x\", \"_y\")) - Suffixes for duplicate column names\n\n","category":"section"},{"location":"tableops/#left_join-Keep-All-Left-Rows","page":"Table Operations","title":"left_join - Keep All Left Rows","text":"Return all rows from left, with matching data from right. Non-matching rows have missing for right columns.\n\nusing Durbyn.TableOps\n\nleft = (id = [1, 2, 3], x = [10, 20, 30])\nright = (id = [2, 3, 4], y = [200, 300, 400])\n\nleft_join(left, right, by=:id)\n# Output: (id = [1, 2, 3], x = [10, 20, 30], y = [missing, 200, 300])\n# All left rows kept; id=1 has no match, so y is missing\n\nUse Case: Enriching a primary dataset with additional information while preserving all original records.\n\nParameters:\n\nleft - Left table (all rows preserved)\nright - Right table (only matching rows included)\nby (keyword) - Join key specification\nsuffix (keyword, default: (\"_x\", \"_y\")) - Suffixes for duplicate column names\n\n","category":"section"},{"location":"tableops/#right_join-Keep-All-Right-Rows","page":"Table Operations","title":"right_join - Keep All Right Rows","text":"Return all rows from right, with matching data from left. Non-matching rows have missing for left columns.\n\nusing Durbyn.TableOps\n\nleft = (id = [1, 2, 3], x = [10, 20, 30])\nright = (id = [2, 3, 4], y = [200, 300, 400])\n\nright_join(left, right, by=:id)\n# Output: (id = [2, 3, 4], x = [20, 30, missing], y = [200, 300, 400])\n# All right rows kept; id=4 has no match, so x is missing\n\nParameters:\n\nleft - Left table (only matching rows included)\nright - Right table (all rows preserved)\nby (keyword) - Join key specification\nsuffix (keyword, default: (\"_x\", \"_y\")) - Suffixes for duplicate column names\n\n","category":"section"},{"location":"tableops/#full_join-Keep-All-Rows","page":"Table Operations","title":"full_join - Keep All Rows","text":"Return all rows from both tables. Non-matching rows have missing for columns from the other table.\n\nusing Durbyn.TableOps\n\nleft = (id = [1, 2, 3], x = [10, 20, 30])\nright = (id = [2, 3, 4], y = [200, 300, 400])\n\nfull_join(left, right, by=:id)\n# Output: (id = [1, 2, 3, 4],\n#          x = [10, 20, 30, missing],\n#          y = [missing, 200, 300, 400])\n# All ids present; missing values where no match\n\nUse Case: Creating a complete view of all records from both sources.\n\nParameters:\n\nleft - Left table\nright - Right table\nby (keyword) - Join key specification\nsuffix (keyword, default: (\"_x\", \"_y\")) - Suffixes for duplicate column names\n\n","category":"section"},{"location":"tableops/#semi_join-Filter-by-Existence","page":"Table Operations","title":"semi_join - Filter by Existence","text":"Return rows from left where the key exists in right. No columns from right are added.\n\nusing Durbyn.TableOps\n\nleft = (id = [1, 2, 3, 4], x = [10, 20, 30, 40])\nright = (id = [2, 4], y = [200, 400])\n\nsemi_join(left, right, by=:id)\n# Output: (id = [2, 4], x = [20, 40])\n# Only left columns; filtered to ids present in right\n\nUse Case: Filtering a table to records that exist in another table (e.g., customers who have orders).\n\nParameters:\n\nleft - Table to filter\nright - Table to check for key existence\nby (keyword) - Join key specification\n\nNote: Unlike inner_join, no columns from right are added to the result.\n\n","category":"section"},{"location":"tableops/#anti_join-Filter-by-Non-Existence","page":"Table Operations","title":"anti_join - Filter by Non-Existence","text":"Return rows from left where the key does NOT exist in right. No columns from right are added.\n\nusing Durbyn.TableOps\n\nleft = (id = [1, 2, 3, 4], x = [10, 20, 30, 40])\nright = (id = [2, 4], y = [200, 400])\n\nanti_join(left, right, by=:id)\n# Output: (id = [1, 3], x = [10, 30])\n# Only left columns; filtered to ids NOT in right\n\nUse Case: Finding records that don't have a match (e.g., customers without orders, missing data).\n\nParameters:\n\nleft - Table to filter\nright - Table to check for key non-existence\nby (keyword) - Join key specification\n\n","category":"section"},{"location":"tableops/#Join-Examples","page":"Table Operations","title":"Join Examples","text":"","category":"section"},{"location":"tableops/#Multiple-Key-Columns","page":"Table Operations","title":"Multiple Key Columns","text":"using Durbyn.TableOps\n\norders = (customer_id = [1, 1, 2, 2],\n          product_id = [\"A\", \"B\", \"A\", \"C\"],\n          quantity = [10, 20, 15, 25])\n\nprices = (customer_id = [1, 2],\n          product_id = [\"A\", \"A\"],\n          price = [100.0, 95.0])\n\ninner_join(orders, prices, by=[:customer_id, :product_id])\n# Output: (customer_id = [1, 2], product_id = [\"A\", \"A\"],\n#          quantity = [10, 15], price = [100.0, 95.0])","category":"section"},{"location":"tableops/#Different-Column-Names","page":"Table Operations","title":"Different Column Names","text":"using Durbyn.TableOps\n\nemployees = (emp_id = [1, 2, 3], name = [\"Shler\", \"Rivka\", \"Dilan\"])\nsalaries = (employee_key = [1, 2, 4], salary = [50000, 60000, 70000])\n\nleft_join(employees, salaries, by=:emp_id => :employee_key)\n# Output: (emp_id = [1, 2, 3], name = [\"Shler\", \"Rivka\", \"Dilan\"],\n#          salary = [50000, 60000, missing])","category":"section"},{"location":"tableops/#Handling-Duplicate-Column-Names","page":"Table Operations","title":"Handling Duplicate Column Names","text":"using Durbyn.TableOps\n\ndf1 = (id = [1, 2], value = [10, 20])\ndf2 = (id = [1, 2], value = [100, 200])\n\ninner_join(df1, df2, by=:id)\n# Output: (id = [1, 2], value_x = [10, 20], value_y = [100, 200])\n# Non-key duplicate columns get suffixes\n\n# Custom suffixes\ninner_join(df1, df2, by=:id, suffix=(\"_left\", \"_right\"))\n# Output: (id = [1, 2], value_left = [10, 20], value_right = [100, 200])","category":"section"},{"location":"tableops/#One-to-Many-Joins","page":"Table Operations","title":"One-to-Many Joins","text":"using Durbyn.TableOps\n\ncustomers = (id = [1, 2], name = [\"Shler\", \"Rivka\"])\norders = (customer_id = [1, 1, 2], order_id = [101, 102, 103], amount = [50, 75, 100])\n\nleft_join(customers, orders, by=:id => :customer_id)\n# Output: (id = [1, 1, 2], name = [\"Shler\", \"Shler\", \"Rivka\"],\n#          order_id = [101, 102, 103], amount = [50, 75, 100])\n# Shler appears twice (has 2 orders)\n\n","category":"section"},{"location":"tableops/#String-Functions","page":"Table Operations","title":"String Functions","text":"","category":"section"},{"location":"tableops/#separate-Split-Column-into-Multiple","page":"Table Operations","title":"separate - Split Column into Multiple","text":"Separate a character column into multiple columns by splitting on a delimiter.\n\nusing Durbyn.TableOps\n\n# Basic separation\ntbl = (id = [1, 2, 3], name = [\"Peshraw-Cohen\", \"Narin-Levi\", \"Hawreh-Katz\"])\n\nseparate(tbl, :name; into=[:first, :last], sep=\"-\")\n# Output: (id = [1, 2, 3],\n#          first = [\"Peshraw\", \"Narin\", \"Hawreh\"],\n#          last = [\"Cohen\", \"Levi\", \"Katz\"])\n\n# Keep original column\nseparate(tbl, :name; into=[:first, :last], sep=\"-\", remove=false)\n# Output: (id = [1, 2, 3],\n#          name = [\"Peshraw-Cohen\", \"Narin-Levi\", \"Hawreh-Katz\"],\n#          first = [\"Peshraw\", \"Narin\", \"Hawreh\"],\n#          last = [\"Cohen\", \"Levi\", \"Katz\"])\n\n# With numeric conversion\ntbl2 = (id = [1, 2], coords = [\"10,20\", \"30,40\"])\nseparate(tbl2, :coords; into=[:x, :y], sep=\",\", convert=true)\n# Output: (id = [1, 2], x = [10.0, 30.0], y = [20.0, 40.0])\n\n# Using regex separator\ntbl3 = (data = [\"a1b\", \"c2d\", \"e3f\"],)\nseparate(tbl3, :data; into=[:letter1, :num, :letter2], sep=r\"[0-9]\")\n\n# Handling uneven splits (extra parts are dropped, missing parts become missing)\ntbl4 = (text = [\"a-b-c\", \"x-y\"],)\nseparate(tbl4, :text; into=[:p1, :p2, :p3], sep=\"-\")\n# Output: (p1 = [\"a\", \"x\"], p2 = [\"b\", \"y\"], p3 = [\"c\", missing])\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\ncol - Column name to separate\ninto (keyword) - Vector of names for the new columns\nsep (keyword, default: \" \") - Separator pattern (String, Char, or Regex)\nremove (keyword, default: true) - Remove the input column\nconvert (keyword, default: false) - Attempt to convert to numeric types. Tries Int first, falls back to Float64 if needed.\n\n","category":"section"},{"location":"tableops/#unite-Combine-Columns-into-One","page":"Table Operations","title":"unite - Combine Columns into One","text":"Combine multiple columns into a single character column.\n\nusing Durbyn.TableOps\n\ntbl = (id = [1, 2, 3],\n       year = [2020, 2021, 2022],\n       month = [1, 6, 12])\n\n# Basic unite\nunite(tbl, :date, :year, :month; sep=\"-\")\n# Output: (id = [1, 2, 3], date = [\"2020-1\", \"2021-6\", \"2022-12\"])\n\n# Keep original columns\nunite(tbl, :date, :year, :month; sep=\"-\", remove=false)\n# Output: (id = [1, 2, 3],\n#          year = [2020, 2021, 2022],\n#          month = [1, 6, 12],\n#          date = [\"2020-1\", \"2021-6\", \"2022-12\"])\n\n# Custom separator\nunite(tbl, :period, :year, :month; sep=\"/\")\n# Output: (id = [1, 2, 3], period = [\"2020/1\", \"2021/6\", \"2022/12\"])\n\n# Multiple columns\ntbl2 = (a = [\"x\", \"y\"], b = [1, 2], c = [\"!\", \"?\"])\nunite(tbl2, :combined, :a, :b, :c; sep=\"\")\n# Output: (combined = [\"x1!\", \"y2?\"],)\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\nnew_col - Name for the new combined column\ncols... - Columns to combine (at least one required)\nsep (keyword, default: \"_\") - Separator between values\nremove (keyword, default: true) - Remove the input columns\n\nNote: If any value is missing, the combined result is missing.\n\n","category":"section"},{"location":"tableops/#Missing-Value-Functions","page":"Table Operations","title":"Missing Value Functions","text":"","category":"section"},{"location":"tableops/#fill_missing-Fill-Missing-Values","page":"Table Operations","title":"fill_missing - Fill Missing Values","text":"Fill missing values using the previous or next non-missing value (forward/backward fill).\n\nusing Durbyn.TableOps\n\ntbl = (id = [1, 2, 3, 4, 5],\n       value = [10, missing, missing, 40, missing])\n\n# Fill down (forward fill) - default\nfill_missing(tbl, :value)\n# Output: (id = [1, 2, 3, 4, 5], value = [10, 10, 10, 40, 40])\n\n# Fill up (backward fill)\nfill_missing(tbl, :value; direction=:up)\n# Output: (id = [1, 2, 3, 4, 5], value = [10, 40, 40, 40, missing])\n\n# Fill both directions (down first, then up)\nfill_missing(tbl, :value; direction=:downup)\n# Output: (id = [1, 2, 3, 4, 5], value = [10, 10, 10, 40, 40])\n\n# Fill both directions (up first, then down)\nfill_missing(tbl, :value; direction=:updown)\n# Output: (id = [1, 2, 3, 4, 5], value = [10, 40, 40, 40, 40])\n\n# Fill multiple columns\ntbl2 = (a = [1, missing, 3], b = [missing, 2, missing])\nfill_missing(tbl2, :a, :b)\n# Output: (a = [1, 1, 3], b = [missing, 2, 2])\n\n# Fill all columns (no columns specified)\nfill_missing(tbl2)\n# Output: (a = [1, 1, 3], b = [missing, 2, 2])\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\ncols... - Columns to fill (if empty, fills all columns)\ndirection (keyword, default: :down) - Fill direction:\n:down - Forward fill (last observation carried forward)\n:up - Backward fill (next observation carried backward)\n:downup - Forward fill, then backward fill\n:updown - Backward fill, then forward fill\n\n","category":"section"},{"location":"tableops/#complete-Complete-Missing-Combinations","page":"Table Operations","title":"complete - Complete Missing Combinations","text":"Expand a table to include all combinations of specified columns, filling new rows with a default value.\n\nusing Durbyn.TableOps\n\ntbl = (year = [2020, 2020, 2021],\n       quarter = [1, 2, 1],\n       value = [100, 200, 150])\n\n# Complete all year-quarter combinations\ncomplete(tbl, :year, :quarter)\n# Output: (year = [2020, 2020, 2021, 2021],\n#          quarter = [1, 2, 1, 2],\n#          value = Union{Missing, Int64}[100, 200, 150, missing])\n\n# With custom fill value\ncomplete(tbl, :year, :quarter; fill_value=0)\n# Output: (year = [2020, 2020, 2021, 2021],\n#          quarter = [1, 2, 1, 2],\n#          value = [100, 200, 150, 0])\n\n# Example with product-region combinations\nsales = (product = [\"A\", \"A\", \"B\"],\n         region = [\"North\", \"South\", \"North\"],\n         sales = [100, 150, 120])\ncomplete(sales, :product, :region; fill_value=0)\n# Output: (product = [\"A\", \"A\", \"B\", \"B\"],\n#          region = [\"North\", \"South\", \"North\", \"South\"],\n#          sales = [100, 150, 120, 0])\n# Adds missing B-South combination with sales=0\n\nParameters:\n\ndata - Any Tables.jl-compatible data source\ncols... - Columns to expand (creates all unique combinations)\nfill_value (keyword, default: missing) - Value for new rows\n\nNote: Original rows are preserved; only missing combinations are added.\n\n","category":"section"},{"location":"tableops/#Complete-Workflow-Examples","page":"Table Operations","title":"Complete Workflow Examples","text":"","category":"section"},{"location":"tableops/#Example-1:-Basic-Data-Analysis-Pipeline","page":"Table Operations","title":"Example 1: Basic Data Analysis Pipeline","text":"using Durbyn.TableOps\nusing Statistics\n\n# Sample employee data\nemployees = (\n    department = [\"Sales\", \"IT\", \"Sales\", \"IT\", \"Sales\", \"HR\", \"HR\"],\n    employee = [\"Shler\", \"Rivka\", \"Dilan\", \"Moshe\", \"Jwan\", \"Avraham\", \"Miriam\"],\n    salary = [60000, 70000, 55000, 75000, 65000, 50000, 52000],\n    years = [5, 8, 3, 10, 6, 2, 4]\n)\n\n# Step 1: Preview data\nglimpse(employees)\n\n# Step 2: Filter high earners\nfiltered = query(employees, row -> row.salary > 55000)\n\n# Step 3: Group by department\ngrouped = groupby(filtered, :department)\n\n# Step 4: Compute statistics\nsummary = summarise(grouped,\n    avg_salary = :salary => mean,\n    avg_years = :years => mean,\n    headcount = data -> length(data.salary))\n\n# Step 5: Sort by average salary\nresult = arrange(summary, :avg_salary => :desc)\n\nglimpse(result)","category":"section"},{"location":"tableops/#Example-2:-Time-Series-Panel-Data","page":"Table Operations","title":"Example 2: Time Series Panel Data","text":"using CSV\nusing Downloads\nusing Tables\nusing Durbyn.TableOps\nusing Statistics\n\n# Download retail data\nlocal_path = Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/refs/heads/main/Data/retail.csv\")\nretail = CSV.File(local_path)\ntbl = Tables.columntable(retail)\n\n# Step 1: Transform from wide to long format\ntbl_long = pivot_longer(tbl, id_cols=:date, names_to=:series, values_to=:value)\nglimpse(tbl_long)\n\n# Step 2: Filter to specific series\ntbl_filtered = query(tbl_long, row -> row.series in [\"series_10\", \"series_20\", \"series_30\"])\n\n# Step 3: Add computed columns\ntbl_with_log = mutate(tbl_filtered, log_value = data -> log.(data.value))\n\n# Step 4: Group by series\ngt = groupby(tbl_with_log, :series)\n\n# Step 5: Compute summary statistics\nsummary = summarise(gt,\n    mean_value = :value => mean,\n    std_value = :value => std,\n    min_value = :value => minimum,\n    max_value = :value => maximum,\n    count = data -> length(data.value))\n\n# Step 6: Sort by mean value\nresult = arrange(summary, :mean_value => :desc)\nglimpse(result)","category":"section"},{"location":"tableops/#Example-3:-Data-Cleaning-with-Missing-Values","page":"Table Operations","title":"Example 3: Data Cleaning with Missing Values","text":"using Durbyn.TableOps\n\n# Messy data with missing values and inconsistent formatting\nraw_data = (\n    date = [\"2024-01\", \"2024-02\", \"2024-03\", \"2024-04\"],\n    region_product = [\"North-A\", \"North-B\", \"South-A\", \"South-B\"],\n    value = [100, missing, 150, missing]\n)\n\n# Step 1: Separate region and product\ncleaned = separate(raw_data, :region_product; into=[:region, :product], sep=\"-\")\n\n# Step 2: Fill missing values (forward fill)\nfilled = fill_missing(cleaned, :value)\n\n# Step 3: Complete all region-product combinations\ncompleted = complete(filled, :region, :product; fill_value=0)\n\nglimpse(completed)","category":"section"},{"location":"tableops/#Example-4:-Using-across-for-Multi-Column-Operations","page":"Table Operations","title":"Example 4: Using across for Multi-Column Operations","text":"using Durbyn.TableOps\nusing Statistics\n\n# Sales data with multiple metrics\nsales = (\n    region = [\"North\", \"North\", \"South\", \"South\", \"East\", \"East\"],\n    product = [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n    revenue = [1000.0, 1500.0, 2000.0, 2500.0, 1800.0, 2200.0],\n    units = [100.0, 150.0, 200.0, 250.0, 180.0, 220.0],\n    returns = [5.0, 8.0, 10.0, 12.0, 9.0, 11.0]\n)\n\n# Compute mean and sum for all numeric columns per region\ngt = groupby(sales, :region)\n\n# Apply multiple functions across multiple columns\nresult = summarise(gt, across([:revenue, :units, :returns], :mean => mean, :total => sum))\n\nglimpse(result)\n# Output columns: region, revenue_mean, revenue_total, units_mean, units_total, returns_mean, returns_total\n\n","category":"section"},{"location":"tableops/#Working-with-GroupedTable","page":"Table Operations","title":"Working with GroupedTable","text":"The GroupedTable type is a central concept in TableOps, similar to grouped data frames in other languages.\n\nusing Durbyn.TableOps\nusing Statistics\n\nsales_data = (\n    region = [\"North\", \"South\", \"North\", \"South\", \"East\", \"East\", \"West\"],\n    product = [\"A\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\"],\n    revenue = [1000, 1500, 2000, 2500, 1800, 2200, 1200],\n    units = [100, 150, 200, 250, 180, 220, 120]\n)\n\n# Group by multiple columns\ngt = groupby(sales_data, :region, :product)\nglimpse(gt)\n\n# Compute complex summaries\nsummary = summarise(gt,\n    total_revenue = :revenue => sum,\n    total_units = :units => sum,\n    avg_price = data -> sum(data.revenue) / sum(data.units),\n    count = data -> length(data.revenue))\n\nglimpse(summary)\n\n# Ungroup to get back to regular table\nungrouped = ungroup(gt)\n\n","category":"section"},{"location":"tableops/#PanelData-Operations","page":"Table Operations","title":"PanelData Operations","text":"TableOps provides special dispatches for PanelData objects that automatically apply operations within each group. This is essential for time series forecasting where you need to process multiple series independently while preserving the panel structure.","category":"section"},{"location":"tableops/#How-PanelData-Operations-Work","page":"Table Operations","title":"How PanelData Operations Work","text":"When you call a TableOps function on a PanelData object:\n\nThe data is automatically grouped by the panel's grouping columns\nThe operation is applied to each group independently\nResults are combined back into a new PanelData with the same metadata\n\nThis differs from regular table operations where the function operates on the entire dataset at once.","category":"section"},{"location":"tableops/#Creating-PanelData","page":"Table Operations","title":"Creating PanelData","text":"using Durbyn.ModelSpecs\n\n# Basic panel data with grouping and date\npanel = PanelData(data;\n    groupby = :series,           # Column(s) identifying each series\n    date = :date,                # Time index column\n    m = 12                       # Seasonal period (12 = monthly with yearly cycle)\n)\n\n# With frequency (m is inferred automatically)\npanel = PanelData(data;\n    groupby = [:store, :product],  # Multiple grouping columns\n    date = :date,\n    frequency = :monthly           # :daily, :weekly, :monthly, :quarterly, :yearly\n)\n\n# With target column for forecasting\npanel = PanelData(data;\n    groupby = :series,\n    date = :date,\n    frequency = :monthly,\n    target = :sales               # The variable to forecast\n)\n\n# With preprocessing: fill time gaps and impute missing values\npanel = PanelData(data;\n    groupby = :series,\n    date = :date,\n    frequency = :monthly,\n    target = :sales,\n    fill_time = true,             # Fill missing time points\n    target_na = (method = :interpolate,)  # Impute missing target values\n)\n\n# Balanced panel: all groups padded to same global time span\npanel = PanelData(data;\n    groupby = :series,\n    date = :date,\n    frequency = :monthly,\n    fill_time = true,\n    balanced = true               # All groups get same start/end dates\n)","category":"section"},{"location":"tableops/#PanelData-Fields","page":"Table Operations","title":"PanelData Fields","text":"Field Type Description\ndata Any The underlying table data\ngroups Vector{Symbol} Columns that identify each series\ndate Symbol or nothing Time index column\nm Int, Vector{Int}, or nothing Seasonal period(s)\nfrequency Symbol or nothing Time frequency\ntarget Symbol or nothing Target variable for forecasting","category":"section"},{"location":"tableops/#Balanced-Panels","page":"Table Operations","title":"Balanced Panels","text":"When working with panel data, different groups (series) often have different time spans. For example:\n\nStore A might have data from January 2024 to December 2024\nStore B might have data from March 2024 to November 2024\n\nBy default, fill_time=true only fills gaps within each group's own time span. This creates an unbalanced panel where groups have different numbers of observations.\n\nThe balanced=true option creates a balanced panel by padding all groups to the global time span (from the earliest date to the latest date across all groups). Groups with shorter histories get missing values for dates outside their original range.","category":"section"},{"location":"tableops/#When-to-Use-Balanced-Panels","page":"Table Operations","title":"When to Use Balanced Panels","text":"Scenario Use balanced Why\nFixed-effects regression ✅ Yes Many panel regression methods require balanced panels\nMatrix-based models ✅ Yes Models that stack series into matrices need equal lengths\nComparing series statistics ✅ Yes Fair comparison requires same observation periods\nCross-sectional aggregation ✅ Yes Computing \"average across all stores on date X\" needs all stores present\nIndependent forecasting ❌ No When forecasting each series independently, unbalanced is fine\nMemory-constrained ❌ No Balanced panels can be much larger if time spans vary widely","category":"section"},{"location":"tableops/#Example:-Balanced-vs-Unbalanced","page":"Table Operations","title":"Example: Balanced vs Unbalanced","text":"using Durbyn.ModelSpecs\nusing Dates\n\n# Store A: Jan 3-5 (3 days), Store B: Jan 1-4 (4 days)\ndata = (\n    store = [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n    date = [Date(2024,1,3), Date(2024,1,4), Date(2024,1,5),\n            Date(2024,1,1), Date(2024,1,2), Date(2024,1,3), Date(2024,1,4)],\n    sales = [130, 140, 150, 100, 110, 120, 130]\n)\n\n# Unbalanced: each group keeps its own time span\npanel_unbalanced = PanelData(data;\n    groupby = :store,\n    date = :date,\n    frequency = :daily,\n    fill_time = true\n)\n# Store A: 3 rows (Jan 3-5)\n# Store B: 4 rows (Jan 1-4)\n# Total: 7 rows\n\n# Balanced: all groups padded to global span (Jan 1-5)\npanel_balanced = PanelData(data;\n    groupby = :store,\n    date = :date,\n    frequency = :daily,\n    fill_time = true,\n    balanced = true\n)\n# Store A: 5 rows (Jan 1-5, with missing for Jan 1-2)\n# Store B: 5 rows (Jan 1-5, with missing for Jan 5)\n# Total: 10 rows\n\nAfter balancing, Store A has missing sales for January 1-2 (before its data began), and Store B has missing sales for January 5 (after its data ended). You can then use target_na or fill_missing to impute these values if needed.\n\n","category":"section"},{"location":"tableops/#Supported-Operations-Reference","page":"Table Operations","title":"Supported Operations Reference","text":"Function PanelData Behavior Returns\nquery Filter rows within each group PanelData\nmutate Add/modify columns within each group PanelData\narrange Sort rows within each group PanelData\nselect Select columns (grouping/date columns auto-included) PanelData\ndistinct Remove duplicates within each group (grouping columns auto-included) PanelData\nfill_missing Fill missing values within each group PanelData\nrename Rename columns (updates group/date metadata) PanelData\npivot_longer Pivot to long (grouping/date columns auto-added to id_cols) PanelData\npivot_wider Pivot to wide (grouping/date columns auto-added to id_cols) PanelData\nsummarise Summarize each group (collapses time dimension) NamedTuple\n\n","category":"section"},{"location":"tableops/#query-Filter-Rows-Per-Series","page":"Table Operations","title":"query - Filter Rows Per Series","text":"Filter rows independently within each series based on a predicate.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\ndata = (series = [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n        date = [1, 2, 3, 4, 1, 2, 3, 4],\n        value = [10, 25, 15, 30, 100, 150, 120, 180])\n\npanel = PanelData(data; groupby=:series, date=:date)\n\n# Keep only rows where value > 20 (applied per series)\nfiltered = query(panel, row -> row.value > 20)\nglimpse(filtered)\n# Series A: keeps dates 2, 4 (values 25, 30)\n# Series B: keeps all dates (all values > 20)\n\nUse Case: Remove outliers, filter to specific time periods, or apply series-specific conditions.\n\n","category":"section"},{"location":"tableops/#mutate-Group-Relative-Feature-Engineering","page":"Table Operations","title":"mutate - Group-Relative Feature Engineering","text":"Create new columns with computations that operate within each series. This is essential for creating features like:\n\nSeries-level statistics (mean, std, min, max)\nDeviations from series mean (centering/normalization)\nLagged values and differences\nRolling statistics\nPercentage of series total\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\nusing Statistics\n\ndata = (series = [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n        date = [1, 2, 3, 4, 1, 2, 3, 4],\n        value = [100, 110, 90, 120, 500, 520, 480, 540])\n\npanel = PanelData(data; groupby=:series, date=:date, m=12)\n\n# Feature engineering within each series\nresult = mutate(panel,\n    # Series statistics (broadcast to all rows)\n    series_mean = d -> fill(mean(d.value), length(d.value)),\n    series_std = d -> fill(std(d.value), length(d.value)),\n\n    # Centering and scaling\n    centered = d -> d.value .- mean(d.value),\n    scaled = d -> (d.value .- mean(d.value)) ./ std(d.value),\n\n    # Percentage of series total\n    pct_of_total = d -> d.value ./ sum(d.value) .* 100,\n\n    # Lagged values\n    lag1 = d -> [missing; d.value[1:end-1]],\n\n    # Differences\n    diff1 = d -> [missing; diff(d.value)],\n\n    # Percent change\n    pct_change = d -> [missing; diff(d.value) ./ d.value[1:end-1] .* 100]\n)\n\nglimpse(result)\n\nKey Point: The function receives only the current group's data, so mean(d.value) computes the mean for that series only, not the global mean.\n\n","category":"section"},{"location":"tableops/#arrange-Sort-Within-Each-Series","page":"Table Operations","title":"arrange - Sort Within Each Series","text":"Sort rows by one or more columns within each series independently.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\n# Data with dates out of order\ndata = (series = [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"],\n        date = [3, 1, 2, 2, 3, 1],\n        value = [30, 10, 20, 200, 300, 100])\n\npanel = PanelData(data; groupby=:series)\n\n# Sort by date within each series\nsorted = arrange(panel, :date)\n# Series A: dates [1, 2, 3], values [10, 20, 30]\n# Series B: dates [1, 2, 3], values [100, 200, 300]\n\n# Sort descending\nsorted_desc = arrange(panel, :date => :desc)\n\n# Multi-column sort\narrange(panel, :category, :date => :desc)\n\nUse Case: Ensure time ordering after joins or other operations that may scramble row order.\n\n","category":"section"},{"location":"tableops/#select-Select-Columns-(Preserving-Structure)","page":"Table Operations","title":"select - Select Columns (Preserving Structure)","text":"Select columns from the panel. Grouping columns and date column are automatically included to preserve the panel structure.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\ndata = (series = [\"A\", \"A\", \"B\", \"B\"],\n        date = [1, 2, 1, 2],\n        value = [100, 110, 200, 210],\n        extra = [1, 2, 3, 4],\n        temp = [20, 21, 22, 23])\n\npanel = PanelData(data; groupby=:series, date=:date)\n\n# Select value column - series and date are auto-included\nresult = select(panel, :value)\n# Result has columns: series, date, value\n\n# Rename while selecting\nresult = select(panel, :sales => :value)\n# Result has columns: series, date, sales\n\n# Attempting to exclude grouping columns throws an error\n# select(panel, :value)  # :series is always included\n\nStructural Protection: The panel's grouping and date columns cannot be accidentally dropped, ensuring the panel structure remains valid.\n\n","category":"section"},{"location":"tableops/#distinct-Unique-Rows-Per-Series","page":"Table Operations","title":"distinct - Unique Rows Per Series","text":"Remove duplicate rows within each series. Grouping columns are automatically included in the uniqueness check.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\ndata = (series = [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"],\n        category = [\"X\", \"X\", \"Y\", \"X\", \"X\", \"Y\"],\n        value = [100, 100, 200, 300, 300, 400])\n\npanel = PanelData(data; groupby=:series)\n\n# Distinct by category within each series\n# (series is auto-included in uniqueness check)\nresult = distinct(panel, :category)\n# Series A: keeps one \"X\" and one \"Y\"\n# Series B: keeps one \"X\" and one \"Y\"\n\n# Keep all columns while deduping\nresult = distinct(panel, :category; keep_all=true)\n\n","category":"section"},{"location":"tableops/#fill_missing-Forward/Backward-Fill-Per-Series","page":"Table Operations","title":"fill_missing - Forward/Backward Fill Per Series","text":"Fill missing values using previous or next values within each series. This prevents values from one series \"leaking\" into another.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\ndata = (series = [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n        date = [1, 2, 3, 4, 1, 2, 3, 4],\n        value = [10, missing, missing, 40, missing, 200, missing, 400])\n\npanel = PanelData(data; groupby=:series, date=:date)\n\n# Forward fill within each series\nfilled_down = fill_missing(panel, :value; direction=:down)\n# Series A: [10, 10, 10, 40]\n# Series B: [missing, 200, 200, 400]  # First value stays missing\n\n# Backward fill\nfilled_up = fill_missing(panel, :value; direction=:up)\n# Series A: [10, 40, 40, 40]\n# Series B: [200, 200, 400, 400]\n\n# Forward then backward (fills all if possible)\nfilled_both = fill_missing(panel, :value; direction=:downup)\n# Series A: [10, 10, 10, 40]\n# Series B: [200, 200, 200, 400]\n\nCritical for Time Series: Without PanelData grouping, forward fill would carry values across series boundaries, corrupting your data.\n\n","category":"section"},{"location":"tableops/#rename-Rename-Columns-(Updating-Metadata)","page":"Table Operations","title":"rename - Rename Columns (Updating Metadata)","text":"Rename columns in the panel. If you rename a grouping column or date column, the panel metadata is automatically updated.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\ndata = (id = [\"A\", \"A\", \"B\", \"B\"],\n        time = [1, 2, 1, 2],\n        val = [100, 110, 200, 210])\n\npanel = PanelData(data; groupby=:id, date=:time)\n\n# Rename grouping column\nrenamed = rename(panel, :series => :id)\n# panel.groups is now [:series]\n\n# Rename date column\nrenamed = rename(panel, :date => :time)\n# panel.date is now :date\n\n# Rename regular column\nrenamed = rename(panel, :value => :val)\n\n","category":"section"},{"location":"tableops/#pivot_longer-Wide-to-Long-(Preserving-Structure)","page":"Table Operations","title":"pivot_longer - Wide to Long (Preserving Structure)","text":"Pivot from wide to long format. Grouping and date columns are automatically added to id_cols to preserve the panel structure.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\n# Wide format: multiple value columns\nwide_data = (series = [\"A\", \"A\", \"B\", \"B\"],\n             date = [1, 2, 1, 2],\n             sales = [100, 110, 200, 210],\n             costs = [80, 85, 150, 160])\n\npanel = PanelData(wide_data; groupby=:series, date=:date)\n\n# Pivot sales and costs to long format\n# series and date are auto-included as id_cols\nlong = pivot_longer(panel,\n    value_cols = [:sales, :costs],\n    names_to = :metric,\n    values_to = :amount)\n\nglimpse(long)\n# Columns: series, date, metric, amount\n# Each original row becomes 2 rows (one for sales, one for costs)\n\n","category":"section"},{"location":"tableops/#pivot_wider-Long-to-Wide-(Preserving-Structure)","page":"Table Operations","title":"pivot_wider - Long to Wide (Preserving Structure)","text":"Pivot from long to wide format. Grouping and date columns are automatically added to id_cols.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\n\n# Long format\nlong_data = (series = [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n             date = [1, 1, 2, 2, 1, 1, 2, 2],\n             metric = [\"sales\", \"costs\", \"sales\", \"costs\", \"sales\", \"costs\", \"sales\", \"costs\"],\n             amount = [100, 80, 110, 85, 200, 150, 210, 160])\n\npanel = PanelData(long_data; groupby=:series, date=:date)\n\n# Pivot metric values to columns\nwide = pivot_wider(panel,\n    names_from = :metric,\n    values_from = :amount)\n\nglimpse(wide)\n# Columns: series, date, sales, costs\n\n","category":"section"},{"location":"tableops/#summarise-Aggregate-Per-Series","page":"Table Operations","title":"summarise - Aggregate Per Series","text":"Compute summary statistics for each series. Returns a NamedTuple (not PanelData) since the time dimension is collapsed.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\nusing Statistics\n\ndata = (series = [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n        date = [1, 2, 3, 4, 1, 2, 3, 4],\n        value = [100, 110, 90, 120, 500, 520, 480, 540])\n\npanel = PanelData(data; groupby=:series, date=:date)\n\n# Compute statistics per series\nstats = summarise(panel,\n    n = d -> length(d.value),\n    mean_val = :value => mean,\n    std_val = :value => std,\n    min_val = :value => minimum,\n    max_val = :value => maximum,\n    range_val = d -> maximum(d.value) - minimum(d.value),\n    cv = d -> std(d.value) / mean(d.value)  # Coefficient of variation\n)\n\nglimpse(stats)\n# (series = [\"A\", \"B\"], n = [4, 4], mean_val = [105.0, 510.0], ...)\n\nNote: summarise returns a NamedTuple, not PanelData, because the result has one row per series (time dimension is gone).\n\n","category":"section"},{"location":"tableops/#Using-across-with-PanelData","page":"Table Operations","title":"Using across with PanelData","text":"Apply the same function(s) across multiple columns within each group.\n\nusing Durbyn.TableOps\nusing Durbyn.ModelSpecs\nusing Statistics\n\ndata = (series = [\"A\", \"A\", \"B\", \"B\"],\n        date = [1, 2, 1, 2],\n        sales = [100.0, 110.0, 200.0, 210.0],\n        costs = [80.0, 85.0, 150.0, 160.0],\n        units = [10.0, 11.0, 20.0, 21.0])\n\npanel = PanelData(data; groupby=:series, date=:date)\n\n# Summarize multiple columns\nstats = summarise(panel, across([:sales, :costs, :units], :mean => mean, :sum => sum))\n# Result has: series, sales_mean, sales_sum, costs_mean, costs_sum, units_mean, units_sum\n\n","category":"section"},{"location":"tableops/#Complete-Forecasting-Workflow-Example","page":"Table Operations","title":"Complete Forecasting Workflow Example","text":"using Durbyn.TableOps\nusing Durbyn.ModelSpecs\nusing Statistics\n\n# Raw retail data with multiple stores\nraw_data = (\n    store = [\"S1\", \"S1\", \"S1\", \"S1\", \"S2\", \"S2\", \"S2\", \"S2\"],\n    date = [1, 2, 3, 4, 1, 2, 3, 4],\n    sales = [100.0, missing, 120.0, 130.0, 500.0, 520.0, missing, 560.0],\n    promo = [0, 1, 0, 1, 1, 0, 1, 0]\n)\n\n# Step 1: Create PanelData\npanel = PanelData(raw_data; groupby=:store, date=:date, m=12, target=:sales)\n\n# Step 2: Handle missing values per series\ncleaned = fill_missing(panel, :sales; direction=:downup)\n\n# Step 3: Feature engineering within each series\nfeatures = mutate(cleaned,\n    # Lag features\n    sales_lag1 = d -> [missing; d.sales[1:end-1]],\n    sales_lag2 = d -> [missing; missing; d.sales[1:end-2]],\n\n    # Rolling mean (simple 2-period)\n    sales_ma2 = d -> [missing; (d.sales[1:end-1] .+ d.sales[2:end]) ./ 2],\n\n    # Series-level features\n    series_mean = d -> fill(mean(d.sales), length(d.sales)),\n    deviation = d -> d.sales .- mean(d.sales),\n\n    # Trend proxy\n    time_idx = d -> collect(1:length(d.sales))\n)\n\n# Step 4: Filter to rows with complete features\nmodel_data = query(features, r -> !ismissing(r.sales_lag2))\n\n# Step 5: Summarize for exploration\nsummary = summarise(panel,\n    n = d -> length(d.sales),\n    mean_sales = :sales => x -> mean(skipmissing(x)),\n    total_sales = :sales => x -> sum(skipmissing(x)))\n\nglimpse(summary)\n\n","category":"section"},{"location":"tableops/#Key-Benefits-of-PanelData-Operations","page":"Table Operations","title":"Key Benefits of PanelData Operations","text":"Automatic Grouping: No need to manually groupby - the panel's group structure is used automatically\nPreserved Metadata: Operations return a new PanelData with the same grouping columns, date column, seasonal period, and other metadata\nStructural Protection: Grouping and date columns cannot be accidentally dropped by select or distinct\nGroup-Relative Computations: In mutate, functions receive only the current group's data, enabling proper within-series feature engineering\nIndependent Processing: Each group is processed independently, preventing data leakage between series (critical for fill_missing)\nSeamless Forecasting Integration: The preserved metadata flows directly into Durbyn's forecasting models\n\n","category":"section"},{"location":"tableops/#Tips-and-Best-Practices","page":"Table Operations","title":"Tips and Best Practices","text":"Use glimpse frequently: It's a quick way to understand your data's structure and verify transformations.\nPredicate functions in query: Keep them simple and readable. For complex filters, break them into logical parts or define named functions.\nType stability in mutate: Ensure your computed columns have consistent types across all rows.\nGroup before summarize: Always create a GroupedTable with groupby before using summarise.\nColumn naming: Use descriptive names in mutate and summarise to make your data self-documenting.\nPivot operations:\nUse pivot_longer when you need to reshape data for modeling or plotting\nUse pivot_wider when you need to create summary tables or compare values across categories\nMemory efficiency: TableOps functions return new NamedTuples, so be mindful of memory when working with very large datasets.\nChaining operations: Use intermediate variables for readability:\n# Recommended: Clear and debuggable\nfiltered = query(data, row -> row.x > 0)\ngrouped = groupby(filtered, :category)\nresult = summarise(grouped, mean_x = :x => mean)\nfill_missing direction: Use :downup or :updown to ensure all missing values are filled when you have missing values at both ends.\ncomplete for time series: Use with fill_missing to handle gaps in time series data:  julia  data |> x -> complete(x, :date) |> x -> fill_missing(x, :value)","category":"section"},{"location":"naive/#Naive-Forecasting-Methods","page":"Naive Methods","title":"Naive Forecasting Methods","text":"tip: Formula Interface is the Recommended Approach\nUse NaiveSpec, SnaiveSpec, RwSpec, or MeanfSpec with @formula for a declarative interface that works with panel data, grouped fitting, and model comparison. The base (array) API is shown near the end.\n\nNaive forecasting methods serve as simple benchmark models for more complex forecasting approaches. Despite their simplicity, they often perform surprisingly well, especially for highly volatile or unpredictable data. Durbyn implements four naive methods:\n\nMethod Function Description Best for\nNaive naive() Uses last observation as forecast Random walk data, benchmarks\nSeasonal Naive snaive() Uses observation from m periods ago Strong seasonal patterns\nRandom Walk with Drift rw(drift=true) Naive + linear trend Trending data\nMean meanf() Uses sample mean as forecast Stationary data\n\n","category":"section"},{"location":"naive/#Formula-Interface-(primary-usage)","page":"Naive Methods","title":"Formula Interface (primary usage)","text":"","category":"section"},{"location":"naive/#Example-1:-Naive-forecast","page":"Naive Methods","title":"Example 1: Naive forecast","text":"using Durbyn, Durbyn.Grammar\n\ndata = (sales = [120, 135, 148, 152, 141, 158, 170, 165, 180, 195],)\n\n# Naive: forecast = last observation\nspec = NaiveSpec(@formula(sales = naive_term()))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"naive/#Example-2:-Seasonal-naive-forecast","page":"Naive Methods","title":"Example 2: Seasonal naive forecast","text":"using Durbyn, Durbyn.Grammar\n\n# Monthly sales data with yearly seasonality\ndata = (sales = [100, 110, 125, 140, 155, 170,\n                 160, 150, 135, 120, 105, 95,\n                 105, 115, 130, 145, 160, 175,\n                 165, 155, 140, 125, 110, 100],)\n\n# Seasonal naive: forecast = value from same month last year\nspec = SnaiveSpec(@formula(sales = snaive_term()), m = 12)\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"naive/#Example-3:-Random-walk-with-drift","page":"Naive Methods","title":"Example 3: Random walk with drift","text":"using Durbyn, Durbyn.Grammar\n\n# Trending data\ndata = (value = cumsum(randn(50) .+ 0.5),)\n\n# RW with drift: forecast = last value + h * average change\nspec = RwSpec(@formula(value = rw_term(drift = true)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 10)\n\n# Access drift coefficient\nfitted.fit.drift      # Drift value\nfitted.fit.drift_se   # Drift standard error","category":"section"},{"location":"naive/#Example-4:-Mean-forecast","page":"Naive Methods","title":"Example 4: Mean forecast","text":"using Durbyn, Durbyn.Grammar\n\n# Stationary data around a mean\ndata = (temp = 20.0 .+ randn(100),)\n\n# Mean: forecast = sample mean for all horizons\nspec = MeanfSpec(@formula(temp = meanf_term()))\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)\n\n# Access mean\nfitted.fit.mu_original  # Mean on original scale","category":"section"},{"location":"naive/#Example-5:-Model-comparison","page":"Naive Methods","title":"Example 5: Model comparison","text":"using Durbyn, Durbyn.Grammar\n\ndata = (y = [100, 105, 102, 108, 115, 120, 118, 125, 130, 128],)\n\n# Compare naive methods\nnaive_spec = NaiveSpec(@formula(y = naive_term()))\nrw_spec = RwSpec(@formula(y = rw_term(drift = true)))\nmean_spec = MeanfSpec(@formula(y = meanf_term()))\n\nnaive_fit = fit(naive_spec, data)\nrw_fit = fit(rw_spec, data)\nmean_fit = fit(mean_spec, data, m = 1)\n\n# Compare residual variance\nnaive_fit.fit.sigma2  # Naive residual variance\nrw_fit.fit.sigma2     # RW with drift residual variance","category":"section"},{"location":"naive/#Example-6:-Panel-data-/-grouped-fitting","page":"Naive Methods","title":"Example 6: Panel data / grouped fitting","text":"using Durbyn, Durbyn.TableOps, Durbyn.ModelSpecs, Durbyn.Grammar\n\n# Stacked table with :product column\npanel = PanelData(tbl; groupby = :product, date = :date, m = 12)\n\n# Fit seasonal naive to each group\nspec = SnaiveSpec(@formula(sales = snaive_term()))\nfitted = fit(spec, panel)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"naive/#Example-7:-Box-Cox-transformation","page":"Naive Methods","title":"Example 7: Box-Cox transformation","text":"using Durbyn, Durbyn.Grammar\n\n# Positive data with increasing variance\ndata = (sales = exp.(cumsum(randn(50) .* 0.1 .+ 0.05)),)\n\n# Apply log transformation (lambda = 0) with bias adjustment\nspec = NaiveSpec(@formula(sales = naive_term()), lambda = 0.0, biasadj = true)\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n","category":"section"},{"location":"naive/#Base-API-(array-interface)","page":"Naive Methods","title":"Base API (array interface)","text":"using Durbyn\n\ny = randn(100)\n\n# Naive forecast\nfit_naive = naive(y)\nfc = forecast(fit_naive; h = 12)\n\n# Seasonal naive (monthly data)\ny_seasonal = repeat([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 3)\nfit_snaive = snaive(y_seasonal, 12)\nfc = forecast(fit_snaive; h = 24)\n\n# Random walk with drift\ny_trend = cumsum(randn(100))\nfit_rw = rw(y_trend; drift = true)\nfc = forecast(fit_rw; h = 10)\n\n# Mean forecast\nfit_mean = meanf(y, 1)\nfc = forecast(fit_mean, 12)\n\nNaiveFit exposes fitted, residuals, sigma2, lag, drift, drift_se, and optional Box-Cox parameters (lambda, biasadj).\n\n","category":"section"},{"location":"naive/#Methodology","page":"Naive Methods","title":"Methodology","text":"","category":"section"},{"location":"naive/#Naive-Method","page":"Naive Methods","title":"Naive Method","text":"The naive forecast uses the last observed value as the forecast for all future horizons:\n\nhaty_T+hT = y_T quad textfor all  h = 1 2 ldots\n\nThis is equivalent to a random walk model without drift.","category":"section"},{"location":"naive/#Seasonal-Naive-Method","page":"Naive Methods","title":"Seasonal Naive Method","text":"The seasonal naive method uses the observation from the same season in the previous cycle:\n\nhaty_T+hT = y_T+h-m cdot k\n\nwhere m is the seasonal period and k = lceil hm rceil is the number of complete seasonal cycles back.\n\nFor example, with monthly data (m = 12), the forecast for January next year uses the value from January this year.","category":"section"},{"location":"naive/#Random-Walk-with-Drift","page":"Naive Methods","title":"Random Walk with Drift","text":"The random walk with drift includes a linear trend based on the average historical change:\n\nhaty_T+hT = y_T + h cdot b\n\nwhere the drift term b is estimated as the mean of first differences:\n\nb = frac1nsum_t=2^T (y_t - y_t-1)\n\nwhere n is the number of valid consecutive pairs. This approach is more robust than the endpoint formula (y_T - y_1)(T-1) because it uses all available consecutive pairs rather than just endpoints, making it more reliable when data has missing values or gaps.","category":"section"},{"location":"naive/#Mean-Method","page":"Naive Methods","title":"Mean Method","text":"The mean method uses the historical average as the forecast:\n\nhaty_T+hT = bary = frac1Tsum_t=1^T y_t\n\nThis assumes the data fluctuates around a constant mean with no trend or seasonality.\n\n","category":"section"},{"location":"naive/#Prediction-Intervals","page":"Naive Methods","title":"Prediction Intervals","text":"Naive methods produce prediction intervals that widen with the forecast horizon, reflecting increasing uncertainty over time.\n\nResidual variance (σ²): The residual variance used for prediction intervals is computed as the mean squared error (MSE) of residuals without centering: σ² = frac1nsum_t e_t^2. This matches R's forecast package behavior.","category":"section"},{"location":"naive/#Naive-/-Random-Walk-(without-drift)","page":"Naive Methods","title":"Naive / Random Walk (without drift)","text":"Forecast variance grows linearly with horizon:\n\ntextVar(haty_T+hT) = h cdot sigma^2\n\nThe standard error is:\n\ntextSE(h) = sqrth cdot sigma","category":"section"},{"location":"naive/#Seasonal-Naive","page":"Naive Methods","title":"Seasonal Naive","text":"Variance increases with the number of complete seasonal cycles:\n\ntextVar(haty_T+hT) = lceil hm rceil cdot sigma^2\n\nwhere m is the seasonal period. The variance increases in steps each time a new seasonal cycle is entered.","category":"section"},{"location":"naive/#Random-Walk-with-Drift-2","page":"Naive Methods","title":"Random Walk with Drift","text":"Includes additional uncertainty from the drift estimate:\n\ntextVar(haty_T+hT) = h cdot sigma^2 + h^2 cdot textSE(b)^2\n\nwhere textSE(b) = s_d  sqrtn is the standard error of the drift coefficient, s_d is the standard deviation of the first differences, and n is the number of valid consecutive pairs.","category":"section"},{"location":"naive/#Mean-Method-2","page":"Naive Methods","title":"Mean Method","text":"Uses the t-distribution for intervals:\n\nhaty_T+hT pm t_1-alpha2 T-1 cdot s cdot sqrt1 + 1T\n\nwhere s is the sample standard deviation and T is the number of observations.\n\n","category":"section"},{"location":"naive/#When-to-Use-Each-Method","page":"Naive Methods","title":"When to Use Each Method","text":"Scenario Recommended Method\nNo clear pattern, random fluctuations naive()\nStrong, stable seasonal pattern snaive()\nClear upward or downward trend rw(drift=true)\nStationary data around a level meanf()\nBenchmark for complex models Any naive method\n\ninfo: Benchmarking Best Practice\nAlways compare your forecast model against naive benchmarks. If a sophisticated model cannot beat the seasonal naive method, consider whether the added complexity is justified.\n\n","category":"section"},{"location":"naive/#Missing-Value-Handling","page":"Naive Methods","title":"Missing Value Handling","text":"All naive methods handle missing values gracefully:\n\nLeading missings: Skipped when finding the starting point\nTrailing missings: The last valid observation is used\nScattered missings: Converted to NaN internally; fitted values use forward-fill from the most recent valid lagged value\n\ny = [1.0, missing, 3.0, 4.0, missing, 6.0]\nfit = naive(y)  # Uses 6.0 (last valid) for forecasts\n# fit.fitted[3] = 1.0 (lagged from y[1], since y[2] is missing)\n\nForward-fill behavior: When computing fitted values, if the lagged value is missing, the method forward-fills from the most recent valid fitted value. This matches the R forecast package's lagwalk() behavior and ensures fitted values are available wherever actual values exist, even when there are gaps in the series.\n\nFor seasonal naive, if a particular seasonal position has missing values, the method searches backwards through prior seasonal cycles to find the most recent valid observation at that position.\n\n","category":"section"},{"location":"naive/#Box-Cox-Transformation","page":"Naive Methods","title":"Box-Cox Transformation","text":"All naive methods support Box-Cox transformation for variance stabilization:\n\n# Log transformation (lambda = 0)\nfit = naive(y; lambda = 0.0)\n\n# Square root transformation (lambda = 0.5)\nfit = naive(y; lambda = 0.5)\n\n# Automatic lambda selection\nfit = naive(y; lambda = \"auto\")\n\n# With bias adjustment for back-transformation\nfit = naive(y; lambda = 0.0, biasadj = true)\n\nLambda requirements:\n\nλ  0: Requires strictly positive values (non-positive values are treated as missing with a warning)\nλ  0: Allows negative values and zeros via signed transformation\nFor x=0: the transform produces -1λ (finite value)\n\nThe biasadj option applies a bias correction when transforming forecasts back to the original scale, which can improve accuracy for skewed distributions. When enabled, bias adjustment is applied to both fitted values and point forecasts.\n\n","category":"section"},{"location":"naive/#References","page":"Naive Methods","title":"References","text":"Hyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3rd ed.). OTexts. https://otexts.com/fpp3/\nMakridakis, S., Wheelwright, S. C., & Hyndman, R. J. (1998). Forecasting: Methods and Applications (3rd ed.). John Wiley & Sons.","category":"section"},{"location":"stats/#Statistics-Module","page":"Statistics","title":"Statistics Module","text":"The Stats module provides a comprehensive toolkit for time series analysis, including decomposition methods, transformation functions, autocorrelation analysis, and unit root tests. These functions are essential for preprocessing, analyzing, and understanding time series data before fitting forecasting models.\n\n","category":"section"},{"location":"stats/#Overview","page":"Statistics","title":"Overview","text":"The Stats module exports the following functions and types:\n\nCategory Functions/Types\nTransformations box_cox, box_cox!, box_cox_lambda, inv_box_cox\nDecomposition decompose, DecomposedTimeSeries, stl, STLResult, mstl, MSTLResult\nDifferencing diff, ndiffs, nsdiffs\nAutocorrelation acf, pacf, ACFResult, PACFResult\nUnit Root Tests adf, ADF, kpss, KPSS, phillips_perron, PhillipsPerron, ocsb, OCSB\nMissing Values handle_missing, longest_contiguous, interpolate_missing, check_missing, MissingMethod, Contiguous, Interpolate, FailMissing\nUtilities fourier, embed, ols, OlsFit, approx, approxfun, seasonal_strength\n\n","category":"section"},{"location":"stats/#Box-Cox-Transformations","page":"Statistics","title":"Box-Cox Transformations","text":"Box-Cox transformations stabilize variance and make data more normally distributed, which can improve forecast accuracy.","category":"section"},{"location":"stats/#Mathematical-Formulation","page":"Statistics","title":"Mathematical Formulation","text":"The Box-Cox transformation is defined as:\n\ny^(lambda) = begincases\nfracy^lambda - 1lambda  textif  lambda neq 0 \nlog(y)  textif  lambda = 0\nendcases","category":"section"},{"location":"stats/#box_cox_lambda","page":"Statistics","title":"box_cox_lambda","text":"Automatically select the optimal Box-Cox transformation parameter.\n\nbox_cox_lambda(x, m; method=\"guerrero\", lower=-1, upper=2)\n\nArguments:\n\nx::AbstractVector: A numeric vector (must be positive for Guerrero method)\nm::Int: Frequency of the data\nmethod::String: Selection method - \"guerrero\" (default) or \"loglik\"\nlower::Float64: Lower bound for λ search (default: -1)\nupper::Float64: Upper bound for λ search (default: 2)\n\nMethods:\n\nGuerrero: Minimizes the coefficient of variation for subseries (Guerrero, 1993)\nLog-likelihood: Maximizes the profile log likelihood of a linear model\n\nExample:\n\nusing Durbyn.Stats\n\ny = [120, 135, 148, 152, 141, 158, 170, 165, 180, 195]\nlambda = box_cox_lambda(y, 12, method=\"guerrero\")","category":"section"},{"location":"stats/#box_cox","page":"Statistics","title":"box_cox","text":"Apply Box-Cox transformation to a series.\n\nbox_cox(x, m; lambda=\"auto\")\n\nArguments:\n\nx::AbstractVector: Input vector\nm::Int: Frequency\nlambda: Transformation parameter or \"auto\" for automatic selection\n\nReturns: Tuple (transformed_vector, lambda_used)\n\nExample:\n\ny_transformed, lambda = box_cox(y, 12; lambda=\"auto\")","category":"section"},{"location":"stats/#box_cox!","page":"Statistics","title":"box_cox!","text":"In-place Box-Cox transformation for memory efficiency.\n\nbox_cox!(output, x, m; lambda)\n\nNote: Use this in tight loops where box_cox is called repeatedly to avoid allocations.","category":"section"},{"location":"stats/#inv_box_cox","page":"Statistics","title":"inv_box_cox","text":"Reverse the Box-Cox transformation.\n\ninv_box_cox(x; lambda, biasadj=false, fvar=nothing)\n\nArguments:\n\nx::AbstractArray: Transformed data\nlambda::Real: Transformation parameter used\nbiasadj::Bool: Apply bias adjustment for mean forecasts (default: false)\nfvar: Forecast variance (required if biasadj=true)\n\nExample:\n\ny_original = inv_box_cox(y_transformed; lambda=0.5)\n\n# With bias adjustment for forecasts\ny_mean = inv_box_cox(y_transformed; lambda=0.5, biasadj=true, fvar=forecast_variance)","category":"section"},{"location":"stats/#References","page":"Statistics","title":"References","text":"Box, G. E. P. and Cox, D. R. (1964). An analysis of transformations. JRSS B, 26, 211-246.\nGuerrero, V.M. (1993). Time-series analysis supported by power transformations. Journal of Forecasting, 12, 37-48.\nBickel, P. J. and Doksum K. A. (1981). An Analysis of Transformations Revisited. JASA, 76, 296-311.\n\n","category":"section"},{"location":"stats/#Time-Series-Decomposition","page":"Statistics","title":"Time Series Decomposition","text":"","category":"section"},{"location":"stats/#Classical-Decomposition-(decompose)","page":"Statistics","title":"Classical Decomposition (decompose)","text":"Decompose a time series into trend, seasonal, and residual components using moving averages.\n\ndecompose(x; m, type=\"additive\", filter=nothing)\n\nArguments:\n\nx::AbstractVector: Time series vector\nm::Int: Frequency (observations per cycle)\ntype::String: \"additive\" or \"multiplicative\"\nfilter: Custom filter coefficients (optional)\n\nReturns: DecomposedTimeSeries struct with fields:\n\nx: Original series\nseasonal: Seasonal component\ntrend: Trend component\nrandom: Residual/remainder\nfigure: Seasonal figure\ntype: Decomposition type\nm: Frequency\n\nExample:\n\nusing Durbyn.Stats\n\nap = air_passengers()\nresult = decompose(ap; m=12, type=\"multiplicative\")\nresult.trend\nresult.seasonal","category":"section"},{"location":"stats/#STL-Decomposition-(stl)","page":"Statistics","title":"STL Decomposition (stl)","text":"Seasonal-Trend decomposition using LOESS (STL) - a robust and flexible method for decomposing time series.\n\nstl(x, m; s_window, s_degree=0, t_window=nothing, t_degree=1,\n    l_window=nothing, l_degree=t_degree, s_jump=nothing, t_jump=nothing,\n    l_jump=nothing, robust=false, inner=nothing, outer=nothing)\n\nArguments:\n\nx::AbstractVector: Time series to decompose\nm::Int: Seasonal frequency (must be ≥ 2)\ns_window: Seasonal smoothing window (integer or \"periodic\")\ns_degree::Int: Seasonal smoothing polynomial degree (0 or 1)\nt_window: Trend smoothing window\nt_degree::Int: Trend smoothing polynomial degree (0 or 1)\nl_window: Low-pass filter window\nl_degree::Int: Low-pass filter polynomial degree\ns_jump, t_jump, l_jump: Subsampling steps\nrobust::Bool: Use robustness iterations (default: false)\ninner, outer: Inner/outer iteration counts\n\nReturns: STLResult struct with:\n\ntime_series: NamedTuple with :seasonal, :trend, :remainder\nweights: Robustness weights\nwindows: (s, t, l) window sizes\ndegrees: (s, t, l) polynomial degrees\njumps: (s, t, l) jump parameters\ninner, outer: Iteration counts\n\nExample:\n\nap = air_passengers()\nresult = stl(ap, 12; s_window=7, robust=true)\n\n# Access components\nresult.time_series.trend\nresult.time_series.seasonal\nresult.time_series.remainder\n\n# Summarize and plot\nsummary(result)\nplot(result)","category":"section"},{"location":"stats/#Multiple-Seasonal-Decomposition-(mstl)","page":"Statistics","title":"Multiple Seasonal Decomposition (mstl)","text":"Decompose time series with multiple seasonal periods using iterative STL.\n\nmstl(x, m; lambda=nothing, iterate=2, s_window=nothing, stl_kwargs...)\n\nArguments:\n\nx::AbstractVector: Time series\nm: Single period (Int) or vector of periods\nlambda: Box-Cox parameter (nothing, \"auto\", or numeric)\niterate::Int: Number of outer iterations (default: 2)\ns_window: Seasonal window(s)\nstl_kwargs...: Additional arguments passed to stl\n\nReturns: MSTLResult struct with:\n\ndata: Original series\ntrend: Trend component\nseasonals: Vector of seasonal components\nm: Periods used\nremainder: Residual component\nlambda: Box-Cox λ used\n\nExample:\n\n# Hourly data with daily and weekly patterns\ny = rand(200) .+ 2sin.(2π*(1:200)/7) .+ 0.5sin.(2π*(1:200)/30)\nresult = mstl(y; m=[7, 30], iterate=2, s_window=[11, 23], robust=true)\n\n# Access components\nresult.trend\nresult.seasonals[1]  # First seasonal component (period 7)\nresult.seasonals[2]  # Second seasonal component (period 30)\nresult.remainder\n\n# Summarize\nsummary(result)","category":"section"},{"location":"stats/#Seasonal-Strength","page":"Statistics","title":"Seasonal Strength","text":"Measure the strength of seasonality in an MSTL decomposition.\n\nseasonal_strength(x; m, kwargs...)\nseasonal_strength(res::MSTLResult)\n\nThe seasonal strength is computed as:\n\ntextstrength = 1 - fractextVar(textremainder)textVar(textremainder + textseasonal)\n\nValues range from 0 (no seasonality) to 1 (strong seasonality).\n\nExample:\n\nresult = mstl(y; m=[7, 30])\nstrength = seasonal_strength(result)\n\n","category":"section"},{"location":"stats/#Autocorrelation-Functions","page":"Statistics","title":"Autocorrelation Functions","text":"","category":"section"},{"location":"stats/#ACF-(acf)","page":"Statistics","title":"ACF (acf)","text":"Compute the sample autocorrelation function.\n\nacf(y, m, nlags=nothing; demean=true)\n\nArguments:\n\ny::AbstractVector: Input time series\nm::Int: Frequency/seasonal period\nnlags: Number of lags (default: min(10*log10(n), n-1))\ndemean::Bool: Subtract mean before computing (default: true)\n\nReturns: ACFResult with:\n\nvalues: ACF values at each lag (including lag 0)\nlags: Lag indices\nn: Series length\nm: Frequency\nci: 95% confidence interval (±1.96/√n)\n\nFormula:\n\nhatrho(k) = fracsum_t=1^n-k (y_t - bary)(y_t+k - bary)sum_t=1^n (y_t - bary)^2\n\nExample:\n\ny = randn(100)\nresult = acf(y, 12)\nresult.values  # ACF values\nresult.ci      # Confidence interval\n\nplot(result)   # Requires Plots.jl","category":"section"},{"location":"stats/#PACF-(pacf)","page":"Statistics","title":"PACF (pacf)","text":"Compute the sample partial autocorrelation function using the Durbin-Levinson algorithm.\n\npacf(y, m, nlags=nothing)\n\nArguments:\n\ny::AbstractVector: Input time series\nm::Int: Frequency/seasonal period\nnlags: Number of lags (default: min(10*log10(n), n-1))\n\nReturns: PACFResult with:\n\nvalues: PACF values (lags 1 to nlags)\nlags: Lag indices\nn: Series length\nm: Frequency\nci: 95% confidence interval\n\nExample:\n\ny = randn(100)\nresult = pacf(y, 12)\nresult.values\nplot(result)\n\n","category":"section"},{"location":"stats/#Differencing","page":"Statistics","title":"Differencing","text":"","category":"section"},{"location":"stats/#diff","page":"Statistics","title":"diff","text":"Compute lagged differences of a vector or matrix.\n\ndiff(x; lag=1, differences=1)\n\nArguments:\n\nx: Vector or matrix\nlag::Int: Lag interval (default: 1)\ndifferences::Int: Number of times to apply differencing (default: 1)\n\nExample:\n\ny = [1, 3, 6, 10, 15]\ndiff(y)                    # [2, 3, 4, 5]\ndiff(y; lag=2)             # [5, 7, 9]\ndiff(y; differences=2)     # [1, 1, 1]","category":"section"},{"location":"stats/#ndiffs","page":"Statistics","title":"ndiffs","text":"Determine the number of non-seasonal differences needed for stationarity.\n\nndiffs(x; alpha=0.05, test=:kpss, deterministic=:level, maxd=2, kwargs...)\n\nArguments:\n\nx::AbstractVector: Time series\nalpha::Float64: Significance level (clamped to [0.01, 0.10])\ntest::Symbol: Unit root test - :kpss, :adf, or :pp\ndeterministic::Symbol: :level (intercept) or :trend (intercept + trend)\nmaxd::Int: Maximum differences to try (default: 2)\n\nTest Behavior:\n\nKPSS: Null = stationarity. Returns smallest d where KPSS does not reject.\nADF/PP: Null = unit root. Returns smallest d where test rejects unit root.\n\nExample:\n\ny = cumsum(randn(100))  # Random walk (non-stationary)\nd = ndiffs(y; test=:kpss)\nprintln(\"Differences needed: $d\")\n\n# Using ADF test\nd_adf = ndiffs(y; test=:adf, deterministic=:trend)","category":"section"},{"location":"stats/#nsdiffs","page":"Statistics","title":"nsdiffs","text":"Determine the number of seasonal differences needed.\n\nnsdiffs(x, m; alpha=0.05, test=:seas, maxD=1, kwargs...)\n\nArguments:\n\nx::AbstractVector: Time series\nm::Int: Seasonal period\nalpha::Float64: Significance level\ntest::Symbol: :seas (default) or :ocsb\nmaxD::Int: Maximum seasonal differences (default: 1)\n\nExample:\n\nap = air_passengers()\nD = nsdiffs(ap, 12)\nprintln(\"Seasonal differences needed: $D\")\n\n","category":"section"},{"location":"stats/#Unit-Root-Tests","page":"Statistics","title":"Unit Root Tests","text":"","category":"section"},{"location":"stats/#ADF-Test-(adf)","page":"Statistics","title":"ADF Test (adf)","text":"Augmented Dickey-Fuller test for unit roots.\n\nadf(y; type=:none, lags=1, selectlags=:fixed)\n\nNull Hypothesis: The series has a unit root (non-stationary)\n\nArguments:\n\ny::AbstractVector: Time series\ntype::Symbol: :none, :drift (intercept), or :trend (intercept + trend)\nlags::Int: Maximum augmentation order\nselectlags::Symbol: :fixed, :aic, or :bic\n\nReturns: ADF struct with:\n\nmodel: Test type used\ncval: Critical values matrix\nclevels: Significance levels [0.01, 0.05, 0.10]\nlag: Selected augmentation order\nteststat: Test statistics (τ-statistics)\ntestreg: Auxiliary regression results\nres: Residuals\n\nExample:\n\ny = cumsum(randn(100))\nresult = adf(y; type=:drift, lags=4, selectlags=:aic)\nprintln(\"Test statistic: $(result.teststat[1])\")\nprintln(\"Critical values: $(result.cval)\")","category":"section"},{"location":"stats/#KPSS-Test-(kpss)","page":"Statistics","title":"KPSS Test (kpss)","text":"Kwiatkowski-Phillips-Schmidt-Shin test for stationarity.\n\nkpss(y; type=:mu, lags=:short, use_lag=nothing)\n\nNull Hypothesis: The series is stationary\n\nArguments:\n\ny::AbstractVector: Time series\ntype::Symbol: :mu (constant) or :tau (constant + trend)\nlags::Symbol: :short, :long, or :nil\nuse_lag: Manually specify bandwidth\n\nReturns: KPSS struct with:\n\ntype: Test type\nlag: Bandwidth used\nteststat: Test statistic\ncval, clevels: Critical values and levels\nres: Regression residuals\n\nExample:\n\ny = randn(100)  # Stationary\nresult = kpss(y; type=:mu)\nprintln(\"Test statistic: $(result.teststat)\")\nprintln(\"Critical values: $(result.cval)\")","category":"section"},{"location":"stats/#Phillips-Perron-Test-(phillips_perron)","page":"Statistics","title":"Phillips-Perron Test (phillips_perron)","text":"Phillips-Perron unit root test with non-parametric correction.\n\nphillips_perron(x; type=:Z_alpha, model=:constant, lags=:short, use_lag=nothing)\n\nNull Hypothesis: The series has a unit root\n\nArguments:\n\nx::AbstractVector: Time series\ntype::Symbol: :Z_alpha or :Z_tau\nmodel::Symbol: :constant or :trend\nlags::Symbol: :short or :long\nuse_lag: Bartlett truncation lag\n\nReturns: PhillipsPerron struct with test results.\n\nExample:\n\nresult = phillips_perron(y; type=:Z_tau, model=:trend)","category":"section"},{"location":"stats/#OCSB-Test-(ocsb)","page":"Statistics","title":"OCSB Test (ocsb)","text":"Osborn-Chui-Smith-Birchenhall test for seasonal unit roots.\n\nocsb(x, m; lag_method=:fixed, maxlag=0, clevels=[0.10, 0.05, 0.01])\n\nNull Hypothesis: Seasonal unit root exists\n\nArguments:\n\nx::AbstractVector: Time series\nm::Int: Seasonal period\nlag_method::Symbol: :fixed, :AIC, :BIC, or :AICc\nmaxlag::Int: Maximum AR order to consider\n\nReturns: OCSB struct with:\n\nteststat: OCSB t-statistic\ncval, clevels: Critical values and levels\nlag: Selected AR order\n\nExample:\n\nap = air_passengers()\nresult = ocsb(ap, 12; lag_method=:AIC)\n\n","category":"section"},{"location":"stats/#Utility-Functions","page":"Statistics","title":"Utility Functions","text":"","category":"section"},{"location":"stats/#Fourier-Terms-(fourier)","page":"Statistics","title":"Fourier Terms (fourier)","text":"Generate Fourier terms for seasonal modeling in regression.\n\nfourier(x; m, K, h=nothing)\n\nArguments:\n\nx::AbstractVector: Time series\nm: Seasonal period\nK::Int: Number of Fourier terms\nh: Forecast horizon (optional)\n\nReturns: Matrix of sin/cos terms\n\nExample:\n\ny = randn(100)\nF = fourier(y; m=12, K=6)\n# Use F as regressors in ARIMA with external regressors","category":"section"},{"location":"stats/#Time-Delay-Embedding-(embed)","page":"Statistics","title":"Time-Delay Embedding (embed)","text":"Create a time-delay embedding matrix (lag matrix).\n\nembed(x, dimension=1)\n\nArguments:\n\nx: Vector or matrix\ndimension::Int: Embedding dimension\n\nReturns: Matrix with lags in descending order (compatible with R's embed)\n\nExample:\n\ny = [1, 2, 3, 4, 5]\nembed(y, 3)\n# Returns:\n# 3  2  1\n# 4  3  2\n# 5  4  3","category":"section"},{"location":"stats/#Ordinary-Least-Squares-(ols)","page":"Statistics","title":"Ordinary Least Squares (ols)","text":"Fit OLS linear regression.\n\nols(y, X)\n\nArguments:\n\ny::AbstractVector: Response vector\nX::AbstractMatrix: Design matrix (include intercept column if needed)\n\nReturns: OlsFit struct with:\n\ncoef: Estimated coefficients\nfitted: Fitted values\nresiduals: Residuals\nsigma2: Residual variance\ncov: Coefficient covariance matrix\nse: Standard errors\ndf_residual: Residual degrees of freedom\n\nExample:\n\nn = 100\nX = hcat(ones(n), randn(n))  # Intercept + predictor\ny = X * [2.0, 3.0] + randn(n) * 0.5\n\nfit = ols(y, X)\nfit.coef       # Coefficients\nfit.se         # Standard errors\nfit.residuals  # Residuals\n\n# Predictions\npredict(fit, X_new)","category":"section"},{"location":"stats/#Interpolation-(approx,-approxfun)","page":"Statistics","title":"Interpolation (approx, approxfun)","text":"Linear or constant interpolation of data.\n\napprox(x, y; xout=nothing, method=:linear, n=50, yleft=nothing,\n       yright=nothing, rule=(1,1), f=0.0, ties=mean, na_rm=true)\n\nArguments:\n\nx, y: Coordinates to interpolate\nxout: Output grid points (default: n equally spaced)\nmethod: :linear or :constant\nn: Number of interpolation points\nyleft, yright: Extrapolation values\nrule: (1,1) for missing at boundaries, (2,2) for boundary values\nf: For :constant, controls step function continuity\nties: Function to collapse duplicate x values\n\nReturns: NamedTuple (x=xout_vec, y=yout_vec)\n\nExample:\n\nx = [1, 2, 4, 5]\ny = [2, 4, 6, 8]\nresult = approx(x, y; n=10)\n\n# Create interpolation function\nf = approxfun(x, y)\nf(3)  # Interpolate at x=3\n\n","category":"section"},{"location":"stats/#Missing-Value-Handling","page":"Statistics","title":"Missing Value Handling","text":"Durbyn provides a type-dispatched system for handling missing values (missing and NaN) in time series data.","category":"section"},{"location":"stats/#Type-Hierarchy","page":"Statistics","title":"Type Hierarchy","text":"All missing value strategies are subtypes of the abstract type MissingMethod:\n\nType Description\nContiguous() Extract the longest contiguous segment without missing values\nInterpolate() Interpolate missing values (seasonal-aware)\nInterpolate(; linear=true) Force linear interpolation\nFailMissing() Throw an error if any missing values are present","category":"section"},{"location":"stats/#handle_missing","page":"Statistics","title":"handle_missing","text":"Dispatch to the appropriate strategy based on the MissingMethod type.\n\nhandle_missing(x, Contiguous())           # longest contiguous segment\nhandle_missing(x, Interpolate(); m=12)    # seasonal interpolation\nhandle_missing(x, FailMissing())          # error if missing\n\nArguments:\n\nx::AbstractArray: Input vector (may contain missing or NaN)\nmethod::MissingMethod: Strategy to use\nm::Union{Int,Nothing}: Seasonal period (used by Interpolate)\n\nExample:\n\nusing Durbyn.Stats\n\nx = [1.0, 2.0, missing, 4.0, 5.0]\n\n# Type dispatch replaces string dispatch\nresult = handle_missing(x, Contiguous())\nresult = handle_missing(x, Interpolate())\nresult = handle_missing(x, FailMissing())  # throws ArgumentError","category":"section"},{"location":"stats/#longest_contiguous","page":"Statistics","title":"longest_contiguous","text":"Extract the longest contiguous segment of non-missing values.\n\nlongest_contiguous(x)\n\nBoth missing and NaN are treated as missing values.\n\nExample:\n\nx = [missing, 1.0, 2.0, 3.0, missing, 4.0, missing]\nlongest_contiguous(x)  # Returns [1.0, 2.0, 3.0]","category":"section"},{"location":"stats/#interpolate_missing","page":"Statistics","title":"interpolate_missing","text":"Interpolate missing values in a time series. For seasonal data, uses STL decomposition with seasonal-aware interpolation. For non-seasonal data, uses linear interpolation.\n\ninterpolate_missing(x; m=nothing, lambda=nothing, linear=nothing)\n\nArguments:\n\nx::AbstractVector: Time series (may contain missing or NaN)\nm::Union{Int,Nothing}: Seasonal period (nothing or 1 = non-seasonal)\nlambda::Union{Nothing,Real}: Box-Cox transformation parameter\nlinear::Union{Nothing,Bool}: Force linear interpolation\n\nAlgorithm (seasonal):\n\nFit preliminary model with Fourier terms and polynomial trend\nApply robust MSTL decomposition\nLinearly interpolate the seasonally adjusted series\nAdd back the seasonal component\nFall back to linear if results are unstable\n\nExample:\n\n# Non-seasonal\ny = [1.0, 2.0, missing, 4.0, 5.0]\ninterpolate_missing(y)\n\n# Seasonal (monthly)\nap = air_passengers()\nap[50] = NaN\ninterpolate_missing(ap; m=12)\n\n# With Box-Cox\ninterpolate_missing(y; lambda=0.5)","category":"section"},{"location":"stats/#check_missing","page":"Statistics","title":"check_missing","text":"Verify that a vector contains no missing values. Returns the input unchanged if clean; throws ArgumentError otherwise.\n\ncheck_missing(x)\n\nExample:\n\ncheck_missing([1.0, 2.0, 3.0])         # Returns [1.0, 2.0, 3.0]\ncheck_missing([1.0, missing, 3.0])      # Throws ArgumentError\ncheck_missing([1.0, NaN, 3.0])          # Throws ArgumentError","category":"section"},{"location":"stats/#Usage-in-ETS","page":"Statistics","title":"Usage in ETS","text":"The ets() function accepts a missing_method keyword argument:\n\nusing Durbyn.ExponentialSmoothing\n\nap = air_passengers()\n\n# Default: extract longest contiguous segment\nfit1 = ets(ap, 12, \"ZZZ\")\n\n# Interpolate missing values\nfit2 = ets(ap, 12, \"ZZZ\"; missing_method=Interpolate())\n\n# Error on missing values\nfit3 = ets(ap, 12, \"ZZZ\"; missing_method=FailMissing())\n\n","category":"section"},{"location":"stats/#Complete-Example:-Time-Series-Preprocessing-Pipeline","page":"Statistics","title":"Complete Example: Time Series Preprocessing Pipeline","text":"using Durbyn\nusing Durbyn.Stats\n\n# Load data\nap = air_passengers()\n\n# 1. Check for stationarity and determine differencing\nd = ndiffs(ap; test=:kpss)\nD = nsdiffs(ap, 12)\nprintln(\"Non-seasonal differences: $d, Seasonal differences: $D\")\n\n# 2. Apply Box-Cox transformation\ny_transformed, lambda = box_cox(ap, 12; lambda=\"auto\")\nprintln(\"Box-Cox lambda: $lambda\")\n\n# 3. Decompose the series\nresult = stl(ap, 12; s_window=7, robust=true)\nsummary(result)\n\n# 4. Examine autocorrelation structure\nacf_result = acf(ap, 12, 24)\npacf_result = pacf(ap, 12, 24)\n\n# 5. Multiple seasonal decomposition (if applicable)\nmstl_result = mstl(ap; m=12, lambda=lambda)\nstrength = seasonal_strength(mstl_result)\nprintln(\"Seasonal strength: $strength\")\n\n# 6. Perform unit root tests\nadf_result = adf(ap; type=:drift, selectlags=:aic)\nkpss_result = kpss(ap; type=:mu)\n\nprintln(\"ADF test statistic: $(adf_result.teststat[1])\")\nprintln(\"KPSS test statistic: $(kpss_result.teststat)\")\n\n","category":"section"},{"location":"stats/#References-2","page":"Statistics","title":"References","text":"Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). STL: A Seasonal-Trend Decomposition Procedure Based on Loess. Journal of Official Statistics, 6(1), 3-73.\nDickey, D. A., & Fuller, W. A. (1979). Distribution of the Estimators for Autoregressive Time Series with a Unit Root. JASA, 74, 427-431.\nKwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). Testing the Null Hypothesis of Stationarity Against the Alternative of a Unit Root. Journal of Econometrics, 54, 159-178.\nPhillips, P. C. B., & Perron, P. (1988). Testing for a Unit Root in Time Series Regression. Biometrika, 72(2), 335-346.\nOsborn, D. R., Chui, A. P. L., Smith, J. P., & Birchenhall, C. R. (1988). Seasonality and the Order of Integration for Consumption. Oxford Bulletin of Economics and Statistics, 50, 361-377.\nBox, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time Series Analysis: Forecasting and Control. Wiley.","category":"section"},{"location":"tbats/#TBATS:-Trigonometric-Seasonal-Exponential-Smoothing","page":"TBATS","title":"TBATS: Trigonometric Seasonal Exponential Smoothing","text":"TBATS (Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend, and Seasonal components) extends BATS by using a Fourier representation for seasonal components, enabling the model to handle:\n\nNon-integer seasonal periods (e.g., 52.18 weeks per year)\nVery long seasonal cycles (hundreds or thousands of periods)\nMultiple complex seasonalities (daily + weekly + yearly)\nDual calendar effects (e.g., Hijri + Gregorian calendars)\n\nThis implementation is a pure Julia port of the R forecast::tbats function based on De Livera, Hyndman & Snyder (2011).\n\n","category":"section"},{"location":"tbats/#Mathematical-Framework","page":"TBATS","title":"Mathematical Framework","text":"","category":"section"},{"location":"tbats/#1.-Box-Cox-Transformation","page":"TBATS","title":"1. Box-Cox Transformation","text":"The model begins with an optional Box-Cox transformation to stabilize variance:\n\ny_t^(omega) = begincases\nfracy_t^omega - 1omega  omega neq 0 6pt\nln(y_t)  omega = 0\nendcases\n\nwhere omega in 0 1 is the Box-Cox parameter estimated from the data.\n\n","category":"section"},{"location":"tbats/#2.-TBATS-State-Space-Model","page":"TBATS","title":"2. TBATS State Space Model","text":"The TBATS model is represented in innovations form with the following components:","category":"section"},{"location":"tbats/#**Observation-Equation**","page":"TBATS","title":"Observation Equation","text":"y_t^(omega) = ell_t-1 + phi b_t-1 + sum_i=1^T s_it-1 + d_t\n\nwhere:\n\nell_t is the local level\nb_t is the trend component\nphi is the damping parameter (0  phi leq 1)\ns_it is the i-th seasonal component\nd_t is the ARMA error term\nT is the number of seasonal periods","category":"section"},{"location":"tbats/#**State-Equations**","page":"TBATS","title":"State Equations","text":"Local Level:\n\nell_t = ell_t-1 + phi b_t-1 + alpha d_t\n\nwhere alpha is the level smoothing parameter.\n\nTrend Component:\n\nb_t = phi b_t-1 + beta d_t\n\nwhere beta is the trend smoothing parameter and phi controls damping.\n\nTrigonometric Seasonal Components:\n\nFor each seasonal period m_i with k_i Fourier harmonics:\n\ns_it = sum_j=1^k_i left s_ijt^(1) cos(lambda_ij t) + s_ijt^(2) sin(lambda_ij t) right\n\nwhere lambda_ij = frac2pi jm_i is the j-th harmonic frequency.\n\nHarmonic State Evolution:\n\nEach harmonic pair evolves via a rotation matrix:\n\nbeginpmatrix\ns_ijt^(1) \ns_ijt^(2)\nendpmatrix\n= beginpmatrix\ncoslambda_ij  sinlambda_ij \n-sinlambda_ij  coslambda_ij\nendpmatrix\nbeginpmatrix\ns_ijt-1^(1) \ns_ijt-1^(2)\nendpmatrix\n+ beginpmatrix\ngamma_1i \ngamma_2i\nendpmatrix d_t\n\nwhere gamma_1i and gamma_2i are smoothing parameters for the seasonal component.\n\nARMA Error Component:\n\nd_t = sum_k=1^p varphi_k d_t-k + varepsilon_t + sum_ell=1^q theta_ell varepsilon_t-ell\n\nwhere:\n\np is the AR order\nq is the MA order\nvarphi_k are AR coefficients\ntheta_ell are MA coefficients\nvarepsilon_t sim mathcalN(0 sigma^2)\n\n","category":"section"},{"location":"tbats/#3.-Key-Advantages-Over-BATS","page":"TBATS","title":"3. Key Advantages Over BATS","text":"Feature BATS TBATS\nSeasonal periods Integer only Non-integer allowed\nState dimension m_i states per season 2k_i states per season\nMax seasonal period ~350 (computational limit) Unlimited (via Fourier)\nDual calendars Not feasible Fully supported\nStorage O(m) O(k), typically k ll m\n\nExample: For weekly seasonality (m = 52), BATS needs 52 states, while TBATS with k=2 harmonics needs only 4 states.\n\n","category":"section"},{"location":"tbats/#4.-Forecasting","page":"TBATS","title":"4. Forecasting","text":"Future states evolve deterministically (errors set to zero):\n\nLevel:\n\nell_t+h = ell_t + sum_u=1^h phi^u-1 b_t\n\nTrend:\n\nb_t+h = phi^h b_t\n\nSeasonal Components:\n\nThe harmonic pairs rotate forward via matrix powers:\n\nbeginpmatrix\ns_ijt+h^(1) \ns_ijt+h^(2)\nendpmatrix\n= R(lambda_ij)^h\nbeginpmatrix\ns_ijt^(1) \ns_ijt^(2)\nendpmatrix\n\nwhere R(lambda) is the rotation matrix.\n\nPoint Forecast (Transformed Scale):\n\nhaty_t+h^(omega) = ell_t+h + phi b_t+h + sum_i=1^T s_it+h\n\nThe forecast is then back-transformed via inverse Box-Cox:\n\nhaty_t+h = begincases\n(omega haty_t+h^(omega) + 1)^1omega  omega neq 0 6pt\nexp(haty_t+h^(omega))  omega = 0\nendcases\n\nForecast Variance:\n\nThe h-step ahead forecast variance is:\n\ntextVar(haty_t+h) = sigma^2 sum_j=0^h-1 c_j^2\n\nwhere c_j depends on the model's transition matrix F and error vector g.\n\n","category":"section"},{"location":"tbats/#Usage-in-Durbyn","page":"TBATS","title":"Usage in Durbyn","text":"Durbyn provides two interfaces for TBATS: the classic API with direct function calls and the grammar interface for declarative model specification.","category":"section"},{"location":"tbats/#Grammar-Interface-(Recommended)","page":"TBATS","title":"Grammar Interface (Recommended)","text":"The grammar interface provides a unified, declarative way to specify TBATS models using @formula and TbatsSpec:\n\nusing Durbyn\nusing Durbyn.ModelSpecs\n\n# Create sample data with weekly seasonality (non-integer period)\nn = 156  # 3 years of weekly data\nt = 1:n\ndata = (sales = 100.0 .+ 10.0 .* sin.(2π .* t ./ 52.18) .+ 2.0 .* randn(n),)\n\n# Basic TBATS with defaults (automatic component selection)\nspec = TbatsSpec(@formula(sales = tbats()))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# TBATS with non-integer weekly seasonality (52.18 weeks/year)\nspec = TbatsSpec(@formula(sales = tbats(seasonal_periods=52.18)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# TBATS with multiple seasonal periods (daily + yearly)\n# Example: hourly data with daily (24h) and yearly (8766h ≈ 365.25 days)\nspec = TbatsSpec(@formula(sales = tbats(seasonal_periods=[24, 8766])))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# TBATS with explicit Fourier orders\nspec = TbatsSpec(@formula(sales = tbats(\n    seasonal_periods=[7, 365.25],\n    k=[3, 10]  # 3 harmonics for weekly, 10 for yearly\n)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# TBATS with specific component selection\nspec = TbatsSpec(@formula(sales = tbats(\n    seasonal_periods=52.18,\n    use_box_cox=true,\n    use_trend=true,\n    use_damped_trend=false,\n    use_arma_errors=true\n)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# Additional options at fit time\nfitted = fit(spec, data, bc_lower=0.0, bc_upper=1.5, biasadj=true)\n\nPanel Data Support:\n\n# Create panel data (Tables.jl compatible)\nn_per_group = 104  # 2 years of weekly data per group\ngroups = [\"A\", \"B\", \"C\"]\n\ntbl = (\n    product = repeat(groups, inner=n_per_group),\n    sales = vcat([\n        100.0 .+ 10.0 .* sin.(2π .* (1:n_per_group) ./ 52) .+ 2.0 .* randn(n_per_group)\n        for _ in groups\n    ]...)\n)\n\n# Fit TBATS to each product separately\nspec = TbatsSpec(@formula(sales = tbats(seasonal_periods=52.18)))\nfitted = fit(spec, tbl, groupby = :product)\nfc = forecast(fitted, h = 12)\n\nModel Comparison:\n\n# Compare TBATS with BATS and other models\nmodels = model(\n    TbatsSpec(@formula(sales = tbats(seasonal_periods=52.18))),  # Non-integer period\n    BatsSpec(@formula(sales = bats(seasonal_periods=52))),        # Integer period only\n    ArimaSpec(@formula(sales = p() + q() + P() + Q())),\n    EtsSpec(@formula(sales = e(\"Z\") + t(\"Z\") + s(\"Z\"))),\n    names = [\"tbats\", \"bats\", \"arima\", \"ets\"]\n)\n\nfitted = fit(models, data)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"tbats/#Classic-API","page":"TBATS","title":"Classic API","text":"For direct usage without the grammar interface:\n\nusing Durbyn\n\ny = rand(100)\nm = [7, 365.25]\n\nmodel = tbats(y, m)\n\nfc = forecast(model, h=20)","category":"section"},{"location":"tbats/#Formula-Interface-(Direct)","page":"TBATS","title":"Formula Interface (Direct)","text":"You can also use the formula interface directly without TbatsSpec:\n\nusing Durbyn\n\ndata = (sales = randn(156) .+ 100,)\nformula = @formula(sales = tbats(seasonal_periods=52.18))\nfit = tbats(formula, data)  # Works with Tables.jl compatible data\nfc = forecast(fit, h = 12)","category":"section"},{"location":"tbats/#Key-keyword-arguments","page":"TBATS","title":"Key keyword arguments","text":"Grammar arguments (in @formula):\n\nseasonal_periods: Real or Vector{Real} specifying seasonal period(s) - can be non-integer\nk: Int or Vector{Int} specifying Fourier orders (harmonics per season)\nuse_box_cox, use_trend, use_damped_trend, use_arma_errors: Bool or nothing to auto-select\n\nFit-time arguments:\n\nbc_lower, bc_upper: bounds for the Box–Cox search when enabled\nbiasadj: apply bias correction during inverse Box–Cox transformation\nmodel: pass a previous TBATSModel to refit the same structure to new data\n\n","category":"section"},{"location":"tbats/#Julia-Implementation-(Array-Interface)","page":"TBATS","title":"Julia Implementation (Array Interface)","text":"","category":"section"},{"location":"tbats/#Function-Signature","page":"TBATS","title":"Function Signature","text":"tbats(\n    y::AbstractVector{<:Real},\n    m::Union{Vector{Int}, Nothing} = nothing;\n    use_box_cox::Union{Bool, AbstractVector{Bool}, Nothing} = nothing,\n    use_trend::Union{Bool, AbstractVector{Bool}, Nothing} = nothing,\n    use_damped_trend::Union{Bool, AbstractVector{Bool}, Nothing} = nothing,\n    use_arma_errors::Bool = true,\n    bc_lower::Real = 0.0,\n    bc_upper::Real = 1.0,\n    biasadj::Bool = false,\n    model = nothing\n)","category":"section"},{"location":"tbats/#Parameters","page":"TBATS","title":"Parameters","text":"y: Univariate time series (1D vector)\nm: Vector of seasonal periods (can be non-integer). Use nothing for non-seasonal models.\nuse_box_cox: Whether to use Box-Cox transformation\nnothing (default): tries both and selects by AIC\ntrue/false: forces the choice\nVector of bools: tries each option\nuse_trend: Whether to include trend component\nnothing (default): tries both\ntrue/false: forces the choice\nuse_damped_trend: Whether to damp the trend\nnothing (default): tries both\ntrue/false: forces the choice\nIgnored if use_trend=false\nuse_arma_errors: Whether to model residuals with ARMA\nbc_lower, bc_upper: Bounds for Box-Cox parameter search\nbiasadj: Use bias-adjusted back-transformation for forecasts\nmodel: Previously fitted TBATS model to refit","category":"section"},{"location":"tbats/#Returns","page":"TBATS","title":"Returns","text":"A TBATSModel struct containing:\n\nlambda: Box-Cox parameter (or nothing)\nalpha: Level smoothing parameter\nbeta: Trend smoothing parameter (or nothing)\ndamping_parameter: Damping parameter φ (or nothing)\ngamma_one_values: First seasonal smoothing parameters\ngamma_two_values: Second seasonal smoothing parameters\nar_coefficients: AR coefficients (or nothing)\nma_coefficients: MA coefficients (or nothing)\nseasonal_periods: Vector of seasonal periods\nk_vector: Vector of Fourier orders per seasonal period\nfitted_values: In-sample fitted values\nerrors: Residuals\nx: State matrix\nseed_states: Initial states\nvariance: Residual variance\nAIC: Akaike Information Criterion\nlikelihood: Log-likelihood\ny: Original time series\nmethod: Model descriptor string\n\n","category":"section"},{"location":"tbats/#Model-Selection","page":"TBATS","title":"Model Selection","text":"The implementation automatically selects:\n\nFourier orders (k_i) for each seasonal period via AIC\nStarts with k=1\nFor small periods (m leq 12): searches downward from k = lfloor(m-1)2rfloor\nFor large periods (m  12): uses step-up/step-down search around k in 567\nARMA orders using auto_arima on residuals\nBox-Cox parameter via profile likelihood\nTrend and damping via AIC comparison\n\nThe descriptor format matches R's forecast::tbats:\n\nTBATS(omega, {p,q}, phi, <m1,k1>, <m2,k2>, ...)\n\nExample: TBATS(0.001, {0,0}, 0.98, <7,3>, <365.25,10>)\n\n","category":"section"},{"location":"tbats/#Examples","page":"TBATS","title":"Examples","text":"","category":"section"},{"location":"tbats/#Example-1:-Weekly-Data-with-Non-Integer-Seasonality","page":"TBATS","title":"Example 1: Weekly Data with Non-Integer Seasonality","text":"using Durbyn\n\ny = randn(520)\nm = 52.18\n\nmodel = tbats(y, m)\nprintln(model)\n\nfc = forecast(model, h=52)\n\nOutput:\n\nTBATS(0.112, {0,0}, -, <52.18,5>)\n\nParameters:\n  Lambda:  0.1120\n  Alpha:   0.0823\n  Gamma-1: 0.0045, 0.0032, 0.0019, 0.0011, 0.0007\n  Gamma-2: 0.0051, 0.0036, 0.0024, 0.0015, 0.0009\n\nSigma:   0.9876\nAIC:     1523.45","category":"section"},{"location":"tbats/#Example-2:-Multiple-Seasonalities-(Daily-Weekly-Yearly)","page":"TBATS","title":"Example 2: Multiple Seasonalities (Daily + Weekly + Yearly)","text":"using Durbyn, Dates\n\nn = 365 * 3\nt = 1:n\ny = 100 .+ 10 .* sin.(2π .* t ./ 365.25) .+     # yearly\n    5 .* sin.(2π .* t ./ 7) .+                   # weekly\n    2 .* randn(n)                                # noise\n\nmodel = tbats(y, [7, 365.25])\nprintln(model)\n\nfc = forecast(model, h=30)","category":"section"},{"location":"tbats/#Example-3:-Dual-Calendar-Effects-(Gregorian-Hijri)","page":"TBATS","title":"Example 3: Dual Calendar Effects (Gregorian + Hijri)","text":"using Durbyn\n\nm_gregorian = 365.25\nm_hijri = 354.37\n\ny = load_data()  # Data with both calendar effects\n\nmodel = tbats(y, [m_gregorian, m_hijri])\nprintln(model)\n\nOutput:\n\nTBATS(0.234, {1,1}, 0.975, <365.25,8>, <354.37,7>)","category":"section"},{"location":"tbats/#Example-4:-Forcing-Model-Components","page":"TBATS","title":"Example 4: Forcing Model Components","text":"using Durbyn\n\ny = randn(200)\nm = [7, 30]\n\nmodel = tbats(\n    y, m;\n    use_box_cox = false,           # No transformation\n    use_trend = true,              # Force trend\n    use_damped_trend = true,       # Force damping\n    use_arma_errors = false        # No ARMA errors\n)","category":"section"},{"location":"tbats/#Example-5:-High-Frequency-Data-(5-minute-intervals)","page":"TBATS","title":"Example 5: High-Frequency Data (5-minute intervals)","text":"using Durbyn\n\nminutes_per_day = 288      # 24 * 60 / 5\nminutes_per_week = 2016    # 7 * 288\n\ny = load_call_center_data()\n\nmodel = tbats(y, [minutes_per_day, minutes_per_week])\n\nprintln(\"State dimension: \", size(model.x, 1))\nprintln(\"Seasonal periods: \", model.seasonal_periods)\nprintln(\"Fourier orders: \", model.k_vector)\n\n","category":"section"},{"location":"tbats/#Forecasting","page":"TBATS","title":"Forecasting","text":"","category":"section"},{"location":"tbats/#Basic-Forecasting","page":"TBATS","title":"Basic Forecasting","text":"using Durbyn\n\nmodel = tbats(y, [7, 365.25])\n\nfc = forecast(model, h=30)\n\nprintln(\"Point forecasts: \", fc.mean)\nprintln(\"80% CI: \", fc.lower[:, 1], \" to \", fc.upper[:, 1])\nprintln(\"95% CI: \", fc.lower[:, 2], \" to \", fc.upper[:, 2])","category":"section"},{"location":"tbats/#Forecast-Options","page":"TBATS","title":"Forecast Options","text":"fc = forecast(\n    model;\n    h = 50,                          # Forecast horizon\n    level = [80, 95],                # Confidence levels\n    fan = false,                     # Fan chart levels\n    biasadj = nothing                # Bias adjustment (inherits from model)\n)","category":"section"},{"location":"tbats/#Fan-Charts","page":"TBATS","title":"Fan Charts","text":"fc = forecast(model, h=30, fan=true)\n\nprintln(\"Confidence levels: \", fc.level)  # [51, 54, 57, ..., 99]\n\n","category":"section"},{"location":"tbats/#Model-Diagnostics","page":"TBATS","title":"Model Diagnostics","text":"","category":"section"},{"location":"tbats/#Fitted-Values-and-Residuals","page":"TBATS","title":"Fitted Values and Residuals","text":"using Durbyn\n\nmodel = tbats(y, m)\n\nfitted_vals = fitted(model)\nresiduals_vals = residuals(model)\n\nusing Statistics\nprintln(\"RMSE: \", sqrt(mean(residuals_vals .^ 2)))\nprintln(\"MAE: \", mean(abs.(residuals_vals)))","category":"section"},{"location":"tbats/#Checking-ARMA-Adequacy","page":"TBATS","title":"Checking ARMA Adequacy","text":"using StatsPlots, Statistics\n\nmodel = tbats(y, m, use_arma_errors=true)\n\nresid = residuals(model)\n\nautocor(resid, 1:20) |> plot","category":"section"},{"location":"tbats/#Information-Criteria","page":"TBATS","title":"Information Criteria","text":"model = tbats(y, m)\n\nprintln(\"AIC: \", model.AIC)\nprintln(\"Log-likelihood: \", model.likelihood)\nprintln(\"Parameters: \", length(model.parameters[:vect]))\nprintln(\"States: \", size(model.seed_states, 1))\n\n","category":"section"},{"location":"tbats/#Computational-Considerations","page":"TBATS","title":"Computational Considerations","text":"","category":"section"},{"location":"tbats/#State-Space-Dimension","page":"TBATS","title":"State Space Dimension","text":"For a TBATS model with:\n\nTrend: 2 states (ell_t, b_t)\nT seasonal components with Fourier orders k_1 ldots k_T: 2sum_i=1^T k_i states\nARMA(p, q): p + q states\n\nTotal states: 2 + 2sum k_i + p + q\n\nExample:\n\nDaily (m=288, k=5): 10 states\nWeekly (m=2016, k=7): 14 states\nTotal: 2 + 10 + 14 = 26 states\n\nCompare to BATS: 2 + 288 + 2016 = 2306 states!","category":"section"},{"location":"tbats/#Choosing-Fourier-Orders","page":"TBATS","title":"Choosing Fourier Orders","text":"The implementation automatically selects k_i via AIC, but general guidelines:\n\nShort periods (m  12): use k approx m2\nMedium periods (m = 12 to 100): use k in 3 10\nLong periods (m  100): use k in 5 15\nVery long periods (m  1000): use k in 10 20\n\nHigher k captures more complex seasonal shapes but increases computation.\n\n","category":"section"},{"location":"tbats/#Empirical-Results-(From-De-Livera-et-al.-2011)","page":"TBATS","title":"Empirical Results (From De Livera et al. 2011)","text":"","category":"section"},{"location":"tbats/#1.-Weekly-U.S.-Gasoline-Demand","page":"TBATS","title":"1. Weekly U.S. Gasoline Demand","text":"Seasonal period: 52.18 weeks/year\nResult: TBATS outperforms BATS because seasonality is non-integer\nMASE improvement: 8.3%","category":"section"},{"location":"tbats/#2.-Call-Center-Arrivals-(5-minute-intervals)","page":"TBATS","title":"2. Call Center Arrivals (5-minute intervals)","text":"Seasonal periods: 169 (daily), 845 (weekly)\nBATS states: 1014\nTBATS states: 24 (with k_1=5, k_2=7)\nResult: 40× reduction in state dimension with better forecasts","category":"section"},{"location":"tbats/#3.-Turkish-Electricity-Consumption","page":"TBATS","title":"3. Turkish Electricity Consumption","text":"Calendars: Gregorian (365.25) + Hijri (354.37)\nResult: BATS cannot handle non-integer dual calendars; TBATS is the only feasible model\nMASE improvement over ETS: 23.1%\n\n","category":"section"},{"location":"tbats/#Comparison:-BATS-vs-TBATS","page":"TBATS","title":"Comparison: BATS vs TBATS","text":"Scenario Use BATS Use TBATS\nInteger seasonal period ✓ ✓\nNon-integer seasonal period ✗ ✓\nShort seasonal period (m  50) ✓ ✓\nLong seasonal period (m  350) ✗ ✓\nMultiple integer seasons ✓ ✓\nDual calendars ✗ ✓\nMemory-constrained ✗ ✓\n\nRule of thumb: Use TBATS when:\n\nAny seasonal period is non-integer\nAny seasonal period exceeds 100\nYou have dual calendar effects\nMemory/computation is limited\n\n","category":"section"},{"location":"tbats/#References","page":"TBATS","title":"References","text":"De Livera, A.M., Hyndman, R.J., & Snyder, R.D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. Journal of the American Statistical Association, 106(496), 1513-1527.\n\n","category":"section"},{"location":"tbats/#See-Also","page":"TBATS","title":"See Also","text":"BATS Documentation - Integer seasonal periods only","category":"section"},{"location":"optimize/#Optimization-Module","page":"Optimization","title":"Optimization Module","text":"The Optimize module provides numerical optimization algorithms for minimizing objective functions. These solvers are used internally throughout Durbyn.jl for model fitting (e.g., maximum likelihood estimation in ETS, ARIMA, and BATS models) and are also available for general-purpose optimization tasks.\n\n","category":"section"},{"location":"optimize/#Overview","page":"Optimization","title":"Overview","text":"The module implements four optimization algorithms accessible through a unified interface:\n\nAlgorithm Function Type Use Case\nNelder-Mead nelder_mead Derivative-free General purpose, no gradient needed\nBFGS bfgs Quasi-Newton Fast convergence with gradients\nL-BFGS-B lbfgsb Bounded quasi-Newton Box-constrained optimization\nBrent brent 1D derivative-free Scalar optimization","category":"section"},{"location":"optimize/#Exported-Functions-and-Types","page":"Optimization","title":"Exported Functions and Types","text":"# Main optimization functions\nexport optimize, nelder_mead, bfgs, lbfgsb, brent\n\n# Options types\nexport NelderMeadOptions, BFGSOptions, LBFGSBOptions, BrentOptions\n\n# Supporting functions\nexport numgrad, numgrad!, numgrad_with_cache!, NumericalGradientCache\nexport numerical_hessian, bfgs_hessian_update!, BFGSWorkspace\nexport scaler, descaler\n\n","category":"section"},{"location":"optimize/#Unified-Interface-(optimize)","page":"Optimization","title":"Unified Interface (optimize)","text":"The optimize function provides a unified interface for all solvers, allowing easy switching between methods.\n\noptimize(x0, fn; grad=nothing, method=\"Nelder-Mead\", lower=-Inf, upper=Inf,\n         control=Dict(), hessian=false, kwargs...)","category":"section"},{"location":"optimize/#Arguments","page":"Optimization","title":"Arguments","text":"x0::Vector{Float64}: Initial parameter vector\nfn::Function: Objective function to minimize, called as fn(x; kwargs...)","category":"section"},{"location":"optimize/#Keyword-Arguments","page":"Optimization","title":"Keyword Arguments","text":"grad::Union{Function,Nothing}: Gradient function, called as grad(x; kwargs...). If nothing, numerical gradients are computed automatically.\nmethod::String: Optimization method:\n\"Nelder-Mead\" (default) - Derivative-free simplex\n\"BFGS\" - Quasi-Newton with line search\n\"L-BFGS-B\" - Limited-memory BFGS with box constraints\n\"Brent\" - 1D optimization (scalar x0 only)\nlower, upper: Bounds for L-BFGS-B and Brent methods\ncontrol::Dict: Control parameters (see below)\nhessian::Bool: If true, compute Hessian at solution","category":"section"},{"location":"optimize/#Control-Parameters","page":"Optimization","title":"Control Parameters","text":"Parameter Default Description\ntrace 0 Verbosity level (0=silent, >0=verbose)\nfnscale 1.0 Function scaling factor\nparscale ones(n) Parameter scaling vector\nndeps 1e-3 Step sizes for numerical derivatives\nmaxit 500/100 Maximum iterations (500 for NM, 100 for others)\nabstol -Inf Absolute convergence tolerance\nreltol sqrt(eps) Relative convergence tolerance\ngtol 0.0 Gradient norm tolerance (BFGS only)\nalpha 1.0 Nelder-Mead reflection coefficient\nbeta 0.5 Nelder-Mead contraction coefficient\ngamma 2.0 Nelder-Mead expansion coefficient\nREPORT 10 Reporting frequency for BFGS\nlmm 5 L-BFGS-B memory parameter\nfactr 1e7 L-BFGS-B tolerance factor\npgtol 0.0 L-BFGS-B projected gradient tolerance","category":"section"},{"location":"optimize/#Returns","page":"Optimization","title":"Returns","text":"Named tuple with fields:\n\npar::Vector{Float64}: Optimal parameters\nvalue::Float64: Function value at optimum\ncounts::NamedTuple: (function_=n, gradient=m) evaluation counts\nconvergence::Int: Status code (0=success, 1=maxit reached)\nmessage: Convergence message (method-dependent)\nhessian: Hessian matrix at solution (if requested)","category":"section"},{"location":"optimize/#Examples","page":"Optimization","title":"Examples","text":"using Durbyn.Optimize\n\n# Define Rosenbrock function\nrosenbrock(x) = 100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2\n\n# Analytical gradient\nrosenbrock_grad(x) = [\n    -400*x[1]*(x[2]-x[1]^2) - 2*(1-x[1]),\n    200*(x[2]-x[1]^2)\n]\n\n# Nelder-Mead (no gradient needed)\nresult = optimize([-1.2, 1.0], rosenbrock)\nprintln(\"Optimal: $(result.par), Value: $(result.value)\")\n\n# BFGS with analytical gradient\nresult = optimize([-1.2, 1.0], rosenbrock; grad=rosenbrock_grad, method=\"BFGS\")\n\n# BFGS with numerical gradient (automatic)\nresult = optimize([-1.2, 1.0], rosenbrock; method=\"BFGS\")\n\n# L-BFGS-B with box constraints\nresult = optimize([0.5, 0.5], rosenbrock; method=\"L-BFGS-B\",\n                  lower=[0.0, 0.0], upper=[2.0, 2.0])\n\n# With control parameters\nresult = optimize([-1.2, 1.0], rosenbrock; method=\"BFGS\",\n                  control=Dict(\"trace\" => 1, \"maxit\" => 500, \"gtol\" => 1e-6))\n\n# Request Hessian at solution\nresult = optimize([-1.2, 1.0], rosenbrock; grad=rosenbrock_grad,\n                  method=\"BFGS\", hessian=true)\nprintln(\"Hessian:\\n$(result.hessian)\")\n\n# 1D optimization with Brent's method\nf1d(x) = (x[1] - 2)^2\nresult = optimize([0.0], f1d; method=\"Brent\", lower=-5.0, upper=5.0)\n\n","category":"section"},{"location":"optimize/#Nelder-Mead-Simplex-(nelder_mead)","page":"Optimization","title":"Nelder-Mead Simplex (nelder_mead)","text":"A derivative-free optimization method that searches for a minimum using a simplex – a polytope with n+1 vertices in n dimensions. The algorithm adaptively reshapes the simplex through reflection, expansion, contraction, and shrink operations.\n\nReference: Nelder, J. A. and Mead, R. (1965). A simplex method for function minimization. Computer Journal, 7, 308–313. Implementation follows the compact formulation in Nash (1990).","category":"section"},{"location":"optimize/#Algorithm","page":"Optimization","title":"Algorithm","text":"The algorithm performs four operations on the simplex:\n\nReflection: Reflect the worst point through the centroid\nExpansion: If reflection improves the best, expand further\nContraction: If reflection fails, contract toward the best\nShrink: If still no improvement, shrink simplex about the best point","category":"section"},{"location":"optimize/#Mathematical-Details","page":"Optimization","title":"Mathematical Details","text":"Given simplex vertices x_1  x_n+1 ordered by function values f(x_1) leq  leq f(x_n+1):\n\nCentroid: barx = frac1nsum_i=1^n x_i\nReflection: x_r = barx + alpha(barx - x_n+1)\nExpansion: x_e = barx + gamma(x_r - barx)\nContraction: x_c = barx + beta(x_n+1 - barx)\n\nDefault coefficients: alpha = 10, beta = 05, gamma = 20","category":"section"},{"location":"optimize/#Usage","page":"Optimization","title":"Usage","text":"nelder_mead(f, x0, options::NelderMeadOptions)\n\nOptions:\n\nNelderMeadOptions(;\n    abstol = -Inf,           # Absolute tolerance on function value\n    reltol = sqrt(eps()),    # Relative tolerance\n    alpha = 1.0,             # Reflection coefficient\n    beta = 0.5,              # Contraction coefficient\n    gamma = 2.0,             # Expansion coefficient\n    trace = false,           # Print diagnostics\n    maxit = 500,             # Maximum function evaluations\n    invalid_penalty = 1e35,  # Penalty for non-finite values\n    project_to_bounds = false,\n    lower = nothing,\n    upper = nothing,\n    init_step_cap = nothing\n)\n\nReturns: Named tuple (x_opt, f_opt, fncount, fail)\n\nfail=0: Converged successfully\nfail=1: Exceeded maximum iterations\nfail=10: Degenerate simplex (shrink failure)","category":"section"},{"location":"optimize/#Example","page":"Optimization","title":"Example","text":"using Durbyn.Optimize\n\nf(x) = (x[1] - 1)^2 + (x[2] - 2)^2\n\nopts = NelderMeadOptions(trace=true, maxit=1000)\nresult = nelder_mead(f, [0.0, 0.0], opts)\nprintln(\"Optimum: $(result.x_opt)\")\n\n","category":"section"},{"location":"optimize/#BFGS-Quasi-Newton-(bfgs)","page":"Optimization","title":"BFGS Quasi-Newton (bfgs)","text":"The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is a quasi-Newton method that iteratively builds an approximation to the inverse Hessian matrix using gradient information. It achieves superlinear convergence on smooth problems without requiring explicit second-derivative computation.\n\nReference: Nocedal, J. and Wright, S. J. (1999). Numerical Optimization. Springer. See also the original papers by Broyden (1970), Fletcher (1970), Goldfarb (1970), and Shanno (1970).","category":"section"},{"location":"optimize/#Algorithm-2","page":"Optimization","title":"Algorithm","text":"BFGS iteratively updates the inverse Hessian approximation B_k using the formula:\n\nB_k+1 = B_k + frac(1 + c^T B_k c  D_1) t t^TD_1 - fract (B_k c)^T + (B_k c) t^TD_1\n\nwhere:\n\nt = x_k+1 - x_k (parameter step)\nc = nabla f_k+1 - nabla f_k (gradient difference)\nD_1 = t^T c","category":"section"},{"location":"optimize/#Features","page":"Optimization","title":"Features","text":"Armijo line search with backtracking\nPeriodic Hessian restarts every 2n gradient evaluations\nParameter masking to freeze variables\nAutomatic numerical gradients if analytical gradient not provided\nGradient norm convergence criterion","category":"section"},{"location":"optimize/#Usage-2","page":"Optimization","title":"Usage","text":"bfgs(f, g, x0; mask=nothing, options=BFGSOptions(), step_sizes=1e-3*ones(n),\n     numgrad_cache=nothing, extra=nothing)\n\nFunction Signatures:\n\nf(n, x, extra) - Objective function\ng(n, x, grad, extra) - Gradient function (modifies grad in-place), or nothing\n\nOptions:\n\nBFGSOptions(;\n    abstol = -Inf,           # Absolute tolerance\n    reltol = sqrt(eps()),    # Relative tolerance\n    gtol = 0.0,              # Gradient norm tolerance\n    trace = false,           # Print progress\n    maxit = 100,             # Maximum iterations\n    report_interval = 10     # Reporting frequency\n)\n\nGradient Norm Convergence (gtol):\n\nWhen gtol > 0, convergence is declared if:\n\nnabla f(x)  textgtol times max(1 f(x))\n\nThis provides a first-order optimality condition. Recommended values: 1e-5 to 1e-8.\n\nReturns: Named tuple (x_opt, f_opt, n_iter, fail, fn_evals, gr_evals)","category":"section"},{"location":"optimize/#Example-2","page":"Optimization","title":"Example","text":"using Durbyn.Optimize\n\n# Internal function signature: f(n, x, extra)\nrosenbrock_internal(n, x, extra) = 100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2\n\n# Gradient modifies grad in-place\nfunction rosenbrock_grad_internal(n, x, grad, extra)\n    grad[1] = -400*x[1]*(x[2]-x[1]^2) - 2*(1-x[1])\n    grad[2] = 200*(x[2]-x[1]^2)\n    return nothing\nend\n\nopts = BFGSOptions(trace=true, gtol=1e-6)\nresult = bfgs(rosenbrock_internal, rosenbrock_grad_internal, [-1.2, 1.0]; options=opts)\n\n# With numerical gradients (g=nothing)\nresult = bfgs(rosenbrock_internal, nothing, [-1.2, 1.0]; options=opts)\n\n","category":"section"},{"location":"optimize/#L-BFGS-B-Bounded-Optimization-(lbfgsb)","page":"Optimization","title":"L-BFGS-B Bounded Optimization (lbfgsb)","text":"A limited-memory variant of BFGS that supports box constraints on variables. Instead of storing the full inverse Hessian approximation, it retains only the last memory_size iterations of gradient information, making it memory-efficient for large-scale problems.\n\nReference: Byrd, R. H., Lu, P., Nocedal, J., and Zhu, C. (1995). A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16, 1190–1208. See also Zhu, C., Byrd, R. H., Lu, P., and Nocedal, J. (1997). Algorithm 778: L-BFGS-B.","category":"section"},{"location":"optimize/#Features-2","page":"Optimization","title":"Features","text":"Box constraints: Lower and upper bounds on variables\nLimited memory: Stores only memory_size gradient steps (default: 10)\nWolfe line search with zoom refinement\nProjected gradient for convergence checking\nParameter masking by setting lower[i] = upper[i] = x0[i]","category":"section"},{"location":"optimize/#Usage-3","page":"Optimization","title":"Usage","text":"lbfgsb(f, g, x0; mask=nothing, lower=-Inf, upper=Inf, options=LBFGSBOptions())\n\nOptions:\n\nLBFGSBOptions(;\n    memory_size = 10,    # Memory size (number of stored iterations)\n    ftol_factor = 1e7,   # Tolerance factor: f_tol = ftol_factor * eps()\n    pg_tol = 1e-5,       # Projected gradient infinity-norm tolerance\n    maxit = 1000,        # Maximum iterations\n    print_level = 0      # Print level (0=silent, >0=verbose)\n)\n\nConvergence Criterion:\n\nThe algorithm converges when:\n\ntextproj(nabla f(x))_infty  textpg_tol\n\nwhere the projected gradient accounts for variables at their bounds.\n\nReturns: Named tuple (x_opt, f_opt, n_iter, fail, fn_evals, gr_evals)","category":"section"},{"location":"optimize/#Example-3","page":"Optimization","title":"Example","text":"using Durbyn.Optimize\n\nrosenbrock(n, x, extra) = 100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2\n\nopts = LBFGSBOptions(memory_size=5, pg_tol=1e-6, print_level=1)\nresult = lbfgsb(rosenbrock, nothing, [0.5, 0.5];\n                lower=[0.0, 0.0], upper=[2.0, 2.0], options=opts)\n\nprintln(\"Bounded optimum: $(result.x_opt)\")\n\n","category":"section"},{"location":"optimize/#Brent's-1D-Method-(brent)","page":"Optimization","title":"Brent's 1D Method (brent)","text":"A derivative-free algorithm for one-dimensional optimization that combines golden section search with parabolic interpolation. Golden section steps provide guaranteed progress, while parabolic interpolation accelerates convergence near the minimum.\n\nReference: Brent, R. P. (1973). Algorithms for Minimization Without Derivatives. Prentice-Hall. Implementation follows the compact formulation in Nash (1990).","category":"section"},{"location":"optimize/#Convergence","page":"Optimization","title":"Convergence","text":"For unimodal functions with positive second derivative at the minimum:\n\nSuperlinear convergence with order approximately 1.324\nError bound:  3epsilonx_min + texttol","category":"section"},{"location":"optimize/#Usage-4","page":"Optimization","title":"Usage","text":"brent(f, lower, upper; options=BrentOptions())\n\nOptions:\n\nBrentOptions(;\n    tol = 1.5e-8,    # Interval tolerance\n    trace = false,    # Print diagnostics\n    maxit = 1000      # Maximum iterations\n)\n\nReturns: Named tuple (x_opt, f_opt, n_iter, fail, fn_evals)","category":"section"},{"location":"optimize/#Example-4","page":"Optimization","title":"Example","text":"using Durbyn.Optimize\n\nf(x) = (x - 3.5)^2 + 2*sin(x)\n\nopts = BrentOptions(tol=1e-10, trace=true)\nresult = brent(f, 0.0, 10.0; options=opts)\n\nprintln(\"Minimum at x = $(result.x_opt), f(x) = $(result.f_opt)\")\n\n","category":"section"},{"location":"optimize/#Numerical-Gradient-Computation","page":"Optimization","title":"Numerical Gradient Computation","text":"The module provides efficient numerical gradient computation using central finite differences.","category":"section"},{"location":"optimize/#numgrad","page":"Optimization","title":"numgrad","text":"numgrad(f, n, x, extra, step_sizes; usebounds=false, lower=nothing, upper=nothing)\n\nComputes the gradient using central differences:\n\nfracpartial fpartial x_i approx fracf(x + epsilon_i e_i) - f(x - epsilon_i e_i)2epsilon_i","category":"section"},{"location":"optimize/#numgrad_with_cache!","page":"Optimization","title":"numgrad_with_cache!","text":"For repeated gradient evaluations, use a pre-allocated cache:\n\ncache = NumericalGradientCache(n)\nnumgrad_with_cache!(cache, f, n, x, extra, step_sizes)\n\nThis eliminates memory allocations during iterative optimization.","category":"section"},{"location":"optimize/#Example-5","page":"Optimization","title":"Example","text":"using Durbyn.Optimize\n\nf(n, x, extra) = x[1]^2 + x[2]^2\nx = [1.0, 2.0]\nstep_sizes = [1e-6, 1e-6]\n\n# Single evaluation\ng = numgrad(f, 2, x, nothing, step_sizes)\n\n# With cache for repeated evaluations\ncache = NumericalGradientCache(2)\nfor i in 1:1000\n    numgrad_with_cache!(cache, f, 2, x, nothing, step_sizes)\n    # cache.gradient contains the gradient\nend\n\n","category":"section"},{"location":"optimize/#Hessian-Computation","page":"Optimization","title":"Hessian Computation","text":"The numerical_hessian function computes the Hessian matrix at a given point using finite differences.\n\nReference: Nocedal, J. and Wright, S. J. (1999). Numerical Optimization. Springer. Chapter 8, finite difference formulas.\n\nnumerical_hessian(fn, x, grad=nothing; fnscale=1.0, parscale=ones(n), step_sizes=1e-3*ones(n))\n\nMethod:\n\nIf grad=nothing: Uses second-order finite differences of the objective function\nIf grad provided: Computes Hessian from gradient via finite differences\n\nReturns: Symmetric Hessian matrix (n x n)","category":"section"},{"location":"optimize/#Example-6","page":"Optimization","title":"Example","text":"using Durbyn.Optimize\n\nrosenbrock(x) = 100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2\n\n# At the optimum\nx_opt = [1.0, 1.0]\nH = numerical_hessian(rosenbrock, x_opt)\nprintln(\"Hessian at optimum:\\n$H\")\n\n# Eigenvalues (should be positive for minimum)\nusing LinearAlgebra\neigvals(H)\n\n","category":"section"},{"location":"optimize/#Parameter-Scaling","page":"Optimization","title":"Parameter Scaling","text":"Scaling parameters can improve numerical conditioning when parameters have different magnitudes.","category":"section"},{"location":"optimize/#scaler-and-descaler","page":"Optimization","title":"scaler and descaler","text":"x_scaled = scaler(x, scale)       # x_scaled = x ./ scale\nx_original = descaler(x_scaled, scale)  # x_original = x_scaled .* scale","category":"section"},{"location":"optimize/#Example-7","page":"Optimization","title":"Example","text":"# Parameters with different scales\npar = [1e-6, 1e6]\nscale = [1e-6, 1e6]\n\n# Scale for optimization\npar_scaled = scaler(par, scale)  # [1.0, 1.0]\n\n# After optimization, recover original scale\npar_opt = descaler(par_scaled_opt, scale)\n\n","category":"section"},{"location":"optimize/#Practical-Guidelines","page":"Optimization","title":"Practical Guidelines","text":"","category":"section"},{"location":"optimize/#Choosing-an-Optimization-Method","page":"Optimization","title":"Choosing an Optimization Method","text":"Scenario Recommended Method\nNo gradient available Nelder-Mead\nSmooth function, gradient available BFGS\nBox constraints needed L-BFGS-B\nLarge-scale problem (many parameters) L-BFGS-B\n1D optimization Brent\nNon-smooth or noisy function Nelder-Mead","category":"section"},{"location":"optimize/#Tips-for-Better-Convergence","page":"Optimization","title":"Tips for Better Convergence","text":"Parameter scaling: Scale parameters to similar magnitudes\ncontrol = Dict(\"parscale\" => [1e-3, 1e3])\nGood starting point: Provide reasonable initial values\nCheck gradients: Verify analytical gradients match numerical\ngrad_analytical = my_gradient(x)\ngrad_numerical = numgrad(f, n, x, nothing, 1e-6*ones(n))\n@assert isapprox(grad_analytical, grad_numerical, rtol=1e-4)\nAdjust tolerances: Tighten tolerances for more precision\ncontrol = Dict(\"reltol\" => 1e-10, \"gtol\" => 1e-8)\nMonitor convergence: Use trace to diagnose issues\ncontrol = Dict(\"trace\" => 1)\n\n","category":"section"},{"location":"optimize/#Complete-Example:-Maximum-Likelihood-Estimation","page":"Optimization","title":"Complete Example: Maximum Likelihood Estimation","text":"using Durbyn.Optimize\nusing Distributions\n\n# Generate data from Normal(mu=5, sigma=2)\ndata = rand(Normal(5.0, 2.0), 100)\n\n# Negative log-likelihood (to minimize)\nfunction neg_loglik(params)\n    mu, log_sigma = params\n    sigma = exp(log_sigma)  # Ensure sigma > 0\n    n = length(data)\n    ll = -n/2 * log(2*pi) - n*log_sigma - sum((data .- mu).^2) / (2*sigma^2)\n    return -ll  # Return negative for minimization\nend\n\n# Analytical gradient\nfunction neg_loglik_grad(params)\n    mu, log_sigma = params\n    sigma = exp(log_sigma)\n    n = length(data)\n\n    d_mu = sum(data .- mu) / sigma^2\n    d_log_sigma = n - sum((data .- mu).^2) / sigma^2\n\n    return [-d_mu, -d_log_sigma]\nend\n\n# Optimize with BFGS\nresult = optimize([0.0, 0.0], neg_loglik;\n                  grad=neg_loglik_grad,\n                  method=\"BFGS\",\n                  hessian=true)\n\n# Extract estimates\nmu_hat = result.par[1]\nsigma_hat = exp(result.par[2])\n\nprintln(\"Estimated mu: $mu_hat (true: 5.0)\")\nprintln(\"Estimated sigma: $sigma_hat (true: 2.0)\")\n\n# Standard errors from Hessian\nusing LinearAlgebra\nse = sqrt.(diag(inv(result.hessian)))\nprintln(\"SE(mu): $(se[1])\")\nprintln(\"SE(log_sigma): $(se[2])\")\n\n","category":"section"},{"location":"optimize/#References","page":"Optimization","title":"References","text":"Nelder, J. A. and Mead, R. (1965). A simplex method for function minimization. Computer Journal, 7, 308–313.\nBroyden, C. G. (1970). The convergence of a class of double-rank minimization algorithms. Journal of the Institute of Mathematics and Its Applications, 6, 76–90.\nFletcher, R. (1970). A new approach to variable metric algorithms. Computer Journal, 13, 317–322.\nGoldfarb, D. (1970). A family of variable metric methods derived by variational means. Mathematics of Computation, 24, 23–26.\nShanno, D. F. (1970). Conditioning of quasi-Newton methods for function minimization. Mathematics of Computation, 24, 647–656.\nByrd, R. H., Lu, P., Nocedal, J., and Zhu, C. (1995). A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16, 1190–1208.\nZhu, C., Byrd, R. H., Lu, P., and Nocedal, J. (1997). Algorithm 778: L-BFGS-B, Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software, 23, 550–560.\nBrent, R. P. (1973). Algorithms for Minimization Without Derivatives. Prentice-Hall.\nNash, J. C. (1990). Compact Numerical Methods for Computers: Linear Algebra and Function Minimisation. 2nd ed. Adam Hilger.\nNocedal, J. and Wright, S. J. (1999). Numerical Optimization. Springer.","category":"section"},{"location":"utils/#Utilities-Module","page":"Utilities","title":"Utilities Module","text":"The Utils module provides helper functions and utilities used throughout Durbyn.jl. This includes example datasets for testing and learning, data manipulation functions, and other supporting tools.\n\n","category":"section"},{"location":"utils/#Example-Datasets","page":"Utilities","title":"Example Datasets","text":"Durbyn.jl provides several classic time series datasets commonly used in forecasting literature and examples. All datasets are returned as Vector{Float64}.","category":"section"},{"location":"utils/#Available-Datasets","page":"Utilities","title":"Available Datasets","text":"Dataset Frequency Length Period Characteristics\nair_passengers Monthly 144 1949-1960 Trend, multiplicative seasonality\nausbeer Quarterly 218 1956-2010 Seasonal pattern, varying trend\nlynx Annual 114 1821-1934 Cyclic (~10 year cycle)\nsunspots Monthly 235* 1749-1768 Cyclic (~11 year solar cycle)\npedestrian_counts Daily 2922 2009-2016 Weekly + annual seasonality\nsimulate_seasonal_data Configurable User-defined - Synthetic data generator\n\n*Truncated for demonstration; full dataset has 2820 observations.\n\n","category":"section"},{"location":"utils/#Real-World-Datasets","page":"Utilities","title":"Real-World Datasets","text":"","category":"section"},{"location":"utils/#air_passengers","page":"Utilities","title":"air_passengers","text":"Monthly airline passenger numbers (in thousands) from January 1949 to December 1960. This is the classic Box & Jenkins dataset exhibiting both trend and multiplicative seasonal patterns.\n\nusing Durbyn\n\nap = air_passengers()\nprintln(\"Length: \", length(ap))      # 144\nprintln(\"Range: \", extrema(ap))      # (104.0, 622.0)\n\n# Fit a model\nfit = ets(ap, 12, \"ZZZ\")\nfc = forecast(fit, h=12)\nplot(fc)\n\nProperties:\n\nFrequency: 12 (monthly)\nCharacteristics: Strong upward trend, multiplicative seasonality with increasing amplitude\n\nSource: Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). Time Series Analysis: Forecasting and Control.\n\n","category":"section"},{"location":"utils/#ausbeer","page":"Utilities","title":"ausbeer","text":"Quarterly Australian beer production in megalitres from Q1 1956 to Q2 2010.\n\nusing Durbyn\n\nbeer = ausbeer()\nprintln(\"Length: \", length(beer))    # 218\nprintln(\"Mean: \", round(mean(beer), digits=1))  # ~430 megalitres\n\n# Fit Holt-Winters\nfit = holt_winters(beer, 4)\nfc = forecast(fit, h=8)\nplot(fc)\n\nProperties:\n\nFrequency: 4 (quarterly)\nCharacteristics: Strong Q4 peak (summer in Australia), varying long-term trend\n\nSource: Australian Bureau of Statistics, Cat. 8301.0.55.001.\n\n","category":"section"},{"location":"utils/#lynx","page":"Utilities","title":"lynx","text":"Annual Canadian lynx trappings from 1821 to 1934 in the Mackenzie River district.\n\nusing Durbyn\n\nlynx_data = lynx()\nprintln(\"Length: \", length(lynx_data))  # 114\nprintln(\"Range: \", extrema(lynx_data))  # (39.0, 6991.0)\n\n# Good for demonstrating cyclic patterns\n# Note: This is annual data, so m=1 (no within-year seasonality)\n\nProperties:\n\nFrequency: 1 (annual)\nCharacteristics: Famous ~10-year population cycle, predator-prey dynamics\n\nSource: Campbell, M.J. & Walker, A.M. (1977). Journal of the Royal Statistical Society Series A, 140, 411-431.\n\n","category":"section"},{"location":"utils/#sunspots","page":"Utilities","title":"sunspots","text":"Monthly mean relative sunspot numbers showing the ~11-year solar cycle.\n\nusing Durbyn\n\nspots = sunspots()\nprintln(\"Length: \", length(spots))   # 235 (truncated)\nprintln(\"Max: \", maximum(spots))     # ~158\n\n# Demonstrates long cycles in time series\n\nProperties:\n\nFrequency: 12 (monthly)\nCharacteristics: ~11-year solar cycle, non-stationary\n\nNote: This is a truncated version (1749-1768). Full dataset available from SILSO.\n\nSource: World Data Center-SILSO, Royal Observatory of Belgium.\n\n","category":"section"},{"location":"utils/#pedestrian_counts","page":"Utilities","title":"pedestrian_counts","text":"Daily pedestrian counts from a city sensor location (2009-2016), exhibiting multiple seasonal patterns.\n\nusing Durbyn\n\npedestrians = pedestrian_counts()\nprintln(\"Length: \", length(pedestrians))  # 2922 (~8 years)\n\n# Ideal for testing multiple seasonality models\n# Weekly pattern (period=7) + Annual pattern (period=365)\nusing Durbyn.Bats\nfit = tbats(pedestrians, [7, 365.25])\nfc = forecast(fit, h=30)\n\nProperties:\n\nFrequency: Daily (multiple seasonal periods)\nCharacteristics:\nWeekly seasonality (period=7): Lower weekend traffic\nAnnual seasonality (period=365.25): Seasonal variations\nUpward trend\n\nSource: Simulated based on Melbourne Pedestrian Counting System patterns.\n\n","category":"section"},{"location":"utils/#Synthetic-Data-Generator","page":"Utilities","title":"Synthetic Data Generator","text":"","category":"section"},{"location":"utils/#simulate_seasonal_data","page":"Utilities","title":"simulate_seasonal_data","text":"Generate synthetic time series with configurable components for testing and experimentation.\n\nsimulate_seasonal_data(n=365; m=12, trend=true, seasonal_strength=1.0,\n                       noise_level=0.1, base_level=100.0, trend_coef=0.1)\n\nArguments:\n\nParameter Default Description\nn 365 Number of observations\nm 12 Seasonal period\ntrend true Include linear trend\nseasonal_strength 1.0 Seasonal amplitude multiplier\nnoise_level 0.1 Noise as fraction of base level\nbase_level 100.0 Base level of the series\ntrend_coef 0.1 Trend coefficient per observation\n\nCommon Frequency Values:\n\nm Data Type\n4 Quarterly\n7 Daily with weekly pattern\n12 Monthly\n24 Hourly with daily pattern\n52 Weekly with annual pattern\n365 Daily with annual pattern\n\nExamples:\n\nusing Durbyn\n\n# Monthly data similar to air_passengers\nmonthly = simulate_seasonal_data(144; m=12, base_level=100.0, trend_coef=0.5)\n\n# Quarterly data similar to ausbeer\nquarterly = simulate_seasonal_data(100; m=4, seasonal_strength=1.5)\n\n# Daily data with weekly pattern\ndaily = simulate_seasonal_data(365; m=7, base_level=1000.0)\n\n# No seasonality (for testing trend-only models)\ntrend_only = simulate_seasonal_data(100; m=1, seasonal_strength=0.0)\n\n# Strong seasonality, no trend\nseasonal_only = simulate_seasonal_data(200; m=12, trend=false, seasonal_strength=2.0)\n\nComplex Seasonality (Multiple Periods):\n\n# Daily data with both weekly and annual patterns\nn = 365 * 2\nweekly = simulate_seasonal_data(n; m=7, trend=false, base_level=0.0, seasonal_strength=0.5)\nannual = simulate_seasonal_data(n; m=365, trend=false, base_level=0.0, seasonal_strength=1.0)\ntrend_only = simulate_seasonal_data(n; m=1, seasonal_strength=0.0, base_level=100.0)\ncomplex_data = trend_only .+ weekly .+ annual\n\nGenerated Series Structure:\n\nY(t) = textBase + textTrend(t) + textSeasonal(t) + textNoise(t)\n\nwhere:\n\nBase = base_level\nTrend(t) = trend_coef * t (if trend=true)\nSeasonal(t) = seasonal_strength * base_level * sin(2π * t / m)\nNoise(t) ~ Normal(0, noise_level * base_level)\n\n","category":"section"},{"location":"utils/#Quick-Reference","page":"Utilities","title":"Quick Reference","text":"using Durbyn\n\n# Load all datasets\nap = air_passengers()      # Monthly, m=12, classic Box-Jenkins\nbeer = ausbeer()           # Quarterly, m=4, Australian beer\nlynx_data = lynx()         # Annual, m=1, cyclic pattern\nspots = sunspots()         # Monthly, m=12, solar cycle\npeds = pedestrian_counts() # Daily, m=[7, 365], multi-seasonal\n\n# Generate custom data\ncustom = simulate_seasonal_data(100; m=12, seasonal_strength=1.5)\n\n","category":"section"},{"location":"utils/#Data-Cleaning-Utilities","page":"Utilities","title":"Data Cleaning Utilities","text":"The Utils module provides functions for handling missing values (missing and NaN) in vectors and matrices.","category":"section"},{"location":"utils/#dropmissing","page":"Utilities","title":"dropmissing","text":"Remove missing values from a vector, or remove rows with missing values from a vector-matrix pair.\n\ndropmissing(x::AbstractVector)                    # drop missing/NaN from vector\ndropmissing(x::AbstractVector, X::AbstractMatrix)  # drop rows with missing/NaN from both\n\nExample:\n\nusing Durbyn.Utils: dropmissing\n\nx = [1.0, NaN, 3.0, missing, 5.0]\ndropmissing(x)  # [1.0, 3.0, 5.0]\n\n# Paired removal (vector + matrix)\nx = [1.0, NaN, 3.0]\nX = [1.0 2.0; 3.0 4.0; 5.0 6.0]\nx_clean, X_clean = dropmissing(x, X)","category":"section"},{"location":"utils/#ismissingish","page":"Utilities","title":"ismissingish","text":"Test whether a value is \"missing-like\" (missing or NaN). Follows Julia's is* predicate convention.\n\nismissingish(v)\n\nExample:\n\nusing Durbyn.Utils: ismissingish\n\nismissingish(missing)  # true\nismissingish(NaN)      # true\nismissingish(1.0)      # false\nismissingish(Inf)      # false","category":"section"},{"location":"utils/#completecases","page":"Utilities","title":"completecases","text":"Return a boolean vector indicating which elements are complete (neither missing nor NaN). Matches the DataFrames.jl naming convention.\n\ncompletecases(x::AbstractArray)\n\nExample:\n\nusing Durbyn.Utils: completecases\n\ncompletecases([1.0, missing, 3.0, NaN])  # [true, false, true, false]","category":"section"},{"location":"utils/#mean2","page":"Utilities","title":"mean2","text":"Compute the mean, with an option to skip missing values.\n\nmean2(x; skipmissing=false)\n\nExample:\n\nusing Durbyn.Utils: mean2\n\nmean2([1.0, 2.0, missing, 4.0]; skipmissing=true)  # 2.333...\n\n","category":"section"},{"location":"utils/#References","page":"Utilities","title":"References","text":"Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). Time Series Analysis: Forecasting and Control (5th ed.). Wiley.\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3rd ed.). OTexts.\nCampbell, M.J. & Walker, A.M. (1977). A Survey of statistical work on the Mackenzie River series. Journal of the Royal Statistical Society Series A, 140, 411-431.","category":"section"},{"location":"ararma/#ARARMA-Model","page":"ARARMA","title":"ARARMA Model","text":"tip: Formula Interface is the Recommended Approach\nThis page starts with the formula interface (recommended for most users), which provides declarative model specification with support for panel data and model comparison. The array interface (base models) is covered later. See the Grammar Guide for complete documentation.\n\n","category":"section"},{"location":"ararma/#Overview","page":"ARARMA","title":"Overview","text":"ARARMA extends the ARAR approach by first applying an adaptive AR prefilter to shorten memory (the ARAR stage), and then fitting a short-memory ARMA(p, q) model on the prefiltered residuals. The goal is to capture long/persistent structure via a composed AR filter and the remaining short-term dynamics via an ARMA kernel.\n\nWhen to use ARARMA:\n\nWhen residuals from AR-only models show MA structure\nFor capturing both long-memory and short-term dynamics\nWhen you need automatic model selection via auto_ararma\nFor series with complex autocorrelation patterns\n\nReference: Parzen, E. (1982). ARARMA Models for Time Series Analysis and Forecasting. Journal of Forecasting, 1(1), 67-82.\n\n","category":"section"},{"location":"ararma/#Formula-Interface","page":"ARARMA","title":"Formula Interface","text":"","category":"section"},{"location":"ararma/#Basic-Example","page":"ARARMA","title":"Basic Example","text":"using Durbyn\n\nseries = air_passengers()\ndata = (sales = series,)\n\n# Using ArarmaSpec for fit/forecast workflow\nspec = ArarmaSpec(@formula(sales = p(1) + q(2)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\nplot(fc)","category":"section"},{"location":"ararma/#Auto-ARARMA-(Model-Selection)","page":"ARARMA","title":"Auto ARARMA (Model Selection)","text":"# Auto ARARMA with default search ranges\nspec = ArarmaSpec(@formula(sales = p() + q()))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# Auto ARARMA with custom search ranges\nspec = ArarmaSpec(@formula(sales = p(0,3) + q(0,2)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# With custom ARAR parameters\nspec = ArarmaSpec(\n    @formula(sales = p() + q()),\n    max_ar_depth = 20,\n    max_lag = 30,\n    crit = :bic\n)\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"ararma/#Panel-Data-(Multiple-Series)","page":"ARARMA","title":"Panel Data (Multiple Series)","text":"# Create panel data with multiple regions\npanel_tbl = (\n    sales = vcat(series, series .* 1.05),\n    region = vcat(fill(\"north\", length(series)), fill(\"south\", length(series)))\n)\n\n# Wrap in PanelData for grouped fitting\npanel = PanelData(panel_tbl; groupby = :region, m = 12)\n\n# Fit to all groups\nspec = ArarmaSpec(@formula(sales = p(1) + q(1)))\ngroup_fit = fit(spec, panel)\n\n# Forecast all groups\ngroup_fc = forecast(group_fit, h = 6)\nplot(group_fc)","category":"section"},{"location":"ararma/#Model-Collections-(Benchmarking)","page":"ARARMA","title":"Model Collections (Benchmarking)","text":"ArarmaSpec slots into model collections for easy benchmarking against other forecasting methods:\n\n# Compare ARARMA against ARAR, ARIMA, and ETS\nmodels = model(\n    ArarmaSpec(@formula(sales = p() + q())),\n    ArarSpec(@formula(sales = arar())),\n    ArimaSpec(@formula(sales = p() + q() + P() + Q())),\n    EtsSpec(@formula(sales = e(\"Z\") + t(\"Z\") + s(\"Z\"))),\n    names = [\"ararma\", \"arar\", \"arima\", \"ets\"]\n)\n\n# Fit all models\nfitted_models = fit(models, panel)\n\n# Forecast with all models\nfc_models = forecast(fitted_models, h = 12)\n\n# Compare forecasts\nplot(fc_models)","category":"section"},{"location":"ararma/#Automatic-vs-Fixed-Order-Selection","page":"ARARMA","title":"Automatic vs Fixed Order Selection","text":"Automatic selection (uses auto_ararma):\n\nAny order term with a range triggers automatic model selection\np() or q() with no arguments use default search ranges\nExamples: p() + q(), p(0,3) + q(), p(1) + q(0,2)\n\nFixed orders (uses ararma directly - faster):\n\nWhen all orders are fixed, the formula interface calls ararma() directly\nMuch faster as it skips the search process\nExample: p(1) + q(2) fits ARARMA(1,2) directly\n\n","category":"section"},{"location":"ararma/#Model-Theory","page":"ARARMA","title":"Model Theory","text":"Given a univariate series Y_t t=12ldotsn, ARARMA produces a fitted model and forecasting mechanism that combine both stages.","category":"section"},{"location":"ararma/#Stage-1-Memory-Shortening-(ARAR)","page":"ARARMA","title":"Stage 1 - Memory Shortening (ARAR)","text":"As in ARAR, we iteratively test for long memory and, if detected, apply a memory-shortening AR filter. At iteration r, the procedure evaluates delays tau=1ldots15 by ordinary least squares and scores each delay by a relative error measure:\n\nmathrmERR(phitau)\n=\nfracdisplaystylesum_t=tau+1^nbig(Y_t-phiY_t-taubig)^2\n     displaystylesum_t=tau+1^nY_t^2\nqquad\nhatphi(tau)inargmin_phi mathrmERR(phitau)\n\nLet mathrmErr(tau) = mathrmERRbig(hatphi(tau)taubig) and hattau=argmin_tau mathrmErr(tau). Then:\n\nIf mathrmErr(hattau)le 8n or if hatphi(hattau)ge 09 with hattau2, declare long memory and filter:\ntilde Y_t = Y_t - hatphiY_t-hattau\nIf hatphi(hattau)ge 09 with hattauin12, fit an AR(2) at lags 1 and 2 and filter:\ntilde Y_t = Y_t - hatphi_1 Y_t-1 - hatphi_2 Y_t-2\nOtherwise, stop.\n\nEach successful reduction composes the prefilter polynomial Psi(B) (with Psi_0=1):\n\nS_t = Psi(B)Y_t = Y_t + Psi_1 Y_t-1 + cdots + Psi_k Y_t-k\nqquad\nPsi(B) = 1 + Psi_1 B + cdots + Psi_k B^k\n\nThe reduction loop terminates when short memory is reached or after three passes (rarely more than two are needed in practice).","category":"section"},{"location":"ararma/#Stage-2-Best-Lag-Subset-AR","page":"ARARMA","title":"Stage 2 - Best-Lag Subset AR","text":"Let X_t = S_t - bar S with bar S the sample mean of the final prefiltered series. Over 4-term lag tuples (1ijk) satisfying 1ijkle m (with m typically 13 or 26), we fit the subset AR:\n\nX_t = phi_1 X_t-1 + phi_i X_t-i + phi_j X_t-j + phi_k X_t-k + Z_t\nqquad Z_tsim mathrmWN(0sigma^2)\n\nYule-Walker equations (using sample autocorrelations hatrho(cdot) of X_t) yield the coefficients:\n\nbeginbmatrix\n1  hatrho(i-1)  hatrho(j-1)  hatrho(k-1)\nhatrho(i-1)  1  hatrho(j-i)  hatrho(k-i)\nhatrho(j-1)  hatrho(j-i)  1  hatrho(k-j)\nhatrho(k-1)  hatrho(k-i)  hatrho(k-j)  1\nendbmatrix\n\nbeginbmatrix\nphi_12ptphi_i2ptphi_j2ptphi_k\nendbmatrix\n=\nbeginbmatrix\nhatrho(1)2pthatrho(i)2pthatrho(j)2pthatrho(k)\nendbmatrix\n\nThe implied variance is\n\nsigma^2 = hatgamma(0)Big(1 - phi_1 hatrho(1) - phi_i hatrho(i) - phi_j hatrho(j) - phi_k hatrho(k)Big)\n\nwhere hatgamma(cdot) are sample autocovariances of X_t. The algorithm selects (1ijk) minimizing this sigma^2.\n\nDefine the composite AR kernel by convolving the prefilter with the selected subset AR:\n\nphi(B) = 1 - phi_1 B - phi_i B^i - phi_j B^j - phi_k B^k\nqquad\nxi(B) = Psi(B)phi(B)\n\nLet c = big(1-phi_1-phi_i-phi_j-phi_kbig)bar S be the AR intercept.","category":"section"},{"location":"ararma/#Stage-3-Short-Memory-ARMA(p,-q)-on-AR-Residuals","page":"ARARMA","title":"Stage 3 - Short-Memory ARMA(p, q) on AR Residuals","text":"Using the AR-only fit implied by xi(B) and c, compute residuals and fit an ARMA(p, q) by maximizing the conditional Gaussian likelihood. Denote the ARMA polynomials\n\nPhi(B) = 1 - varphi_1 B - cdots - varphi_p B^p\nqquad\nTheta(B) = 1 + theta_1 B + cdots + theta_q B^q\n\nThe ARMA stage estimates (varphi_1ldotsvarphi_ptheta_1ldotstheta_qsigma^2) via Nelder-Mead. The code log-parameterizes the variance for numerical stability.\n\nThe ARMA optimizer uses log-barrier penalties to enforce stability (sumvarphi  095, sumtheta  095).\n\nInformation criteria. With effective residual length n_texteff and k=p+q+1 parameters (including variance), the log-likelihood ell yields\n\nmathrmAIC=2k-2ell qquad mathrmBIC=(log n_texteff)k-2ell","category":"section"},{"location":"ararma/#Forecasting","page":"ARARMA","title":"Forecasting","text":"With the composite kernel xi(B) and intercept c from Stage 2, and the ARMA(p,q) layer from Stage 3, h-step-ahead forecasts are formed recursively. Let P_n Y_n+h denote the minimum-MSE predictor of Y_n+h. Writing xi(B)=1+xi_1 B+cdots+xi_K B^K,\n\nP_n Y_n+h\n=\n- sum_j=1^K xi_j  P_n Y_n+h-j\n+ c + text(MA terms from  Theta(B)text)\nqquad hge 1\n\nwith initialization P_n Y_n+h=Y_n+h for hle 0 and future shocks set to zero for the MA recursion. Forecast standard errors follow from the MA representation and the estimated innovation variance sigma^2.\n\n","category":"section"},{"location":"ararma/#Array-Interface-(Base-Model)","page":"ARARMA","title":"Array Interface (Base Model)","text":"using Durbyn\nusing Durbyn.Ararma\n\nap = air_passengers()\n\n# ARARMA with specified orders\nfit1 = ararma(ap, p = 4, q = 1)\nfc1 = forecast(fit1, h = 12)\nplot(fc1)\n\n# Automatic ARARMA order selection\nfit2 = auto_ararma(ap)\nfc2 = forecast(fit2, h = 12)\nplot(fc2)\n\n# Access model components\nprintln(fit1)               # Model summary\nfitted_vals = fitted(fit1)\nresid = residuals(fit1)","category":"section"},{"location":"ararma/#Function-Signatures","page":"ARARMA","title":"Function Signatures","text":"ararma(y::Vector{<:Real};\n       max_ar_depth::Int=26,\n       max_lag::Int=40,\n       p::Int=4,\n       q::Int=1,\n       options::NelderMeadOptions=NelderMeadOptions()) -> ArarmaModel\n\nArguments:\n\ny: A numeric vector containing the observed time series\nmax_ar_depth: Maximum lag depth for subset AR selection (default: 26)\nmax_lag: Maximum lag for autocovariance computation (default: 40)\np: AR order for ARMA stage (default: 4)\nq: MA order for ARMA stage (default: 1)\noptions: Nelder-Mead optimization options\n\nReturns: An ArarmaModel struct containing fitted model components\n\nauto_ararma(y::Vector{<:Real};\n            max_p::Int=4,\n            max_q::Int=2,\n            crit::Symbol=:aic,\n            max_ar_depth::Int=26,\n            max_lag::Int=40,\n            options::NelderMeadOptions=NelderMeadOptions()) -> ArarmaModel\n\nArguments:\n\ny: A numeric vector containing the observed time series\nmax_p: Maximum AR order to search (default: 4)\nmax_q: Maximum MA order to search (default: 2)\ncrit: Information criterion for selection, :aic or :bic (default: :aic)\n\nReturns: The best ArarmaModel according to the selected criterion\n\n","category":"section"},{"location":"ararma/#Comparison-with-ARAR","page":"ARARMA","title":"Comparison with ARAR","text":"Feature ARAR ARARMA\nReference Brockwell & Davis (2016) Parzen (1982)\nMemory shortening Yes (threshold 0.93) Yes (threshold 0.9)\nAR component Subset AR(4) via Yule-Walker Subset AR(4) via Yule-Walker\nMA component No Yes, ARMA(p,q) on residuals\nUse case Simple, robust forecasting Captures MA structure in residuals\n\nSee ARAR for the simpler AR-only model.\n\n","category":"section"},{"location":"ararma/#Reference","page":"ARARMA","title":"Reference","text":"Parzen, E. (1982). ARARMA Models for Time Series Analysis and Forecasting. Journal of Forecasting, 1(1), 67-82.","category":"section"},{"location":"expsmoothing/#Exponential-Smoothing-(ETS):-State-Space-Form,-Additive-and-Multiplicative-Models","page":"Exponential Smoothing","title":"Exponential Smoothing (ETS): State-Space Form, Additive & Multiplicative Models","text":"tip: Formula Interface is the Recommended Approach\nThis page starts with the formula interface (recommended for most users), which provides declarative model specification with EtsSpec, SesSpec, HoltSpec, HoltWintersSpec, and support for panel data and model comparison. The array interface (base models) is covered later. See the Grammar Guide for complete documentation.\n\nThis page summarizes the ETS state-space framework which is implemented in Durbyn.jl as ets() for automatic forecasting, and the admissible parameter regions for stability/forecastability. It includes both additive and multiplicative error models, following Hyndman et al. (2002, 2008).\n\n","category":"section"},{"location":"expsmoothing/#Model-taxonomy-and-notation","page":"Exponential Smoothing","title":"Model taxonomy and notation","text":"ETS models are categorized by (Error, Trend, Seasonality):\n\nANN / MNN — simple exponential smoothing (additive vs multiplicative error)  \nAAN / MAN — additive trend (Holt, with additive vs multiplicative error)  \nADN — damped additive trend (only additive error common in practice)  \nAAA / MAM — additive trend + additive/multiplicative seasonality  \nANA / AAA / ADA — seasonal additive-error forms (with no / additive / damped trend)  \nOther hybrids (e.g. multiplicative seasonality with additive error, damped multiplicative trend) can be defined analogously.\n\nWe use smoothing parameters alphabetagamma and damping phi (if present).   Additive vs multiplicative error models give the same point forecasts but different likelihoods and intervals.\n\n","category":"section"},{"location":"expsmoothing/#Additive-error-state-space-form","page":"Exponential Smoothing","title":"Additive error state-space form","text":"beginaligned\ntextbfObservationquad\n Y_t = Hx_t-1 + varepsilon_t \ntextbfStatequad\n x_t = Fx_t-1 + Gvarepsilon_t qquad varepsilon_t sim WN(0sigma^2)\nendaligned\n\nForecast mean and variance at horizon h:\n\nmu_n(h) = H F^h-1 x_n qquad\nv_n(h) = sigma^2left(1 + sum_j=1^h-1 (HF^j-1G)^2right)\n\n","category":"section"},{"location":"expsmoothing/#Multiplicative-error-form","page":"Exponential Smoothing","title":"Multiplicative error form","text":"For multiplicative error models:\n\nObservation:  \nY_t = hatY_t (1+varepsilon_t)\nwhere varepsilon_t sim WN(0sigma^2).\nKey property: Point forecasts are the same as additive-error models, but prediction intervals scale with the level.","category":"section"},{"location":"expsmoothing/#Examples","page":"Exponential Smoothing","title":"Examples","text":"MNN (no trend, no seasonality):\nY_t = ell_t-1(1+varepsilon_t) qquad\nell_t = ell_t-1(1+alphavarepsilon_t)\nMAN (additive trend):\nY_t = (ell_t-1+b_t-1)(1+varepsilon_t) \nell_t = (ell_t-1+b_t-1)(1+alphavarepsilon_t) \nb_t = b_t-1 + beta(ell_t-1+b_t-1)varepsilon_t\nMAM (additive trend + multiplicative seasonality):\nY_t = (ell_t-1+b_t-1)s_t-m(1+varepsilon_t) \nell_t = (ell_t-1+b_t-1)(1+alphavarepsilon_t) \nb_t = b_t-1+beta(ell_t-1+b_t-1)varepsilon_t \ns_t = s_t-m(1+gammavarepsilon_t)\n\nOther multiplicative combinations (e.g. damped trend, hybrid seasonality) follow analogously.\n\n","category":"section"},{"location":"expsmoothing/#Model-properties","page":"Exponential Smoothing","title":"Model properties","text":"Let M = F-GH.\n\nObservability: operatornamerank(H^top(F^top)H^topdots(F^top)^p-1H^top)=p  \nReachability: operatornamerank(GFGdotsF^p-1G)=p  \nStability: eigenvalues of M lie inside the unit circle  \nForecastability: weaker notion, unstable modes do not affect forecasts if orthogonal to forecast functional\n\nNon-seasonal additive/multiplicative ETS are minimal (reachable & observable).   Standard seasonal ETS are not (contain redundant seasonal states).\n\n","category":"section"},{"location":"expsmoothing/#Admissible-regions-(non-seasonal,-additive-and-multiplicative)","page":"Exponential Smoothing","title":"Admissible regions (non-seasonal, additive & multiplicative)","text":"For ANN/AAN/ADN (and their multiplicative analogues), the admissible stability regions are identical:\n\nANN / MNN\n0  alpha  2\nAAN / MAN\n0  alpha  2 qquad 0  beta  4-2alpha\nADN (damped additive trend)\n0  phi le 1 qquad\n1-tfrac1phi  alpha  1+tfrac1phi qquad\nalpha(phi-1)  beta  (1+phi)(2-alpha)\n\nThus, admissible regions do not depend on whether errors are additive or multiplicative.\n\n","category":"section"},{"location":"expsmoothing/#Seasonal-ETS","page":"Exponential Smoothing","title":"Seasonal ETS","text":"","category":"section"},{"location":"expsmoothing/#Standard-Holt–Winters-seasonal-form","page":"Exponential Smoothing","title":"Standard Holt–Winters seasonal form","text":"In ANA/AAA/ADA with recursion s_t=s_t-m+gammavarepsilon_t,   M has a unit eigenvalue → unstable, non-minimal.   Forecasts can remain valid (forecastable) but states are corrupted.\n\nCharacteristic polynomial factorization (ADA case):\n\nf(lambda) = (1-lambda)P(lambda)\n\nwith forecastability polynomial P(lambda) whose roots must lie inside the unit circle.   AAA is the special case phi=1.","category":"section"},{"location":"expsmoothing/#Normalized-seasonal-ETS","page":"Exponential Smoothing","title":"Normalized seasonal ETS","text":"Fix instability by imposing a sum-to-zero seasonal constraint each period:\n\nS(B)s_t = theta(B)gammavarepsilon_t\n\nwhere S(B)=1+B+cdots+B^m-1,   theta(B)=tfrac1m(m-1)+(m-2)B+cdots+B^m-2.\n\nOperationally: after updating seasonals, subtract the average of last m shocks.   This normalization restores stability.\n\n\n\ninfo: Need multiple seasonal cycles?\nThe bats models extend ETS with Box–Cox transforms, ARMA errors, damped trends, and multiple seasonal periods following De Livera, Hyndman & Snyder (2011).","category":"section"},{"location":"expsmoothing/#References","page":"Exponential Smoothing","title":"References","text":"Hyndman, Koehler, Snyder & Grose (2002). A state space framework for automatic forecasting using exponential smoothing methods.\nHyndman, Akram & Archibald (2006). The admissible parameter space for exponential smoothing models.\nHyndman, R.J., Koehler, A.B., Ord, J.K., Snyder, R.D. (2008) Forecasting with exponential smoothing: the state space approach, Springer-Verlag: New York. http://www.exponentialsmoothing.net\nHyndman and Athanasopoulos (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. https://otexts.com/fpp2/\n\n","category":"section"},{"location":"expsmoothing/#Formula-Interface-(Primary-Usage)","page":"Exponential Smoothing","title":"Formula Interface (Primary Usage)","text":"The formula interface provides a modern, declarative way to specify exponential smoothing models with full support for single series, model comparison, and panel data.","category":"section"},{"location":"expsmoothing/#Example-1:-Automatic-ETS-Selection","page":"Exponential Smoothing","title":"Example 1: Automatic ETS Selection","text":"Let the algorithm choose the best error, trend, and seasonal components:\n\nusing Durbyn\nusing Durbyn.Grammar\n\n# Load data\ndata = (sales = [120, 135, 148, 152, 141, 158, 170, 165, 180, 195],)\n\n# Automatic model selection (error, trend, seasonal all set to \"Z\" for automatic)\nspec = EtsSpec(@formula(sales = e(\"Z\") + t(\"Z\") + s(\"Z\")))\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)\nplot(fc)\n# Check selected model\nprintln(fitted.fit.method)  # Shows selected ETS model\n\n# Access fitted values and residuals\nfitted_values = fitted.fit.fitted\nresids = fitted.fit.residuals\n\nKey features:\n\ne(\"Z\"), t(\"Z\"), s(\"Z\") trigger automatic selection\nm = 12 specifies monthly seasonality\nSearches over all admissible ETS models","category":"section"},{"location":"expsmoothing/#Example-2:-Specific-ETS-Model","page":"Exponential Smoothing","title":"Example 2: Specific ETS Model","text":"Specify exact error, trend, and seasonal components:\n\n# ETS(A,A,M): Additive error, Additive trend, Multiplicative seasonality\nspec = EtsSpec(@formula(sales = e(\"A\") + t(\"A\") + s(\"M\")))\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)\nplot(fc)\n# ETS(M,Ad,M): Multiplicative error, Additive damped trend, Multiplicative seasonal\nspec_damped = EtsSpec(@formula(sales = e(\"M\") + t(\"A\") + s(\"M\") + drift()))\nfitted_damped = fit(spec_damped, data, m = 12)\nfc_damped = forecast(fitted_damped, h = 12)\nplot(fc_damped)\n# ETS(A,N,N): Simple exponential smoothing (additive error, no trend, no seasonality)\nspec_ses = EtsSpec(@formula(sales = e(\"A\") + t(\"N\") + s(\"N\")))\nfitted_ses = fit(spec_ses, data)\nfc_ses = forecast(fitted_ses, h = 12)\nplot(fc_ses)\n\nComponent specification:\n\nError: \"A\" (additive), \"M\" (multiplicative), \"Z\" (auto)\nTrend: \"N\" (none), \"A\" (additive), \"Ad\" (additive damped), \"M\" (multiplicative), \"Md\" (multiplicative damped), \"Z\" (auto)\nSeasonal: \"N\" (none), \"A\" (additive), \"M\" (multiplicative), \"Z\" (auto)","category":"section"},{"location":"expsmoothing/#Example-3:-Specialized-ETS-Specs","page":"Exponential Smoothing","title":"Example 3: Specialized ETS Specs","text":"Use convenience specs for common models:\n\n# Simple Exponential Smoothing (SES)\nspec_ses = SesSpec(@formula(sales = ses()))\nfitted_ses = fit(spec_ses, data)\nfc_ses = forecast(fitted_ses, h = 12)\nplot(fc_ses)\n\n# Holt's Linear Trend\nspec_holt = HoltSpec(@formula(sales = holt()))\nfitted_holt = fit(spec_holt, data)\nfc_holt = forecast(fitted_holt, h = 12)\nplot(fc_holt)\n# Holt's method with damped trend (recommended for long horizons)\nspec_holt_damped = HoltSpec(@formula(sales = holt(damped = true)))\nfitted_holt_damped = fit(spec_holt_damped, data)\nfc_holt_damped = forecast(fitted_holt_damped, h = 12)\nplot(fc_holt_damped)\n# Holt-Winters Seasonal\nap = (passengers = air_passengers(), )\nspec_hw = HoltWintersSpec(@formula(passengers = hw(seasonal=:additive)))\nfitted_hw = fit(spec_hw, ap, m = 12)\nfc_hw = forecast(fitted_hw, h = 12)\nplot(fc_hw)\n# Holt-Winters with multiplicative seasonality\nspec_hw_mult = HoltWintersSpec(@formula(passengers = hw(seasonal=:multiplicative)))\nfitted_hw_mult = fit(spec_hw_mult, ap, m = 12)\nfc_hw = forecast(fitted_hw_mult, h = 12)\nplot(fc_hw)\n\nSpecialized specs:\n\nSesSpec: Simple exponential smoothing\nHoltSpec: Linear trend (with optional damping)\nHoltWintersSpec: Seasonal models (additive or multiplicative)","category":"section"},{"location":"expsmoothing/#Example-4:-Fitting-Multiple-Models-Together","page":"Exponential Smoothing","title":"Example 4: Fitting Multiple Models Together","text":"Fit different ETS specifications and manually compare results:\n\nusing Durbyn\nusing Durbyn.Grammar\n\n# Create synthetic monthly sales data with trend and seasonality\nn = 72  # 6 years of monthly data\ntt = 1:n\ntrend = 100 .+ 2 .* tt\nseasonal = 20 .* sin.(2π .* tt ./ 12)  # Annual seasonality\nnoise = randn(n) .* 5\nsales_data = trend .+ seasonal .+ noise\n\n# Split into training and test sets\nn_test = 12\ntrain_sales = sales_data[1:(end - n_test)]\ntest_sales = sales_data[(end - n_test + 1):end]\n\n# Create data structure for training\ndata = (sales = train_sales,)\ntest = (sales = test_sales,)\n\n# Fit multiple ETS models at once\n# Fit multiple ETS models at once\n  models = model(\n      EtsSpec(@formula(sales = e(\"A\") + t(\"A\") + s(\"A\"))),           # Additive Holt-Winters\n      EtsSpec(@formula(sales = e(\"M\") + t(\"A\") + s(\"M\"))),           # Multiplicative seasonality\n      EtsSpec(@formula(sales = e(\"A\") + t(\"A\") + drift() + s(\"A\"))), # Damped trend\n      SesSpec(@formula(sales = ses())),                              # Simple exponential smoothing\n      HoltSpec(@formula(sales = holt())),                            # Holt's method\n      names = [\"hw_aaa\", \"ets_mam\", \"hw_damped\", \"ses\", \"holt\"]\n  )\n\n# Fit all models\nfitted = fit(models, data, m = 12)\n\n# Forecast with all models\nfc = forecast(fitted, h = 12)\n\n# Compare forecasts against test data\nacc = accuracy(fc, test)\nglimpse(acc)\n\n# Manually compare information criteria\nfor (name, model_result) in zip(models.names, fitted.models)\n    println(\"$name: AIC = $(round(model_result.fit.aic, digits=2)), BIC = $(round(model_result.fit.bic, digits=2))\")\nend\n\n# Plot forecasts (if plotting is available)\nplot(fc)\n\nfc_tbl = as_table(fc)\nglimpse(fc_tbl)\n\n\nKey features:\n\nGenerate synthetic data with trend and seasonality\nFit multiple ETS specifications at once\nMix different exponential smoothing methods\nCompare forecasts against held-out test data\nManually inspect AIC, BIC, and other diagnostics\nForecasts generated for all models\n\nAlternative damped trend specification:\n\n# Instead of using drift() in the formula, you can use the damped parameter\nEtsSpec(@formula(sales = e(\"A\") + t(\"A\") + s(\"A\")), damped=true)","category":"section"},{"location":"expsmoothing/#Example-5:-Panel-Data-(Multiple-Time-Series)","page":"Exponential Smoothing","title":"Example 5: Panel Data (Multiple Time Series)","text":"Fit ETS models to multiple series:\n\nnote: Optional Dependencies\n\n\nThis example requires CSV and Downloads packages, which are not installed by default with Durbyn.\n\nInstall them first:\n\nusing Pkg\nPkg.add([\"CSV\", \"Downloads\"])\n\nusing Durbyn\nusing Durbyn.ModelSpecs\nusing Durbyn.Grammar\nusing Downloads\nusing Tables\nusing CSV\n\n# Download and load data\npath = Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/refs/heads/main/Data/retail.csv\")\ntbl = Tables.columntable(CSV.File(path))\n\n# Reshape to long format\ntbl = pivot_longer(tbl; id_cols=:date, names_to=:series, values_to=:value)\n\nglimpse(tbl)\n\n# Split into train and test sets using table operations\n# Get unique dates to determine split point\nall_dates = unique(tbl.date)\nn_dates = length(all_dates)\nsplit_date = all_dates[end-11]  # Hold out last 12 periods for testing\n\n# Create train and test sets\ntrain = query(tbl, row -> row.date <= split_date)\ntest = query(tbl, row -> row.date > split_date)\n\nprintln(\"Training data:\")\nglimpse(train)\nprintln(\"\\nTest data:\")\nglimpse(test)\n\n# Create panel data wrapper for training\npanel = PanelData(train; groupby=:series, date=:date, m=12);\n\nglimpse(panel)\n\n# Fit automatic ETS to all series\nspec = EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\")))\nfitted = fit(spec, panel)\n\n# Forecast all series\nfc = forecast(fitted, h = 12)\n\n# Get tidy forecast table\nfc_tbl = as_table(fc)\n\nglimpse(fc_tbl)\n\n# Plot forecasts \nlist_series(fc)  # See what's available\nplot(fc)  # Quick look at first series\nplot(fc, series=:all, facet=true, n_cols=4)  # Overview\n\n# Detailed inspection\nplot(fc, series=\"series_1\", actual=test)\n\n\nPanel data features:\n\nFits separate model to each series\nAutomatic model selection for each series individually\nReturns structured output for all series\nEfficient for hundreds or thousands of series","category":"section"},{"location":"expsmoothing/#Example-6:-Box-Cox-Transformation","page":"Exponential Smoothing","title":"Example 6: Box-Cox Transformation","text":"Handle non-constant variance with Box-Cox transformation:\n\n# Automatic lambda selection\nspec = EtsSpec(@formula(sales = e(\"A\") + t(\"A\") + s(\"M\")))\nfitted = fit(spec, data, m = 12, lambda = \"auto\", biasadj = true)\n\n# Check selected lambda\nprintln(fitted.fit.lambda)\n\n# Manual lambda\nfitted_lambda = fit(spec, data, m = 12, lambda = 0.5)\n# Check fixed lambda\nprintln(fitted_lambda.fit.lambda)\n\n\n\nTransformation features:\n\nlambda = \"auto\" selects optimal transformation\nbiasadj = true applies bias adjustment to forecasts\nCommon values: 0 (log), 0.5 (square root), 1 (no transform)\n\n","category":"section"},{"location":"expsmoothing/#Array-Interface-(Base-Models)","page":"Exponential Smoothing","title":"Array Interface (Base Models)","text":"The array interface provides direct access to exponential smoothing engines for numeric vectors. ","category":"section"},{"location":"expsmoothing/#Simple-Exponential-Smoothing-(SES)","page":"Exponential Smoothing","title":"Simple Exponential Smoothing (SES)","text":"Simple exponential smoothing is the simplest form of exponential smoothing (equivalent to ETS(A,N,N) or ETS(M,N,N)), with no trend or seasonality components. It is suitable for forecasting data with no clear trend or seasonal pattern.","category":"section"},{"location":"expsmoothing/#Mathematical-Formulation","page":"Exponential Smoothing","title":"Mathematical Formulation","text":"","category":"section"},{"location":"expsmoothing/#Additive-Error-Form-(ANN)","page":"Exponential Smoothing","title":"Additive Error Form (ANN)","text":"beginaligned\nY_t = ell_t-1 + varepsilon_t \nell_t = ell_t-1 + alphavarepsilon_t\nendaligned\n\nwhere ell_t is the level at time t, alpha in (01) is the smoothing parameter, and varepsilon_t sim WN(0sigma^2).\n\nComponent form:\n\nell_t = alpha Y_t + (1-alpha)ell_t-1\n\nForecast function: The h-step ahead forecast is simply the last estimated level:\n\nhatY_n+hn = ell_n quad textfor all  h ge 1\n\nPrediction variance:\n\ntextVarhatY_n+hn = sigma^2 h","category":"section"},{"location":"expsmoothing/#Multiplicative-Error-Form-(MNN)","page":"Exponential Smoothing","title":"Multiplicative Error Form (MNN)","text":"beginaligned\nY_t = ell_t-1(1 + varepsilon_t) \nell_t = ell_t-1(1 + alphavarepsilon_t)\nendaligned\n\nPoint forecasts are identical to the additive form, but prediction intervals scale with the level.","category":"section"},{"location":"expsmoothing/#Admissible-Parameter-Space","page":"Exponential Smoothing","title":"Admissible Parameter Space","text":"For stability and forecastability:\n\n0  alpha  2\n\nIn practice, alpha is typically constrained to (01) for conventional exponential smoothing behavior.","category":"section"},{"location":"expsmoothing/#Usage","page":"Exponential Smoothing","title":"Usage","text":"The ses() function provides two initialization methods:\n\ninitial = \"optimal\" (default): Uses state-space optimization via ETS framework\ninitial = \"simple\": Uses conventional Holt-Winters initialization\n\nusing Durbyn\nusing Durbyn.ExponentialSmoothing\n\n# Load example data\ny = [10.5, 12.3, 11.8, 13.1, 12.9, 14.2, 13.8, 15.1, 14.7, 16.0]\n\n# Fit SES with optimal initialization\nses_model = ses(y)\n\n# Fit SES with specified alpha\nfit_fixed = ses(y, alpha = 0.3)\n\n# Fit SES with Box-Cox transformation\nfit_bc = ses(y, lambda = 0.5)\n\n# Generate forecasts\nfc = forecast(ses_model, h = 6)\n\n# For seasonal data (frequency m)\nmonthly_data = randn(60) .+ 100\nfit_seasonal = ses(monthly_data, 12)  # m = 12 for monthly data\nfc_seasonal = forecast(fit_seasonal, h = 12)","category":"section"},{"location":"expsmoothing/#Model-Output","page":"Exponential Smoothing","title":"Model Output","text":"The SES struct contains:\n\nfitted: Fitted values (one-step ahead predictions)\nresiduals: Residuals (observed - fitted)\ncomponents: Model components (level)\nx: Original time series data\npar: Model parameters (alpha)\ninitstate: Initial level estimate\nstates: Level estimates over time\nsigma2: Residual variance\naic, bic, aicc: Information criteria (when initial = \"optimal\")\nmse, amse: Mean squared error measures\nlambda: Box-Cox transformation parameter (if used)\nbiasadj: Boolean flag for bias adjustment","category":"section"},{"location":"expsmoothing/#When-to-Use-SES","page":"Exponential Smoothing","title":"When to Use SES","text":"Use simple exponential smoothing when:\n\nData exhibits no clear trend or seasonal pattern\nYou need quick, computationally efficient forecasts\nRecent observations should be weighted more heavily than older ones\nYou have limited data and want a parsimonious model\n\nLimitations:\n\nCannot capture trend or seasonality\nForecasts are constant (flat line)\nMay underperform for data with systematic patterns\n\nFor data with trend or seasonality, consider:\n\nHolt's method (holt()) for trended data\nHolt-Winters (hw()) for seasonal data\nETS (ets()) for automatic model selection\n\n","category":"section"},{"location":"expsmoothing/#Holt's-Linear-Trend-Method","page":"Exponential Smoothing","title":"Holt's Linear Trend Method","text":"Holt's method (also known as double exponential smoothing) extends SES to capture linear trends in time series data. It uses two smoothing parameters: α for the level and β for the trend component.","category":"section"},{"location":"expsmoothing/#Mathematical-Formulation-2","page":"Exponential Smoothing","title":"Mathematical Formulation","text":"","category":"section"},{"location":"expsmoothing/#Standard-Holt's-Method-(Additive-Trend)","page":"Exponential Smoothing","title":"Standard Holt's Method (Additive Trend)","text":"beginaligned\nY_t = ell_t-1 + b_t-1 + varepsilon_t \nell_t = alpha Y_t + (1-alpha)(ell_t-1 + b_t-1) \nb_t = beta(ell_t - ell_t-1) + (1-beta)b_t-1\nendaligned\n\nwhere ell_t is the level, b_t is the trend, alpha beta in (01) are smoothing parameters, and varepsilon_t sim WN(0sigma^2).\n\nComponent form:\n\nLevel: ell_t = alpha Y_t + (1-alpha)(ell_t-1 + b_t-1)\nTrend: b_t = beta(ell_t - ell_t-1) + (1-beta)b_t-1\n\nForecast function: The h-step ahead forecast incorporates the trend:\n\nhatY_n+hn = ell_n + h cdot b_n","category":"section"},{"location":"expsmoothing/#Damped-Trend","page":"Exponential Smoothing","title":"Damped Trend","text":"beginaligned\nY_t = ell_t-1 + phi b_t-1 + varepsilon_t \nell_t = alpha Y_t + (1-alpha)(ell_t-1 + phi b_t-1) \nb_t = beta(ell_t - ell_t-1) + (1-beta)phi b_t-1\nendaligned\n\nwhere phi in (01 is the damping parameter.\n\nForecast function:\n\nhatY_n+hn = ell_n + (phi + phi^2 + cdots + phi^h) b_n = ell_n + phifrac1-phi^h1-phib_n\n\nThe damping parameter controls how quickly the trend dampens:\n\nphi = 1: Standard Holt (no damping)\nphi  1: Damped trend (trend flattens out in forecasts)\n\nAdvantages of damped trend:\n\nMore realistic long-term forecasts\nPrevents unbounded linear extrapolation\nOften improves forecast accuracy for horizons h > 10","category":"section"},{"location":"expsmoothing/#Exponential-(Multiplicative)-Trend","page":"Exponential Smoothing","title":"Exponential (Multiplicative) Trend","text":"beginaligned\nY_t = ell_t-1 cdot b_t-1^phi + varepsilon_t \nell_t = alpha Y_t + (1-alpha) ell_t-1 cdot b_t-1^phi \nb_t = beta fracell_tell_t-1 + (1-beta) b_t-1^phi\nendaligned\n\nUsed when the trend grows/declines exponentially rather than linearly.","category":"section"},{"location":"expsmoothing/#Admissible-Parameter-Space-2","page":"Exponential Smoothing","title":"Admissible Parameter Space","text":"For standard Holt (no damping):\n\nbeginaligned\n0  alpha  2 \n0  beta  4 - 2alpha\nendaligned\n\nFor damped Holt (phi  1):\n\nbeginaligned\n0  phi le 1 \n1 - frac1phi  alpha  1 + frac1phi \nalpha(phi - 1)  beta  (1+phi)(2-alpha)\nendaligned","category":"section"},{"location":"expsmoothing/#Usage-2","page":"Exponential Smoothing","title":"Usage","text":"using Durbyn\nusing Durbyn.ExponentialSmoothing\n\n# Simulate data with linear trend\nt = 1:50\ny = 100 .+ 2 .* t .+ randn(50) .* 5\n\n# Standard Holt's method (m parameter optional since no seasonality)\nholt_model = holt(y)\nprintln(holt_model)\n\n# Generate forecasts\nfc = forecast(holt_model, h=10)\nplot(fc)\n\n# Damped trend (recommended for long horizons)\nfit_damped = holt(y, damped=true)\nfc_damped = forecast(fit_damped, h=24)\n\n# Holt with fixed parameters\nfit_fixed = holt(y, alpha=0.8, beta=0.2)\n\n# Exponential trend\nfit_exp = holt(y, exponential=true)\n\n# With Box-Cox transformation\nfit_bc = holt(y, lambda=\"auto\", biasadj=true)\n\n# Simple initialization\nfit_simple = holt(y, initial=\"simple\")\n\n# Can also specify m explicitly (though typically not needed)\nfit_explicit = holt(y, 1, damped=true)","category":"section"},{"location":"expsmoothing/#Model-Output-2","page":"Exponential Smoothing","title":"Model Output","text":"The Holt struct contains:\n\nfitted: Fitted values (one-step ahead predictions)\nresiduals: Residuals (observed - fitted)\ncomponents: Model components (level and trend)\nx: Original time series data\npar: Model parameters (alpha, beta, and phi if damped)\ninitstate: Initial level and trend estimates\nstates: Level and trend estimates over time\nsigma2: Residual variance\naic, bic, aicc: Information criteria (when initial = \"optimal\")\nmse, amse: Mean squared error measures\nlambda: Box-Cox transformation parameter (if used)\nbiasadj: Boolean flag for bias adjustment\nmethod: Method description (e.g., \"Holt's method\", \"Damped Holt's method\")","category":"section"},{"location":"expsmoothing/#When-to-Use-Holt's-Method","page":"Exponential Smoothing","title":"When to Use Holt's Method","text":"Use Holt's linear trend method when:\n\nData exhibits a clear linear trend (increasing or decreasing)\nNo seasonal pattern is present\nYou need to extrapolate the trend into the future\nRecent trend behavior should influence forecasts\n\nUse damped trends when:\n\nLong-horizon forecasts are needed (h > 10)\nThe trend may not continue indefinitely at the same rate\nYou want more conservative, realistic forecasts\nHistorical data shows trends that eventually flatten\n\nLimitations:\n\nCannot capture seasonality (use Holt-Winters hw() instead)\nAssumes trend is approximately linear\nWithout damping, forecasts can be unrealistic for long horizons\nMay overreact to recent trend changes\n\nComparison with SES:\n\nSES: No trend, forecasts are flat (constant)\nHolt: Linear trend, forecasts increase/decrease linearly\nDamped Holt: Trend that dampens, forecasts flatten over time\n\n","category":"section"},{"location":"expsmoothing/#Automatic-ETS-Model-Selection","page":"Exponential Smoothing","title":"Automatic ETS Model Selection","text":"using Durbyn\nusing Durbyn.ExponentialSmoothing\n# Fit automatically selected ETS model to a monthly series (m = 12)\nap = air_passengers()\nets_model = ets(ap(), 12, \"ZZZ\")\n\n# Specify a particular structure (multiplicative seasonality, additive trend, additive errors)\nfit2 = ets(ap, 12, \"AAM\")\nfc2 = forecast(fit2, h=12)\nplot(fc2)\n\n# Use a damped trend search and automatic Box–Cox selection\nfit3 = ets(ap, 12, \"ZZZ\"; damped=nothing, lambda=\"auto\", biasadj=true)\nfc3 = forecast(fit3, h=12)\nplot(fc3)","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"note: Note\nTable operations functions (glimpse, select, query, arrange, groupby, mutate, summarise, pivot_longer, pivot_wider) are documented in the Table Operations guide.","category":"section"},{"location":"diffusion/#Diffusion-Models:-Technology-Adoption-and-Market-Penetration-Forecasting","page":"Diffusion","title":"Diffusion Models: Technology Adoption and Market Penetration Forecasting","text":"Diffusion models are specialized forecasting tools designed to predict technology adoption, market penetration, and product life cycle dynamics. Unlike traditional time series methods that model temporal autocorrelation, diffusion models capture the underlying social dynamics of how innovations spread through a population.\n\nDurbyn implements four classic diffusion models:\n\nModel Best For Key Feature\nBass New product forecasting Separates innovators from imitators\nGompertz Biological/market growth Asymmetric S-curve with early inflection\nGSGompertz Flexible adoption patterns Generalized shape control\nWeibull Reliability/lifetime analysis Distribution-based flexibility\n\n","category":"section"},{"location":"diffusion/#Theory-and-Background","page":"Diffusion","title":"Theory and Background","text":"Diffusion of innovations theory (Rogers, 1962) describes how new ideas and technologies spread through social systems. The adoption process typically follows an S-curve pattern:\n\nIntroduction phase: Few early adopters (innovators)\nGrowth phase: Rapid adoption as word-of-mouth spreads\nMaturity phase: Market saturation approaches\nSaturation: Adoption levels off at market potential\n\nDiffusion models mathematically capture this dynamic by modeling cumulative adoption as a function of time and market potential.\n\n","category":"section"},{"location":"diffusion/#Model-Types","page":"Diffusion","title":"Model Types","text":"","category":"section"},{"location":"diffusion/#1.-Bass-Diffusion-Model","page":"Diffusion","title":"1. Bass Diffusion Model","text":"The Bass model (Bass, 1969) is the most widely used diffusion model in marketing science. It explicitly models two adoption mechanisms:\n\nInnovation effect (coefficient p): External influence from mass media, advertising\nImitation effect (coefficient q): Internal influence from word-of-mouth, social contagion","category":"section"},{"location":"diffusion/#Mathematical-Formulation","page":"Diffusion","title":"Mathematical Formulation","text":"Cumulative adoption:\n\nA_t = m cdot frac1 - e^-(p+q)t1 + fracqp e^-(p+q)t\n\nAdoption per period (first differences):\n\na_t = A_t - A_t-1\n\nHazard rate interpretation:\n\nfracf(t)1-F(t) = p + q cdot F(t)\n\nwhere the probability of adoption at time t given non-adoption is linear in cumulative adoption.","category":"section"},{"location":"diffusion/#Parameters","page":"Diffusion","title":"Parameters","text":"Parameter Symbol Description Typical Range\nMarket potential m Total eventual adopters > max(cumsum(y))\nInnovation coefficient p External influence rate 0.001 - 0.1\nImitation coefficient q Internal influence rate 0.1 - 0.5\n\nKey insight: The ratio q/p determines the shape of the adoption curve. Higher ratios produce more pronounced peaks.","category":"section"},{"location":"diffusion/#Decomposition","page":"Diffusion","title":"Decomposition","text":"The Bass model uniquely provides decomposition of adoption into innovator and imitator components:\n\ntextInnovators_t = p cdot (m - A_t)\n\ntextImitators_t = a_t - textInnovators_t\n\n","category":"section"},{"location":"diffusion/#2.-Gompertz-Growth-Curve","page":"Diffusion","title":"2. Gompertz Growth Curve","text":"The Gompertz model (Gompertz, 1825) produces an asymmetric S-curve with the inflection point occurring earlier than the midpoint. Originally developed for mortality modeling, it's widely used for biological and market growth phenomena.","category":"section"},{"location":"diffusion/#Mathematical-Formulation-2","page":"Diffusion","title":"Mathematical Formulation","text":"Cumulative adoption:\n\nA_t = m cdot e^-a cdot e^-b cdot t\n\nAdoption per period:\n\na_t = m cdot a cdot b cdot e^-bt cdot e^-a cdot e^-bt","category":"section"},{"location":"diffusion/#Parameters-2","page":"Diffusion","title":"Parameters","text":"Parameter Symbol Description Effect\nMarket potential m Asymptotic maximum Upper bound of curve\nDisplacement a X-axis translation Shifts curve horizontally\nGrowth rate b Steepness parameter Controls adoption speed\n\nKey property: The inflection point occurs at t* = ln(a)/b when cumulative adoption reaches m/e ≈ 0.368m.\n\n","category":"section"},{"location":"diffusion/#3.-Gamma/Shifted-Gompertz-(GSGompertz)","page":"Diffusion","title":"3. Gamma/Shifted Gompertz (GSGompertz)","text":"The Gamma/Shifted Gompertz model (Bemmaor, 1994) generalizes the Bass model by allowing heterogeneity in the imitation coefficient across adopters. This produces more flexible curve shapes.","category":"section"},{"location":"diffusion/#Mathematical-Formulation-3","page":"Diffusion","title":"Mathematical Formulation","text":"Cumulative adoption:\n\nA_t = m cdot (1 - e^-bt)(1 + a cdot e^-bt)^-c","category":"section"},{"location":"diffusion/#Parameters-3","page":"Diffusion","title":"Parameters","text":"Parameter Symbol Description Special Cases\nMarket potential m Total eventual adopters -\nShape a Heterogeneity parameter a = p/q (Bass relation)\nScale b Rate parameter b = p + q (Bass relation)\nShift c Distribution parameter c = 1 gives Bass-like curve\n\nConnection to Bass: When c = 1, the GSGompertz reduces to a form equivalent to the Bass model with a = p/q and b = p + q.\n\n","category":"section"},{"location":"diffusion/#4.-Weibull-Distribution-Model","page":"Diffusion","title":"4. Weibull Distribution Model","text":"The Weibull model (Sharif & Islam, 1980) uses the Weibull cumulative distribution function for adoption modeling. It's particularly useful when adoption follows reliability/lifetime distribution patterns.","category":"section"},{"location":"diffusion/#Mathematical-Formulation-4","page":"Diffusion","title":"Mathematical Formulation","text":"Cumulative adoption:\n\nA_t = m cdot left(1 - e^-(ta)^bright)\n\nAdoption per period:\n\na_t = fracm cdot ba cdot left(fractaright)^b-1 cdot e^-(ta)^b","category":"section"},{"location":"diffusion/#Parameters-4","page":"Diffusion","title":"Parameters","text":"Parameter Symbol Description Shape Effect\nMarket potential m Total eventual adopters Upper bound\nScale a Characteristic life Stretches/compresses time\nShape b Shape parameter b < 1: decreasing hazard; b = 1: exponential; b > 1: increasing hazard\n\n","category":"section"},{"location":"diffusion/#Parameter-Initialization","page":"Diffusion","title":"Parameter Initialization","text":"Durbyn provides two initialization strategies for optimization:","category":"section"},{"location":"diffusion/#Linearize-(Default)","page":"Diffusion","title":"Linearize (Default)","text":"Uses analytical methods to compute starting values:\n\nBass: Linear regression on y ~ Y + Y² where Y = cumsum(y), solving the resulting quadratic\nGompertz: Jukic et al. (2004) three-point method with Bass optimization for m\nGSGompertz: Derives from Bass parameters using Bemmaor's conversion formulas\nWeibull: Median-ranked OLS using Bernard's approximation","category":"section"},{"location":"diffusion/#Preset","page":"Diffusion","title":"Preset","text":"Uses fixed starting values (useful when analytical methods fail):\n\nModel Preset Values\nBass m=0.5, p=0.5, q=0.5 (scaled by 10*sum(y) for m)\nGompertz m=1, a=1, b=1 (scaled)\nGSGompertz m=0.5, a=0.5, b=0.5, c=0.5 (scaled)\nWeibull m=0.5, a=0.5, b=0.5 (scaled)\n\n","category":"section"},{"location":"diffusion/#Usage","page":"Diffusion","title":"Usage","text":"","category":"section"},{"location":"diffusion/#Basic-Fitting","page":"Diffusion","title":"Basic Fitting","text":"using Durbyn\nusing Durbyn.Diffusion\n\n# Sample adoption data (units per period)\ny = [5, 10, 25, 45, 70, 85, 75, 50, 30, 15]\n\n# Fit Bass diffusion model (default)\nfit = diffusion(y)\n# Or equivalently:\nfit = diffusion(y, model_type=Bass)\n\n# View results\nprintln(fit)\n\nOutput:\n\nDiffusion Model (Bass)\n─────────────────────────────\nObservations: 10\nParameters:\n  m: 485.123456\n  p: 0.032145\n  q: 0.387654\nMSE: 12.345678\nLoss function: L2\nOptimized on: cumulative","category":"section"},{"location":"diffusion/#Fitting-Different-Model-Types","page":"Diffusion","title":"Fitting Different Model Types","text":"# Gompertz model\nfit_gomp = diffusion(y, model_type=Gompertz)\n\n# Gamma/Shifted Gompertz\nfit_gsg = diffusion(y, model_type=GSGompertz)\n\n# Weibull model\nfit_weib = diffusion(y, model_type=Weibull)","category":"section"},{"location":"diffusion/#Forecasting","page":"Diffusion","title":"Forecasting","text":"# Fit model\nfit = diffusion(y, model_type=Bass)\n\n# Generate 5-period forecast\nfc = forecast(fit, h=5)\n\n# Access results\nfc.mean        # Point forecasts\nfc.lower[1]    # 80% lower prediction bounds\nfc.lower[2]    # 95% lower prediction bounds\nfc.upper[1]    # 80% upper prediction bounds\nfc.upper[2]    # 95% upper prediction bounds\n\n# Plot forecast\nplot(fc)","category":"section"},{"location":"diffusion/#Prediction-at-Specific-Time-Points","page":"Diffusion","title":"Prediction at Specific Time Points","text":"# Predict adoption for periods 1-20\nfit = diffusion(y, model_type=Bass)\npred = predict(fit, 1:20)\n\npred.adoption      # Adoption per period\npred.cumulative    # Cumulative adoption","category":"section"},{"location":"diffusion/#Fixed-Parameters","page":"Diffusion","title":"Fixed Parameters","text":"Fix specific parameters while estimating others:\n\n# Fix market potential at 500, estimate p and q\nfit = diffusion(y, model_type=Bass, w=(m=500.0, p=nothing, q=nothing))\n\n# Fix innovation coefficient, estimate m and q\nfit = diffusion(y, model_type=Bass, w=(m=nothing, p=0.03, q=nothing))\n\n# Fix all parameters (compute fit only)\nfit = diffusion(y, model_type=Bass, w=(m=500.0, p=0.03, q=0.38))","category":"section"},{"location":"diffusion/#Custom-Initial-Values","page":"Diffusion","title":"Custom Initial Values","text":"# Provide numeric initial values\nfit = diffusion(y, model_type=Bass, initpar=[500.0, 0.03, 0.38])\n\n# Use preset initialization\nfit = diffusion(y, model_type=Bass, initpar=\"preset\")","category":"section"},{"location":"diffusion/#Loss-Functions","page":"Diffusion","title":"Loss Functions","text":"# Mean Squared Error (default)\nfit_mse = diffusion(y, loss=2)\n\n# Mean Absolute Error (more robust to outliers)\nfit_mae = diffusion(y, loss=1)","category":"section"},{"location":"diffusion/#Handling-Leading-Zeros","page":"Diffusion","title":"Handling Leading Zeros","text":"# Data with leading zeros (common in early adoption)\ny = [0, 0, 0, 5, 10, 25, 45, 70, 85, 75]\n\n# Remove leading zeros before fitting (default)\nfit = diffusion(y, cleanlead=true)\nprintln(\"Offset: $(fit.offset)\")  # Number of zeros removed\n\n# Keep leading zeros\nfit = diffusion(y, cleanlead=false)","category":"section"},{"location":"diffusion/#Optimization-Options","page":"Diffusion","title":"Optimization Options","text":"# Custom optimization settings\nfit = diffusion(y,\n    model_type=Bass,\n    method=\"L-BFGS-B\",     # Optimization algorithm\n    maxiter=1000,          # Maximum iterations\n    mscal=true,            # Scale market potential for stability\n    cumulative=true        # Optimize on cumulative values\n)\n\n","category":"section"},{"location":"diffusion/#Accessing-Fitted-Results","page":"Diffusion","title":"Accessing Fitted Results","text":"The DiffusionFit struct contains comprehensive model information:\n\nfit = diffusion(y, model_type=Bass)\n\n# Model type\nfit.model_type        # Bass, Gompertz, GSGompertz, or Weibull\n\n# Fitted parameters\nfit.params            # NamedTuple: (m=..., p=..., q=...) for Bass\nfit.params.m          # Market potential\nfit.params.p          # Innovation coefficient (Bass)\nfit.params.q          # Imitation coefficient (Bass)\n\n# Fitted values\nfit.fitted            # Fitted adoption per period\nfit.cumulative        # Fitted cumulative adoption\nfit.residuals         # Residuals (actual - fitted)\n\n# Diagnostics\nfit.mse               # Mean squared error\nfit.loss              # Loss function used (1=MAE, 2=MSE)\nfit.optim_cumulative  # Whether optimized on cumulative\n\n# Data\nfit.y                 # Cleaned data (after removing leading zeros)\nfit.y_original        # Original data\nfit.offset            # Number of leading zeros removed\n\n# Initialization\nfit.init_params       # Initial parameters before optimization\n\n","category":"section"},{"location":"diffusion/#Bass-Model-Decomposition","page":"Diffusion","title":"Bass Model Decomposition","text":"The Bass model uniquely provides innovator/imitator decomposition:\n\nusing Durbyn.Diffusion\n\n# Generate Bass curve with decomposition\nn = 20\nm, p, q = 1000.0, 0.03, 0.38\n\ncurve = bass_curve(n, m, p, q)\n\ncurve.cumulative    # Cumulative adoption\ncurve.adoption      # Adoption per period\ncurve.innovators    # Innovation component\ncurve.imitators     # Imitation component\n\n# Verify: adoption = innovators + imitators\nall(curve.adoption .≈ curve.innovators .+ curve.imitators)  # true\n\n","category":"section"},{"location":"diffusion/#Curve-Generation-Functions","page":"Diffusion","title":"Curve Generation Functions","text":"Generate theoretical diffusion curves for visualization or analysis:\n\nusing Durbyn.Diffusion\n\n# Bass curve\ncurve = bass_curve(50, 1000.0, 0.03, 0.38)\n\n# Gompertz curve\ncurve = gompertz_curve(50, 1000.0, 5.0, 0.3)\n\n# GSGompertz curve\ncurve = gsgompertz_curve(50, 1000.0, 0.08, 0.41, 1.0)\n\n# Weibull curve\ncurve = weibull_curve(50, 1000.0, 15.0, 2.5)\n\n# All return NamedTuple with:\n# - cumulative: Vector of cumulative adoption\n# - adoption: Vector of adoption per period\n# - (Bass only) innovators, imitators: Decomposition\n\n","category":"section"},{"location":"diffusion/#Model-Comparison-Example","page":"Diffusion","title":"Model Comparison Example","text":"using Durbyn\nusing Durbyn.Diffusion\n\n# Historical smartphone adoption data (millions of units)\ny = [2, 8, 25, 55, 95, 140, 175, 195, 205, 210, 212, 213]\n\n# Fit all four models\nmodels = Dict(\n    \"Bass\" => diffusion(y, model_type=Bass),\n    \"Gompertz\" => diffusion(y, model_type=Gompertz),\n    \"GSGompertz\" => diffusion(y, model_type=GSGompertz),\n    \"Weibull\" => diffusion(y, model_type=Weibull)\n)\n\n# Compare MSE\nfor (name, fit) in models\n    println(\"$name MSE: $(round(fit.mse, digits=2))\")\nend\n\n# Forecast with best model\nbest_model = argmin(Dict(k => v.mse for (k, v) in models))\nfc = forecast(models[best_model], h=5)\nprintln(\"\\nBest model: $best_model\")\nprintln(\"Forecast: $(round.(fc.mean, digits=1))\")\n\n","category":"section"},{"location":"diffusion/#Use-Cases","page":"Diffusion","title":"Use Cases","text":"","category":"section"},{"location":"diffusion/#New-Product-Launch-Forecasting","page":"Diffusion","title":"New Product Launch Forecasting","text":"# Early sales data for new product (units/month)\nsales = [100, 250, 580, 1200, 2100, 3500, 5200, 6800]\n\n# Fit Bass model\nfit = diffusion(sales, model_type=Bass)\n\n# Forecast remaining product lifecycle\nfc = forecast(fit, h=24)\n\n# Key insights\nprintln(\"Estimated market potential: $(round(fit.params.m)) units\")\nprintln(\"Innovation coefficient (p): $(round(fit.params.p, digits=4))\")\nprintln(\"Imitation coefficient (q): $(round(fit.params.q, digits=4))\")\nprintln(\"Peak period (estimated): period $(argmax(fc.mean) + length(sales))\")","category":"section"},{"location":"diffusion/#Technology-Adoption-Analysis","page":"Diffusion","title":"Technology Adoption Analysis","text":"# Annual internet user growth (millions)\nusers = [16, 36, 70, 147, 248, 361, 513, 719, 1018, 1319]\n\n# Compare models\nfit_bass = diffusion(users, model_type=Bass)\nfit_gomp = diffusion(users, model_type=Gompertz)\n\n# Bass provides adoption dynamics insight\nprintln(\"Innovation effect (p): $(round(fit_bass.params.p, digits=4))\")\nprintln(\"Imitation effect (q): $(round(fit_bass.params.q, digits=4))\")\nprintln(\"Word-of-mouth multiplier (q/p): $(round(fit_bass.params.q/fit_bass.params.p, digits=1))x\")","category":"section"},{"location":"diffusion/#Market-Saturation-Analysis","page":"Diffusion","title":"Market Saturation Analysis","text":"# EV adoption data\nev_sales = [15, 45, 120, 280, 550, 920, 1400, 1950]\n\nfit = diffusion(ev_sales, model_type=Bass)\n\n# Current penetration\ncurrent_cumulative = sum(ev_sales)\nmarket_potential = fit.params.m\npenetration = current_cumulative / market_potential * 100\n\nprintln(\"Market potential: $(round(market_potential)) units\")\nprintln(\"Current penetration: $(round(penetration, digits=1))%\")\nprintln(\"Remaining market: $(round(market_potential - current_cumulative)) units\")\n\n","category":"section"},{"location":"diffusion/#API-Reference","page":"Diffusion","title":"API Reference","text":"","category":"section"},{"location":"diffusion/#Main-Functions","page":"Diffusion","title":"Main Functions","text":"# Fit diffusion model\ndiffusion(y; model_type=Bass, kwargs...) -> DiffusionFit\nfit_diffusion(y; model_type=Bass, kwargs...) -> DiffusionFit\n\n# Generate forecast\nforecast(fit::DiffusionFit; h::Int, level=[80, 95]) -> Forecast\n\n# Predict at specific times\npredict(fit::DiffusionFit, t::AbstractVector) -> NamedTuple","category":"section"},{"location":"diffusion/#Curve-Generation","page":"Diffusion","title":"Curve Generation","text":"bass_curve(n, m, p, q) -> NamedTuple\ngompertz_curve(n, m, a, b) -> NamedTuple\ngsgompertz_curve(n, m, a, b, c) -> NamedTuple\nweibull_curve(n, m, a, b) -> NamedTuple","category":"section"},{"location":"diffusion/#Initialization-Functions","page":"Diffusion","title":"Initialization Functions","text":"bass_init(y) -> NamedTuple\ngompertz_init(y; kwargs...) -> NamedTuple\ngsgompertz_init(y; kwargs...) -> NamedTuple\nweibull_init(y) -> NamedTuple","category":"section"},{"location":"diffusion/#fit_diffusion-Keyword-Arguments","page":"Diffusion","title":"fit_diffusion Keyword Arguments","text":"Argument Type Default Description\nmodel_type DiffusionModelType Bass Model to fit\ncleanlead Bool true Remove leading zeros\nw NamedTuple or Nothing nothing Fixed parameters\nloss Int 2 Loss function (1=MAE, 2=MSE)\ncumulative Bool true Optimize on cumulative values\nmscal Bool true Scale market parameter\nmaxiter Int 500 Maximum iterations\nmethod String \"L-BFGS-B\" Optimization method\ninitpar String or Vector \"linearize\" Initialization method\n\n","category":"section"},{"location":"diffusion/#References","page":"Diffusion","title":"References","text":"Bass, F.M. (1969). A new product growth for model consumer durables. Management Science, 15(5), 215-227.\nBemmaor, A.C. (1994). Modeling the diffusion of new durable goods: Word-of-mouth effect versus consumer heterogeneity. In G. Laurent et al. (Eds.), Research Traditions in Marketing. Kluwer Academic Publishers.\nGompertz, B. (1825). On the nature of the function expressive of the law of human mortality. Philosophical Transactions of the Royal Society, 115, 513-583.\nJukic, D., Kralik, G., & Scitovski, R. (2004). Least-squares fitting Gompertz curve. Journal of Computational and Applied Mathematics, 169, 359-375.\nRogers, E.M. (1962). Diffusion of Innovations. Free Press.\nSharif, M.N. & Islam, M.N. (1980). The Weibull distribution as a general model for forecasting technological change. Technological Forecasting and Social Change, 18(3), 247-256.","category":"section"},{"location":"arar/#ARAR-Model","page":"ARAR","title":"ARAR Model","text":"tip: Formula Interface is the Recommended Approach\nThis page starts with the formula interface (recommended for most users), which provides declarative model specification with support for panel data and model comparison. The array interface (base models) is covered later. See the Grammar Guide for complete documentation.\n\n","category":"section"},{"location":"arar/#Overview","page":"ARAR","title":"Overview","text":"The ARAR (AutoRegressive with Adaptive Reduction) model combines memory-shortening transformations with autoregressive modeling. It first reduces long memory in the input series via iterative filtering, then fits an AR model with adaptively selected lags to the shortened series.\n\nWhen to use ARAR:\n\nShort or nonstationary time series\nWhen conventional ARIMA methods are unstable or overfit\nWhen you want a pure AR model without MA components\nFast, robust forecasting with automatic lag selection\n\nReference: Brockwell, P.J., & Davis, R.A. (2016). Introduction to Time Series and Forecasting. Springer.\n\n","category":"section"},{"location":"arar/#Formula-Interface","page":"ARAR","title":"Formula Interface","text":"","category":"section"},{"location":"arar/#Basic-Example","page":"ARAR","title":"Basic Example","text":"using Durbyn\n\nseries = air_passengers()\ndata = (sales = series,)\n\n# Using ArarSpec for fit/forecast workflow\nspec = ArarSpec(@formula(sales = arar()))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\nplot(fc)","category":"section"},{"location":"arar/#Custom-Parameters","page":"ARAR","title":"Custom Parameters","text":"# Specify max_ar_depth and max_lag\nspec = ArarSpec(@formula(sales = arar(max_ar_depth=20, max_lag=30)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"arar/#Panel-Data-(Multiple-Series)","page":"ARAR","title":"Panel Data (Multiple Series)","text":"# Create panel data with multiple regions\npanel_tbl = (\n    sales = vcat(series, series .* 1.05),\n    region = vcat(fill(\"north\", length(series)), fill(\"south\", length(series)))\n)\n\n# Wrap in PanelData for grouped fitting\npanel = PanelData(panel_tbl; groupby = :region, m = 12)\n\n# Fit to all groups\nspec = ArarSpec(@formula(sales = arar()))\ngroup_fit = fit(spec, panel)\n\n# Forecast all groups\ngroup_fc = forecast(group_fit, h = 6)\nplot(group_fc)","category":"section"},{"location":"arar/#Model-Collections-(Benchmarking)","page":"ARAR","title":"Model Collections (Benchmarking)","text":"ArarSpec slots into model collections for easy benchmarking against other forecasting methods:\n\n# Compare ARAR against ARIMA and ETS\nmodels = model(\n    ArarSpec(@formula(sales = arar())),\n    ArimaSpec(@formula(sales = p() + q() + P() + Q())),\n    EtsSpec(@formula(sales = e(\"Z\") + t(\"Z\") + s(\"Z\"))),\n    names = [\"arar\", \"arima\", \"ets\"]\n)\n\n# Fit all models\nfitted_models = fit(models, panel)\n\n# Forecast with all models\nfc_models = forecast(fitted_models, h = 12)\n\n# Compare forecasts\nplot(fc_models)\n\n","category":"section"},{"location":"arar/#Model-Theory","page":"ARAR","title":"Model Theory","text":"The ARAR model applies a memory-shortening transformation; if the underlying process of a time series Y_t t=12ldotsn is \"long-memory\", it then fits an autoregressive model.","category":"section"},{"location":"arar/#Memory-Shortening","page":"ARAR","title":"Memory Shortening","text":"The model follows five steps to classify Y_t and take one of three actions:\n\nL: declare Y_t as long memory and form tilde Y_t = Y_t - hatphi Y_t-hattau\nM: declare Y_t as moderately long memory and form tilde Y_t = Y_t - hatphi_1 Y_t-1 - hatphi_2 Y_t-2\nS: declare Y_t as short memory.\n\nIf Y_t is declared L or M, the series is transformed again until the transformed series is classified as short memory. (At most three transformations are applied; in practice, more than two is rare.)","category":"section"},{"location":"arar/#Algorithm-Steps","page":"ARAR","title":"Algorithm Steps","text":"For each tau=12ldots15, find hatphi(tau) that minimizes\nmathrmERR(phitau) =\nfracdisplaystylesum_t=tau+1^nbig(Y_t-phiY_t-taubig)^2\n     displaystylesum_t=tau+1^nY_t^2\nthen set mathrmErr(tau)=mathrmERRbig(hatphi(tau)taubig) and choose hattau=argmin_taumathrmErr(tau).\nIf mathrmErr(hattau)le 8n, then Y_t is a long-memory series.\nIf hatphi(hattau)ge 093 and hattau2, then Y_t is a long-memory series.\nIf hatphi(hattau)ge 093 and hattauin12, then Y_t is a long-memory series.\nIf hatphi(hattau)093, then Y_t is a short-memory series.","category":"section"},{"location":"arar/#Subset-Autoregressive-Model","page":"ARAR","title":"Subset Autoregressive Model","text":"We now describe how ARAR fits an autoregression to the mean-corrected series X_t=S_t-bar S, t=k+1ldotsn, where S_t is the memory-shortened version of Y_t obtained above and bar S is the sample mean of S_k+1ldotsS_n.\n\nThe fitted model has the form\n\nX_t = phi_1 X_t-1 + phi_l_1 X_t-l_1 + phi_l_2 X_t-l_2 + phi_l_3 X_t-l_3 + Z_t\nqquad Z_t sim mathrmWN(0sigma^2)","category":"section"},{"location":"arar/#Yule-Walker-Equations","page":"ARAR","title":"Yule-Walker Equations","text":"The coefficients phi_j and the noise variance sigma^2 follow from the Yule-Walker equations for given lags l_1l_2l_3:\n\nbeginbmatrix\n1  hatrho(l_1-1)  hatrho(l_2-1)  hatrho(l_3-1)\nhatrho(l_1-1)  1  hatrho(l_2-l_1)  hatrho(l_3-l_1)\nhatrho(l_2-1)  hatrho(l_2-l_1)  1  hatrho(l_3-l_2)\nhatrho(l_3-1)  hatrho(l_3-l_1)  hatrho(l_3-l_2)  1\nendbmatrix\nbeginbmatrix\nphi_12pt\nphi_l_12pt\nphi_l_22pt\nphi_l_3\nendbmatrix\n=\nbeginbmatrix\nhatrho(1)2pt\nhatrho(l_1)2pt\nhatrho(l_2)2pt\nhatrho(l_3)\nendbmatrix\n\nsigma^2 = hatgamma(0)Big( 1 - phi_1hatrho(1) - phi_l_1hatrho(l_1) - phi_l_2hatrho(l_2) - phi_l_3hatrho(l_3) Big)\n\nwhere hatgamma(j) and hatrho(j), j=012ldots, are the sample autocovariances and autocorrelations of X_t. The algorithm computes phi(cdot) for each set of lags with 1l_1l_2l_3le m (m typically 13 or 26) and selects the model with minimal Yule-Walker estimate of sigma^2.","category":"section"},{"location":"arar/#Forecasting","page":"ARAR","title":"Forecasting","text":"If the short-memory filter found in the first step has coefficients Psi_0Psi_1ldotsPsi_k (kge0, Psi_0=1), then\n\nS_t = Psi(B)Y_t = Y_t + Psi_1 Y_t-1 + cdots + Psi_k Y_t-k\nqquad\nPsi(B) = 1 + Psi_1 B + cdots + Psi_k B^k \n\nIf the subset AR coefficients are phi_1phi_l_1phi_l_2phi_l_3 then, for X_t=S_t-bar S,\n\nphi(B)X_t = Z_t qquad\nphi(B) = 1 - phi_1 B - phi_l_1 B^l_1 - phi_l_2 B^l_2 - phi_l_3 B^l_3\n\nFrom the two displays above,\n\nxi(B)Y_t = phi(1)bar S + Z_t\nqquad xi(B) = Psi(B)phi(B)\n\nAssuming this model is appropriate and Z_t is uncorrelated with Y_j for jt, the minimum-MSE linear predictors P_n Y_n+h of Y_n+h (for nk+l_3) satisfy the recursion\n\nP_n Y_n+h = - sum_j=1^k+l_3 xi_j  P_n Y_n+h-j + phi(1)bar S qquad hge 1\n\nwith initial conditions P_n Y_n+h=Y_n+h for hle 0.\n\n","category":"section"},{"location":"arar/#Array-Interface-(Base-Model)","page":"ARAR","title":"Array Interface (Base Model)","text":"The array interface provides direct access to the ARAR fitting engine for working with numeric vectors.\n\nusing Durbyn\nusing Durbyn.Ararma\n\nap = air_passengers()\n\n# Basic ARAR with default parameters\narar_fit = arar(ap)\nfc = forecast(arar_fit, h = 12)\nplot(fc)\n\n# ARAR with custom parameters\narar_fit = arar(ap, max_ar_depth = 20, max_lag = 30)\nfc = forecast(arar_fit, h = 12)\nplot(fc)\n\n# Access model components\nprintln(arar_fit)           # Model summary\nfitted_vals = fitted(arar_fit)\nresid = residuals(arar_fit)","category":"section"},{"location":"arar/#Function-Signature","page":"ARAR","title":"Function Signature","text":"arar(y::AbstractArray;\n     max_ar_depth::Union{Int, Nothing}=nothing,\n     max_lag::Union{Int, Nothing}=nothing) -> ARAR\n\nArguments:\n\ny: A one-dimensional array containing the observed time series data\nmax_ar_depth: Maximum lag to consider when selecting the best 4-lag AR model (default: auto-selected based on series length)\nmax_lag: Maximum lag for computing autocovariances (default: auto-selected based on series length)\n\nReturns: An ARAR struct containing fitted model components\n\n","category":"section"},{"location":"arar/#Comparison-with-ARARMA","page":"ARAR","title":"Comparison with ARARMA","text":"Feature ARAR ARARMA\nReference Brockwell & Davis (2016) Parzen (1982)\nMemory shortening Yes (threshold 0.93) Yes (threshold 0.9)\nAR component Subset AR(4) via Yule-Walker Subset AR(4) via Yule-Walker\nMA component No Yes, ARMA(p,q) on residuals\nUse case Simple, robust forecasting Captures MA structure in residuals\n\nSee ARARMA for the extended model with ARMA fitting.\n\n","category":"section"},{"location":"arar/#Reference","page":"ARAR","title":"Reference","text":"Brockwell, Peter J., and Richard A. Davis. Introduction to Time Series and Forecasting. Springer (2016)","category":"section"},{"location":"grammar/#Durbyn-Grammar","page":"Grammar","title":"Durbyn Grammar","text":"Durbyn provides an expressive, composable grammar for defining forecasting models. This unified interface lets you describe ARIMA, SARIMA, and exponential smoothing models with concise, readable syntax using the @formula macro and specialized model specifications.\n\nFuture releases will extend this grammar to support additional statistical models (state space models, structural time series, etc.) and machine learning forecasting methods, all accessible through the same consistent interface.\n\n","category":"section"},{"location":"grammar/#Overview","page":"Grammar","title":"Overview","text":"The Durbyn grammar system consists of:\n\nFormula interface: Use @formula to declaratively specify model components\nModel specifications: Wrap formulas in specs like ArimaSpec, EtsSpec, SesSpec, etc.\nUnified fitting: Call fit(spec, data) with optional grouping for panel data\nConsistent forecasting: Use forecast(fitted, h) for both single and grouped models; external variables can be passed if the model supports them\n\nThis design eliminates manual tuning loops and provides a consistent interface across all model families.\n\n","category":"section"},{"location":"grammar/#ARIMA-Grammar","page":"Grammar","title":"ARIMA Grammar","text":"The ARIMA grammar lets you describe ARIMA and SARIMA models with flexible order specifications and exogenous variable support.","category":"section"},{"location":"grammar/#Formula-Basics","page":"Grammar","title":"Formula Basics","text":"Define the relationship between a response variable (target in ML terminology) and its ARIMA structure:\n\n@formula(sales = p() + d() + q())\n\nEvery formula requires a response variable (left-hand side; called target in ML) and one or more model components (right-hand side). Components may specify ARIMA orders, seasonal orders, or regressors (exogenous variables; called features in ML).","category":"section"},{"location":"grammar/#Non-Seasonal-Orders","page":"Grammar","title":"Non-Seasonal Orders","text":"Function Meaning Default or form\np() Non-seasonal AR order Search range 2–5\np(k) Fix AR order Uses k exactly\np(min,max) Search AR order range Searches min through max\nd() Differencing order (auto) auto_arima chooses\nd(k) Fix differencing order Uses k exactly\nq() Non-seasonal MA order Search range 2–5\nq(k) Fix MA order Uses k exactly\nq(min,max) Search MA order range Searches min through max\n\nAny range (min,max) triggers full auto_arima search. If all orders are fixed, the formula interface automatically calls the faster arima routine.","category":"section"},{"location":"grammar/#Seasonal-Orders","page":"Grammar","title":"Seasonal Orders","text":"Seasonal counterparts include P, D, and Q:\n\n@formula(sales = p() + d() + q() + P() + Q())\n\nFunction Meaning Default or form\nP() Seasonal AR order Search range 1–2\nP(k) Fix seasonal AR order Uses k exactly\nP(min,max) Search seasonal AR order range Searches min through max\nD() Seasonal differencing (auto) auto_arima chooses\nD(k) Fix seasonal differencing order Uses k exactly\nQ() Seasonal MA order Search range 1–2\nQ(k) Fix seasonal MA order Uses k exactly\nQ(min,max) Search seasonal MA order range Searches min through max\n\nRemember to provide the seasonal period m when fitting: fit(spec, data, m=12).","category":"section"},{"location":"grammar/#Exogenous-Regressors","page":"Grammar","title":"Exogenous Regressors","text":"","category":"section"},{"location":"grammar/#Explicit-Variables","page":"Grammar","title":"Explicit Variables","text":"Add regressors (features) by listing column names:\n\n@formula(sales = p() + q() + price + promotion)\n\nThese become VarTerms—during fitting, Durbyn pulls the matching columns from your data.","category":"section"},{"location":"grammar/#Automatic-Selection-(auto())","page":"Grammar","title":"Automatic Selection (auto())","text":"Use auto() to include all numeric columns as regressors, excluding the response variable (target), group columns, and optional date column:\n\n@formula(sales = auto())                    # pure auto ARIMA + automatic xregs\n@formula(sales = p() + q() + auto())        # combine with explicit ARIMA orders\n\nAutomatic selection is mutually exclusive with explicit exogenous variables or xreg_formula.","category":"section"},{"location":"grammar/#Complex-Designs-(xreg_formula)","page":"Grammar","title":"Complex Designs (xreg_formula)","text":"For interactions or transformations, supply a secondary formula when constructing ArimaSpec:\n\nspec = ArimaSpec(\n    @formula(sales = p() + q()),\n    xreg_formula = Formula(\"~ temperature * promotion + price^2\")\n)\n\nThe xreg_formula is evaluated via Utils.model_matrix, producing the necessary design matrix before fitting.","category":"section"},{"location":"grammar/#ARIMA-Examples","page":"Grammar","title":"ARIMA Examples","text":"Fixed orders (fast estimation):\n\nspec = ArimaSpec(@formula(sales = p(1) + d(1) + q(1)))\nfitted = fit(spec, (sales = y,))\n\nAuto ARIMA with search ranges:\n\nspec = ArimaSpec(@formula(sales = p(0,3) + d() + q(0,3)))\nfitted = fit(spec, (sales = y,))\n\nSeasonal model with exogenous variables:\n\nspec = ArimaSpec(@formula(sales = p() + d() + q() + P() + Q() + price + promotion), m = 12)\nfitted = fit(spec, data; m = 12)\n\nPanel data with automatic xreg:\n\nspec = ArimaSpec(@formula(value = p() + d() + q() + P() + Q() + auto()))\npanel = PanelData(tbl; groupby = :store, date = :date, m = 12)\nfitted = fit(spec, panel)\nfc = forecast(fitted, h = 12)\n\n","category":"section"},{"location":"grammar/#ETS-Grammar","page":"Grammar","title":"ETS Grammar","text":"The ETS grammar mirrors the ARIMA DSL, letting you describe exponential smoothing models with expressive, composable terms.","category":"section"},{"location":"grammar/#Formula-Basics-2","page":"Grammar","title":"Formula Basics","text":"Use @formula to define the response variable (target) and its ETS components:\n\n@formula(sales = e(\"A\") + t(\"N\") + s(\"N\"))\n\nEach term is created with helper functions (e, t, s, drift). The resulting formula feeds into EtsSpec.","category":"section"},{"location":"grammar/#Component-Functions","page":"Grammar","title":"Component Functions","text":"Function Meaning Accepted Codes\ne() Error component \"A\" additive, \"M\" multiplicative, \"Z\" auto\nt() Trend component \"N\" none, \"A\" additive, \"M\" multiplicative, \"Z\" auto\ns() Seasonal component \"N\" none, \"A\" additive, \"M\" multiplicative, \"Z\" auto\n\nExamples:\n\ne(\"A\")              # Additive errors\nt(\"M\")              # Multiplicative trend\ns(\"Z\")              # Auto-select seasonal type\n\nAny component you omit defaults to \"Z\" (automatic selection). Combine the components as needed for your model structure.","category":"section"},{"location":"grammar/#Damping-and-Drift","page":"Grammar","title":"Damping and Drift","text":"Use drift() to control trend damping:\n\nCall Effect\ndrift() Force a damped trend (damped = true)\ndrift(false) Forbid damping (damped = false)\ndrift(:auto) Let ETS decide (damped = nothing)\ndrift(\"auto\") Same as drift(:auto)\n\nYou can combine drift with any trend choice. When omitted, the ETS search decides whether to include damping.","category":"section"},{"location":"grammar/#Creating-EtsSpec","page":"Grammar","title":"Creating EtsSpec","text":"Construct the specification with your formula and optional keywords (passed through to ets):\n\nspec = EtsSpec(\n    @formula(sales = e(\"Z\") + t(\"A\") + s(\"A\") + drift()),\n    m = 12,           # seasonal period\n    ic = \"aicc\"       # information criterion for model selection\n)\n\nfitted = fit(spec, (sales = sales_vec,); m = 12)\nfc = forecast(fitted, h = 12)\n\nYou can override spec options at fit time—keywords supplied to fit take precedence over those stored in the specification.","category":"section"},{"location":"grammar/#ETS-Quick-Recipes","page":"Grammar","title":"ETS Quick Recipes","text":"Simple Exponential Smoothing (SES):\n\nspec = EtsSpec(@formula(value = e(\"A\") + t(\"N\") + s(\"N\")))\nfitted = fit(spec, (value = y,))\n\nHolt's Linear Trend:\n\nspec = EtsSpec(@formula(value = e(\"A\") + t(\"A\") + s(\"N\") + drift(false)))\nfitted = fit(spec, (value = y,))\n\nHolt-Winters (Additive), monthly seasonality:\n\nspec = EtsSpec(@formula(value = e(\"A\") + t(\"A\") + s(\"A\") + drift(:auto)), m = 12)\nfitted = fit(spec, (value = y,), m = 12)\n\nAuto ETS with grouped data:\n\nspec = EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\")))\nfitted = fit(spec, table; groupby = :store, m = 12)\nfc = forecast(fitted, h = 8)","category":"section"},{"location":"grammar/#Specialized-ETS-Shortcuts","page":"Grammar","title":"Specialized ETS Shortcuts","text":"You can also target specialized exponential smoothing families directly:\n\n# Simple Exponential Smoothing\nses_spec = SesSpec(@formula(value = ses()))\n\n# Holt's linear trend (damped trend forced on)\nholt_spec = HoltSpec(@formula(value = holt(damped=true)))\n\n# Holt-Winters with multiplicative seasonality\nhw_spec = HoltWintersSpec(@formula(value = hw(seasonal=\"multiplicative\")), m = 12)\n\n# Croston's intermittent-demand method\ncroston_spec = CrostonSpec(@formula(demand = croston()))\n\nThese specs share the same grouped/PanelData support as EtsSpec, and all options passed via the specification or fit keywords are forwarded to the underlying implementations.\n\n","category":"section"},{"location":"grammar/#Multi-Model-Fitting","page":"Grammar","title":"Multi-Model Fitting","text":"Use ModelCollection to fit multiple specifications simultaneously:\n\nusing Durbyn\nusing Durbyn.ModelSpecs\nusing Durbyn.Grammar\n\n# Long table with :series / :date / :value columns\npanel = PanelData(tbl; groupby = :series, date = :date, m = 12)\n\nmodels = model(\n    ArimaSpec(@formula(value = p() + q())),\n    BatsSpec(@formula(value = bats(seasonal_periods=12))),\n    EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\") + drift(:auto))),\n    SesSpec(@formula(value = ses())),\n    HoltSpec(@formula(value = holt(damped=true))),\n    HoltWintersSpec(@formula(value = hw(seasonal=\"multiplicative\")); m = 12),\n    CrostonSpec(@formula(value = croston())),\n    names = [\"arima\", \"bats\", \"ets_auto\", \"ses\", \"holt_damped\", \"hw_mul\", \"croston\"]\n)\n\nfitted = fit(models, panel)       # each spec fitted to every series\nfc     = forecast(fitted, h = 12) # ForecastModelCollection\n\nas_table(fc)                # stacked tidy table with model_name column\n\nas_table stacks every model (and group) with a model_name column, so downstream comparisons stay tidy. You can filter to a specific model or pivot wider using Durbyn.TableOps functions, or use other Julia packages like DataFrames.jl, DataFramesMeta.jl, or Query.jl.\n\n","category":"section"},{"location":"grammar/#Complete-End-to-End-Example","page":"Grammar","title":"Complete End-to-End Example","text":"Here's a comprehensive workflow demonstrating model comparison, forecasting, and accuracy evaluation with panel data:\n\nnote: Optional Dependencies\nThis example requires CSV and Downloads packages:using Pkg\nPkg.add([\"CSV\", \"Downloads\"])\n\nusing Durbyn, Durbyn.TableOps, Durbyn.Grammar\nusing CSV, Downloads, Tables\n\n# 1. Load and prepare data\npath = Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/refs/heads/main/Data/retail.csv\")\nwide = Tables.columntable(CSV.File(path))\n\n# Reshape to long format\ntbl = pivot_longer(wide; id_cols=:date, names_to=:series, values_to=:value)\nglimpse(tbl)\n\n# 2. Split into train and test sets\nall_dates = unique(tbl.date)\nsplit_date = all_dates[end-11]  # Hold out last 12 periods for testing\n\ntrain = query(tbl, row -> row.date <= split_date)\ntest = query(tbl, row -> row.date > split_date)\n\nprintln(\"Training data:\")\nglimpse(train)\nprintln(\"\\nTest data:\")\nglimpse(test)\n\n# 3. Create panel data wrapper\npanel = PanelData(train; groupby=:series, date=:date, m=12)\nglimpse(panel)\n\n# 4. Define multiple models for comparison\nmodels = model(\n    ArarSpec(@formula(value = arar())),                                # ARAR via grammar\n    BatsSpec(@formula(value = bats(seasonal_periods=12))),             # BATS with seasonality\n    ArimaSpec(@formula(value = p() + q())),                              # Auto ARIMA\n    EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\") + drift(:auto))),  # Auto ETS with drift\n    SesSpec(@formula(value = ses())),                                    # Simple exponential smoothing\n    HoltSpec(@formula(value = holt(damped=true))),                       # Damped Holt\n    HoltWintersSpec(@formula(value = hw(seasonal=:multiplicative))),     # Holt-Winters multiplicative\n    CrostonSpec(@formula(value = croston())),                            # Croston's method\n    names=[\"arar\", \"bats\", \"arima\", \"ets_auto\", \"ses\", \"holt_damped\", \"hw_mul\", \"croston\"]\n)\n\n# 5. Fit all models to all series\nfitted = fit(models, panel)\n\n# 6. Generate forecasts (h=12 to match test set)\nfc = forecast(fitted, h=12)\n\n# 7. Convert to tidy table format\nfc_tbl = as_table(fc)\nglimpse(fc_tbl)\n\n# 8. Calculate accuracy metrics across all models and series\nacc_results = accuracy(fc, test)\nprintln(\"\\nAccuracy by Series and Model:\")\nglimpse(acc_results)\n\n# 9. Visualization\nlist_series(fc)  # Show available series\n\n# Quick overview of all series for first model\nplot(fc, series=:all, facet=true, n_cols=4)\n\n# Detailed inspection with actual values from test set\nplot(fc, series=\"series_10\", actual=test)\n\n# 10. Find best and worst performing series\n# Filter accuracy results for a specific metric (e.g., MAPE)\nbest_series = acc_results.series[argmin(acc_results.MAPE)]\nworst_series = acc_results.series[argmax(acc_results.MAPE)]\n\n# Compare best vs worst performers\nplot(fc, series=[best_series, worst_series], facet=true, actual=test)\n\nKey Features Demonstrated:\n\nData preparation: Download, reshape, and split data using TableOps\nModel comparison: Fit 8 different forecasting methods simultaneously (ARAR, BATS, ARIMA, ETS, and classical methods)\nPanel forecasting: Automatic iteration over multiple time series\nTrain/test split: Proper out-of-sample evaluation\nAccuracy metrics: Compare model performance across series\nVisualization: Multiple plotting options for analysis\nTidy output: Structured forecast tables ready for downstream analysis\n\n","category":"section"},{"location":"grammar/#ARAR-Grammar","page":"Grammar","title":"ARAR Grammar","text":"The ARAR grammar exposes the arar() term so you can configure the adaptive-reduction model with the same declarative workflow as ARIMA and ETS.","category":"section"},{"location":"grammar/#Formula-term","page":"Grammar","title":"Formula term","text":"@formula(value = arar())                           # use defaults\n@formula(value = arar(max_ar_depth=20))            # custom depth\n@formula(value = arar(max_ar_depth=20, max_lag=40))\n\nBoth keywords are optional; if omitted, Durbyn derives appropriate values from the series length. Validation happens at macro-expansion time so mistakes are caught immediately.","category":"section"},{"location":"grammar/#Direct-formula-fitting","page":"Grammar","title":"Direct formula fitting","text":"using Durbyn\nusing Durbyn.Ararma\n\ndata = (value = air_passengers(),)\nformula = @formula(value = arar(max_lag=30))\narar_model = arar(formula, data)          # tables.jl compatible data\nfc  = forecast(arar_model; h = 12)\n\nThe estimator lives in the Durbyn.Ararma submodule, so call arar(formula, data) from there (either via using Durbyn.Ararma or Durbyn.Ararma.arar(...)). It works with any Tables.jl source and returns the familiar ARAR struct.","category":"section"},{"location":"grammar/#Model-specification-(ArarSpec)","page":"Grammar","title":"Model specification (ArarSpec)","text":"To leverage grouped fitting, forecasting, and model collections, wrap the formula in ArarSpec:\n\nspec = ArarSpec(@formula(value = arar(max_ar_depth=15)))\nfitted = fit(spec, data)\nfc = forecast(fitted; h = 8)\n\nFor panel data:\n\npanel = PanelData(tbl; groupby = :region)\ngroup_fit = fit(spec, panel)\ngroup_fc = forecast(group_fit; h = 6)\n\nAnd to compare against other specs:\n\nmodels = model(\n    ArarSpec(@formula(value = arar())),\n    ArimaSpec(@formula(value = p() + q())),\n    EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\"))),\n    names = [\"arar\", \"arima\", \"ets\"]\n)\n\nfitted = fit(models, panel)\nfc = forecast(fitted; h = 12)\n\nThe ARAR grammar therefore integrates seamlessly with every Durbyn workflow—single series, grouped/panel data, and large-scale model comparisons.\n\n","category":"section"},{"location":"grammar/#ARARMA-Grammar","page":"Grammar","title":"ARARMA Grammar","text":"The ARARMA grammar extends the ARAR approach by fitting a short-memory ARMA(p,q) model after the adaptive reduction stage. Like ARIMA, it uses the p() and q() terms to specify model orders, but the distinction comes from using ArarmaSpec instead of ArimaSpec.","category":"section"},{"location":"grammar/#Formula-terms","page":"Grammar","title":"Formula terms","text":"ARARMA reuses ARIMA's order grammar:\n\n@formula(value = p() + q())                    # auto selection with defaults\n@formula(value = p(1) + q(2))                  # fixed ARARMA(1,2)\n@formula(value = p(0,3) + q(0,2))              # search ranges\n\nKey differences from ARIMA:\n\nARARMA does not support d(), D(), P(), or Q() terms (differencing is handled by the ARAR stage)\nARARMA does not support exogenous regressors (no variables, no auto())\nARARMA adds ARAR-specific parameters: max_ar_depth and max_lag","category":"section"},{"location":"grammar/#Automatic-vs-Fixed-Order-Selection","page":"Grammar","title":"Automatic vs Fixed Order Selection","text":"If ANY order is a range → uses auto_ararma():\n\np() + q() → searches with defaults (p: 0-4, q: 0-2)\np(0,3) + q() → searches p ∈ {0,1,2,3}, q with defaults\np(1) + q(0,2) → searches q ∈ {0,1,2} with fixed p=1\n\nIf ALL orders are fixed → uses ararma() directly (faster):\n\np(1) + q(2) → fits ARARMA(1,2) without search","category":"section"},{"location":"grammar/#Direct-formula-fitting-2","page":"Grammar","title":"Direct formula fitting","text":"using Durbyn\nusing Durbyn.Ararma\n\ndata = (value = air_passengers(),)\n\n# Fixed ARARMA(1,2)\nformula = @formula(value = p(1) + q(2))\nararma_model = ararma(formula, data)\nfc = forecast(ararma_model; h = 12)\n\n# Auto ARARMA with custom parameters\nformula = @formula(value = p() + q())\nararma_model = ararma(formula, data, max_ar_depth=20, max_lag=30, crit=:bic)\nfc = forecast(ararma_model; h = 12)\n\nThe estimator lives in the Durbyn.Ararma submodule. It works with any Tables.jl source and returns an ArarmaModel struct.","category":"section"},{"location":"grammar/#Model-specification-(ArarmaSpec)","page":"Grammar","title":"Model specification (ArarmaSpec)","text":"To leverage grouped fitting, forecasting, and model collections, wrap the formula in ArarmaSpec:\n\n# Fixed ARARMA(2,1)\nspec = ArarmaSpec(@formula(value = p(2) + q(1)))\nfitted = fit(spec, data)\nfc = forecast(fitted; h = 8)\n\n# Auto ARARMA with custom ARAR parameters\nspec = ArarmaSpec(\n    @formula(value = p() + q()),\n    max_ar_depth = 20,\n    max_lag = 30,\n    crit = :bic\n)\nfitted = fit(spec, data)\nfc = forecast(fitted; h = 12)\n\nFor panel data:\n\npanel = PanelData(tbl; groupby = :region, m = m)\nspec = ArarmaSpec(@formula(value = p(1) + q(1)))\ngroup_fit = fit(spec, panel)\ngroup_fc = forecast(group_fit; h = 6)\n\nAnd to compare against other specs:\n\nmodels = model(\n    ArarmaSpec(@formula(value = p() + q())),\n    ArarSpec(@formula(value = arar())),\n    ArimaSpec(@formula(value = p() + q() + P() + Q())),\n    EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\"))),\n    names = [\"ararma\", \"arar\", \"arima\", \"ets\"]\n)\n\nfitted = fit(models, panel)\nfc = forecast(fitted; h = 12)\n\nThe ARARMA grammar therefore integrates seamlessly with every Durbyn workflow—single series, grouped/panel data, and large-scale model comparisons.\n\n","category":"section"},{"location":"grammar/#BATS-Grammar","page":"Grammar","title":"BATS Grammar","text":"The BATS grammar provides a declarative interface for Box-Cox transformation, ARMA errors, Trend, and Seasonal components models. BATS is designed for complex seasonal patterns with integer seasonal periods and supports automatic component selection.","category":"section"},{"location":"grammar/#Formula-Terms","page":"Grammar","title":"Formula Terms","text":"The bats() term supports flexible seasonal and component specifications:\n\n@formula(value = bats())                                      # use defaults (auto selection)\n@formula(value = bats(seasonal_periods=12))                   # single seasonal period\n@formula(value = bats(seasonal_periods=[24, 168]))            # multiple seasonal periods\n@formula(value = bats(seasonal_periods=12, use_box_cox=true)) # specify Box-Cox\n@formula(value = bats(\n    seasonal_periods=12,\n    use_box_cox=true,\n    use_trend=true,\n    use_damped_trend=false,\n    use_arma_errors=true\n))\n\nAvailable parameters:\n\nseasonal_periods: Int or Vector{Int} for seasonal period(s)\nuse_box_cox: Bool, Vector{Bool}, or nothing (auto selection)\nuse_trend: Bool, Vector{Bool}, or nothing (auto selection)\nuse_damped_trend: Bool, Vector{Bool}, or nothing (auto selection)\nuse_arma_errors: Bool to enable ARMA error modeling (default: true)\n\nWhen component parameters are nothing, BATS searches over both true and false options and selects the best combination using AIC.","category":"section"},{"location":"grammar/#Direct-Formula-Fitting","page":"Grammar","title":"Direct Formula Fitting","text":"using Durbyn\n\ndata = (sales = randn(120) .+ 10,)\n\n# BATS with monthly seasonality\nformula = @formula(sales = bats(seasonal_periods=12))\nbats_model = bats(formula, data)  # Works with Tables.jl compatible data\nfc = forecast(bats_model, h = 12)\n\n# BATS with multiple seasonal periods and Box-Cox\nformula = @formula(sales = bats(\n    seasonal_periods=[24, 168],\n    use_box_cox=true\n))\nbats_model = bats(formula, data)\nfc = forecast(bats_model, h = 12)\n\nThe bats function in the Durbyn.Bats module directly accepts formulas, making it easy to work with Tables.jl-compatible data sources.","category":"section"},{"location":"grammar/#Model-Specification-(BatsSpec)","page":"Grammar","title":"Model Specification (BatsSpec)","text":"To leverage grouped fitting, forecasting, and model collections, wrap the formula in BatsSpec:\n\nusing Durbyn.ModelSpecs\n\n# Basic BATS with auto selection\nspec = BatsSpec(@formula(value = bats()))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# BATS with monthly seasonality\nspec = BatsSpec(@formula(value = bats(seasonal_periods=12)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# BATS with specific components\nspec = BatsSpec(@formula(value = bats(\n    seasonal_periods=12,\n    use_box_cox=true,\n    use_trend=true,\n    use_damped_trend=true,\n    use_arma_errors=true\n)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\n\n# Additional options at fit time\nfitted = fit(spec, data, bc_lower=0.0, bc_upper=1.5, biasadj=true)","category":"section"},{"location":"grammar/#Panel-Data","page":"Grammar","title":"Panel Data","text":"BATS integrates with panel data for multi-series forecasting:\n\n# Create panel data (Tables.jl compatible)\ntbl = (\n    date = repeat(1:120, 3),\n    product = repeat([\"A\", \"B\", \"C\"], inner=120),\n    sales = randn(360) .+ 10\n)\n\n# Fit BATS to each product\nspec = BatsSpec(@formula(sales = bats(seasonal_periods=12)))\nfitted = fit(spec, tbl, groupby = :product)\nfc = forecast(fitted, h = 12)\n\n# Or use PanelData wrapper\npanel = PanelData(tbl; groupby = :product, date = :date, m = 12)\nfitted = fit(spec, panel)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"grammar/#Model-Comparison","page":"Grammar","title":"Model Comparison","text":"Compare BATS against other forecasting methods:\n\nmodels = model(\n    BatsSpec(@formula(value = bats(seasonal_periods=12))),\n    ArimaSpec(@formula(value = p() + q() + P() + Q())),\n    EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\"))),\n    ArarSpec(@formula(value = arar())),\n    names = [\"bats\", \"arima\", \"ets\", \"arar\"]\n)\n\n# Fit all models\nfitted = fit(models, panel)\nfc = forecast(fitted, h = 12)\n\n# Convert to tidy table\nfc_table = as_table(fc)\n\n# Compare accuracy\nacc = accuracy(fc, test_data)","category":"section"},{"location":"grammar/#Multiple-Seasonal-Periods","page":"Grammar","title":"Multiple Seasonal Periods","text":"BATS excels at handling multiple seasonal patterns:\n\n# Hourly data with daily (24) and weekly (168) seasonality\nspec = BatsSpec(@formula(demand = bats(seasonal_periods=[24, 168])))\nfitted = fit(spec, hourly_data)\nfc = forecast(fitted, h = 168)  # Forecast one week ahead\n\n# Half-hourly data with daily (48) and weekly (336) seasonality\nspec = BatsSpec(@formula(load = bats(seasonal_periods=[48, 336])))\nfitted = fit(spec, half_hourly_data)\nfc = forecast(fitted, h = 48)  # Forecast one day ahead","category":"section"},{"location":"grammar/#Use-Cases","page":"Grammar","title":"Use Cases","text":"When to use BATS:\n\nMultiple integer seasonal periods (e.g., hourly data with daily and weekly patterns)\nNeed variance stabilization (Box-Cox transformation)\nNon-constant variance over time\nComplex error structures requiring ARMA modeling\n\nBATS vs TBATS:\n\nUse BATS for integer seasonal periods (faster, exact representation)\nUse TBATS for non-integer periods, very long seasonal cycles, or dual-calendar effects\nTBATS uses Fourier representation for more efficient handling of long seasonality\n\nComponent Selection Tips:\n\nLet use_box_cox=nothing (default) search over both options if variance changes\nUse use_arma_errors=true when residuals show autocorrelation\nSet use_damped_trend=true when trend shouldn't grow indefinitely\nSpecify bc_lower and bc_upper to constrain Box-Cox lambda search","category":"section"},{"location":"grammar/#Integration-with-Durbyn-Workflows","page":"Grammar","title":"Integration with Durbyn Workflows","text":"The BATS grammar integrates seamlessly with all Durbyn features:\n\n# Single series\nspec = BatsSpec(@formula(sales = bats(seasonal_periods=12)))\nfitted = fit(spec, (sales = y,))\nfc = forecast(fitted, h = 12)\n\n# Grouped data with parallel processing\nfitted = fit(spec, df, groupby = :store, parallel = true)\n\n# Model collection for comparison\nmodels = model(\n    BatsSpec(@formula(sales = bats(seasonal_periods=12))),\n    ArimaSpec(@formula(sales = p() + q() + P() + Q())),\n    names = [\"bats\", \"arima\"]\n)\nfitted = fit(models, df, groupby = :store)\nfc = forecast(fitted, h = 12)\nfc_tbl = as_table(fc)\n\n","category":"section"},{"location":"grammar/#Croston-Grammar","page":"Grammar","title":"Croston Grammar","text":"The Croston grammar enables declarative specification of intermittent demand forecasting models through the same unified interface as ARIMA and ETS. Croston methods are designed for time series with many zero values and sporadic non-zero demands, common in spare parts inventory and slow-moving items.\n\nWhat is Intermittent Demand? Intermittent demand series exhibit:\n\nMany zero values (typically >50% zeros)\nSporadic, irregular non-zero demands\nUnpredictable timing between demand occurrences\n\nStandard forecasting methods (ARIMA, ETS) struggle with such data because they assume continuous patterns and cannot properly model the dual nature of intermittent demand: magnitude (how much) and timing (when).","category":"section"},{"location":"grammar/#Formula-Terms-2","page":"Grammar","title":"Formula Terms","text":"The croston() term supports multiple method variants and configuration options:\n\n# Default: Croston method (Shenstone & Hyndman 2005)\n@formula(demand = croston())\n\n# Syntetos-Boylan Approximation - RECOMMENDED (bias-corrected)\n@formula(demand = croston(method=\"sba\"))\n\n# Shale-Boylan-Johnston - Alternative bias correction\n@formula(demand = croston(method=\"sbj\"))\n\n# Classical Croston (1972) - Original method with modern optimization\n@formula(demand = croston(method=\"classic\"))\n\n# With custom optimization parameters (IntermittentDemand module)\n@formula(demand = croston(\n    method=\"sba\",\n    cost_metric=\"mar\",\n    number_of_params=2,\n    optimize_init=true\n))\n\nThe method parameter determines which algorithm to use, while additional parameters control optimization behavior for advanced methods.","category":"section"},{"location":"grammar/#Method-Variants","page":"Grammar","title":"Method Variants","text":"Four Croston method variants are available:\n\nMethod Description Module Best For Bias Correction\n\"sba\" ⭐ Syntetos-Boylan Approximation IntermittentDemand Default choice - bias-corrected, best accuracy 1 - α/2\n\"sbj\" Shale-Boylan-Johnston IntermittentDemand Alternative if SBA over-forecasts 1 - α/(2-α)\n\"classic\" Classical Croston (1972) IntermittentDemand Original method with modern optimization None (biased)\n\"hyndman\" Croston (Shenstone & Hyndman 2005) ExponentialSmoothing Standard implementation, fixed alpha None\n\nWhy Bias Correction Matters: The classical Croston method systematically over-forecasts due to Jensen's inequality when computing the ratio of smoothed demand to smoothed intervals. Both SBA and SBJ apply correction factors to reduce this bias, with empirical studies showing significant accuracy improvements.\n\nRecommendation: Start with method=\"sba\" - it's the most validated and generally performs best. Only consider SBJ if SBA shows consistent over-forecasting in your validation studies.","category":"section"},{"location":"grammar/#IntermittentDemand-Parameters","page":"Grammar","title":"IntermittentDemand Parameters","text":"When using \"classic\", \"sba\", or \"sbj\" methods, additional parameters control the optimization process (based on Kourentzes 2014 recommendations):\n\n@formula(demand = croston(\n    method = \"sba\",                  # Method variant\n    cost_metric = \"mar\",             # Loss function: \"mar\", \"msr\", \"mae\", \"mse\"\n    number_of_params = 2,            # 1 or 2 smoothing parameters\n    optimize_init = true,            # Optimize initial states\n    init_strategy = \"mean\",          # \"mean\" or \"naive\" initialization\n    rm_missing = false               # Remove missing values\n))\n\nParameter Details:\n\ncost_metric (default: \"mar\"): Optimization loss function\n\"mar\": Mean Absolute Rate error (recommended)\n\"msr\": Mean Squared Rate error (recommended)\n\"mae\": Mean Absolute Error (classical)\n\"mse\": Mean Squared Error (classical)\nnumber_of_params (default: 2): Number of smoothing parameters\n1: Single parameter for both demand size and intervals\n2: Separate parameters (recommended for better accuracy)\noptimize_init (default: true): Optimize initial state values\ntrue: Optimize starting values (recommended, especially for short series)\nfalse: Use heuristic initialization\ninit_strategy (default: \"mean\"): Initial value strategy\n\"mean\": Use mean of non-zero demands and intervals\n\"naive\": Use first observed values\nrm_missing (default: false): Handle missing values\ntrue: Remove missing observations\nfalse: Keep all observations\n\nNote: These parameters only apply to \"classic\", \"sba\", and \"sbj\" methods. They are ignored for method=\"hyndman\".","category":"section"},{"location":"grammar/#Direct-Formula-Fitting-2","page":"Grammar","title":"Direct Formula Fitting","text":"For single-series analysis, you can fit directly using the formula interface:\n\nusing Durbyn\n\n# Intermittent demand data (many zeros)\ndata = (demand = [6, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 3, 0],)\n\n# Recommended: Syntetos-Boylan Approximation (bias-corrected)\nspec = CrostonSpec(@formula(demand = croston(method=\"sba\")))\nfit_sba = fit(spec, data)\nfc_sba = forecast(fit_sba, h = 12)\nplot(fc_sba)\n\n# Alternative: Shale-Boylan-Johnston correction\nspec_sbj = CrostonSpec(@formula(demand = croston(method=\"sbj\")))\nfit_sbj = fit(spec_sbj, data)\nfc_sbj = forecast(fit_sbj, h = 12)\n\n# Classical Croston with custom optimization parameters\nspec_classic = CrostonSpec(@formula(demand = croston(\n    method = \"classic\",\n    cost_metric = \"msr\",        # Mean Squared Rate\n    number_of_params = 2,       # Separate smoothing parameters\n    optimize_init = true        # Optimize initial values\n)))\nfit_classic = fit(spec_classic, data)\nfc_classic = forecast(fit_classic, h = 12)\n\n# Compare methods\nprintln(\"SBA forecast:     \", mean(fc_sba.mean))\nprintln(\"SBJ forecast:     \", mean(fc_sbj.mean))\nprintln(\"Classic forecast: \", mean(fc_classic.mean))\n\nImplementation Details: CrostonSpec automatically routes to the appropriate estimator:\n\nmethod = \"hyndman\" → ExponentialSmoothing.croston (simple baseline)\nmethod = \"classic\", \"sba\", \"sbj\" → Durbyn.IntermittentDemand (advanced methods with optimization)\n\nNo additional modules need to be loaded beyond using Durbyn.","category":"section"},{"location":"grammar/#Panel-Data-and-Grouped-Fitting","page":"Grammar","title":"Panel Data and Grouped Fitting","text":"CrostonSpec integrates seamlessly with panel data for multi-product forecasting:\n\nusing Durbyn, CSV, Downloads, Tables\n\n# Load intermittent demand data with multiple products\n# Data should have columns: product_id, date, demand\npanel = PanelData(tbl; groupby = :product_id, date = :date)\n\n# Fit SBA to all products (automatically parallelized)\nspec = CrostonSpec(@formula(demand = croston(method=\"sba\")))\nfitted = fit(spec, panel)\n\n# Generate forecasts for all products\nfc = forecast(fitted, h = 12)\n\n# Convert to tidy table for analysis\nfc_table = as_table(fc)\nglimpse(fc_table)","category":"section"},{"location":"grammar/#Model-Comparison-2","page":"Grammar","title":"Model Comparison","text":"Compare Croston variants with other forecasting methods:\n\n# Define multiple models\nmodels = model(\n    CrostonSpec(@formula(demand = croston(method=\"sba\"))),\n    CrostonSpec(@formula(demand = croston(method=\"sbj\"))),\n    CrostonSpec(@formula(demand = croston(method=\"classic\"))),\n    SesSpec(@formula(demand = ses())),\n    EtsSpec(@formula(demand = e(\"Z\") + t(\"Z\") + s(\"N\"))),\n    names = [\"croston_sba\", \"croston_sbj\", \"croston_classic\", \"ses\", \"ets\"]\n)\n\n# Fit all models to panel data\nfitted = fit(models, panel)\n\n# Generate forecasts with all methods\nfc = forecast(fitted, h = 12)\n\n# Compare accuracy against test data\nacc_results = accuracy(fc, test_data)\n\n# Find best performing model\nbest_model = acc_results.model_name[argmin(acc_results.MAPE)]\nprintln(\"Best model: \", best_model)\n\n# Visualize comparison\nplot(fc, series = \"product_123\", actual = test_data)","category":"section"},{"location":"grammar/#Use-Cases-and-Best-Practices","page":"Grammar","title":"Use Cases and Best Practices","text":"When to use Croston methods:\n\nTime series with >50% zero values\nSporadic, irregular demand patterns\nSpare parts and slow-moving inventory\nSpecialty products with infrequent sales\n\nMethod selection:\n\n\"sba\" (Syntetos-Boylan Approximation): Best choice for most applications\n\"sbj\" (Shale-Boylan-Johnston): Alternative bias correction, try if SBA underperforms\n\"classic\": Historical comparison or when bias correction is not needed\n\"hyndman\": Quick baseline, simpler implementation\n\nParameter recommendations (Kourentzes 2014):\n\nUse cost_metric = \"mar\" or \"msr\" instead of classical MSE/MAE\nEnable number_of_params = 2 for separate smoothing of size and intervals\nSet optimize_init = true especially for short time series\nLet optimization run without restrictive parameter bounds\n\nIntegration tips:\n\nCroston works seamlessly with PanelData for multi-product forecasting\nCombine with other methods in ModelCollection for comprehensive comparison\nUse as_table() for tidy output ready for downstream analysis\nThe Croston grammar integrates with all Durbyn workflows—single series, grouped data, and model comparison","category":"section"},{"location":"grammar/#References","page":"Grammar","title":"References","text":"Croston, J. (1972). \"Forecasting and stock control for intermittent demands\". Operational Research Quarterly, 23(3), 289-303.\nSyntetos, A.A. and Boylan, J.E. (2005). \"The accuracy of intermittent demand estimates\". International Journal of Forecasting, 21(2), 303-314.\nKourentzes, N. (2014). \"On Intermittent Demand Model Optimisation and Selection\". International Journal of Production Economics, 156, 180-190.\n\n","category":"section"},{"location":"grammar/#Tips-and-Best-Practices","page":"Grammar","title":"Tips and Best Practices","text":"","category":"section"},{"location":"grammar/#ARIMA-Tips","page":"Grammar","title":"ARIMA Tips","text":"Any range triggers automatic model selection\nFixed orders call fast direct estimation\nExogenous support includes explicit columns, auto(), or complex formulas\nCombine with PanelData to store group/date metadata cleanly\nIf you omit newdata when forecasting, Durbyn reuses each group's most recent exogenous values","category":"section"},{"location":"grammar/#ETS-Tips","page":"Grammar","title":"ETS Tips","text":"Always specify m (seasonal period) when you expect seasonal behavior. If you omit it, ETS defaults to m = 1\nKeywords like lambda, alpha, or ic are forwarded directly to the underlying ets implementation\nGrouped fits reuse the same grammar—fit(spec, data; groupby = [:region]) returns GroupedFittedModels\nForecast works the same way for both single and grouped models","category":"section"},{"location":"grammar/#General-Tips","page":"Grammar","title":"General Tips","text":"Use PanelData to encapsulate grouping, date, and seasonal period information\nSpecifications are reusable—define once, fit to multiple datasets\nKeywords in fit() override those stored in the spec\nas_table() provides tidy output for downstream analysis and visualization\nCombine multiple specs in a ModelCollection for easy model comparison","category":"section"},{"location":"intermittent/#Intermittent-Demand-Forecasting","page":"Intermittent Demand","title":"Intermittent Demand Forecasting","text":"","category":"section"},{"location":"intermittent/#Overview","page":"Intermittent Demand","title":"Overview","text":"Intermittent demand occurs in time series with many zero values and occasional non-zero spikes, commonly found in spare parts inventory, specialty products, and slow-moving items. Traditional forecasting methods like ARIMA or exponential smoothing perform poorly on such data due to the preponderance of zeros and the sporadic nature of demand occurrences.\n\nThe Croston family of methods addresses intermittent demand by decomposing the forecasting problem into separate components: demand size when it occurs and demand timing (intervals or probabilities). This decomposition enables more accurate modeling of the underlying demand process.","category":"section"},{"location":"intermittent/#Croston's-Method","page":"Intermittent Demand","title":"Croston's Method","text":"Croston's method models intermittent demand by maintaining two exponentially smoothed states: demand size z_t when demand occurs, and inter-demand intervals x_t.\n\nReferences:\n\nCroston, J. (1972). \"Forecasting and stock control for intermittent demands\". Operational Research Quarterly, 23(3), 289-303.\nShenstone, L., and Hyndman, R.J. (2005). \"Stochastic models underlying Croston's method for intermittent demand forecasting\". Journal of Forecasting, 24, 389-402.","category":"section"},{"location":"intermittent/#Notation","page":"Intermittent Demand","title":"Notation","text":"y_t: observed demand at time t (often zero)\nz_t: non-zero demand size (observed only when y_t  0)\nx_t: inter-demand interval (time between non-zero demands)\nhatz_t hatx_t: exponentially smoothed estimates\nalpha_z alpha_x in (01: smoothing parameters for size and interval\nq: number of non-zero demands observed up to time t","category":"section"},{"location":"intermittent/#Update-Equations","page":"Intermittent Demand","title":"Update Equations","text":"The exponential smoothing updates occur only when demand is observed (y_t  0):\n\nhatz_q = alpha_z z_t + (1-alpha_z)hatz_q-1\n\nhatx_q = alpha_x x_t + (1-alpha_x)hatx_q-1\n\nwhere x_t is the time since the previous non-zero demand.","category":"section"},{"location":"intermittent/#Forecast","page":"Intermittent Demand","title":"Forecast","text":"The Croston forecast represents the expected demand rate per period:\n\nhaty_t+h = frachatz_qhatx_q quad h geq 1\n\nThis forecast is constant for all future periods (flat forecast profile).","category":"section"},{"location":"intermittent/#Implementation-Types","page":"Intermittent Demand","title":"Implementation Types","text":"The implementation handles four distinct cases based on the data characteristics:\n\nCrostonOne: All demands are zero - returns zero forecast\nCrostonTwo: Only one non-zero demand and one interval - returns constant forecast\nCrostonThree: Insufficient data (≤1 demand or ≤1 interval) - returns NaN\nCrostonFour: Standard case with multiple demands - applies full Croston method","category":"section"},{"location":"intermittent/#Data-Structures","page":"Intermittent Demand","title":"Data Structures","text":"","category":"section"},{"location":"intermittent/#CrostonFit","page":"Intermittent Demand","title":"CrostonFit","text":"Stores the fitted Croston model containing:\n\nmodely: Simple exponential smoothing model for demand sizes\nmodelp: Simple exponential smoothing model for inter-demand intervals\ntype: One of four CrostonType enum values indicating the fitting approach\nx: Original demand series\ny: Non-zero demands only\ntt: Inter-demand intervals\nm: Seasonal period (typically 1 for non-seasonal intermittent demand)","category":"section"},{"location":"intermittent/#CrostonForecast","page":"Intermittent Demand","title":"CrostonForecast","text":"Stores forecast output containing:\n\nmean: Forecast values (demand rate per period)\nmodel: The underlying CrostonFit object\nmethod: Description string (\"Croston's Method\")\nm: Seasonal period","category":"section"},{"location":"intermittent/#Functions","page":"Intermittent Demand","title":"Functions","text":"","category":"section"},{"location":"intermittent/#croston(y,-m;-alpha,-options)","page":"Intermittent Demand","title":"croston(y, m; alpha, options)","text":"Fits a Croston model to intermittent demand data.\n\nArguments:\n\ny::AbstractArray: Demand time series (may contain zeros)\nm::Int: Seasonal period (default: 1)\nalpha::Union{Float64,Bool,Nothing}: Smoothing parameter (nothing for automatic optimization)\noptions::NelderMeadOptions: Optimization settings for parameter estimation\n\nReturns:\n\nCrostonFit: Fitted model object\n\nExample:\n\nusing Durbyn.ExponentialSmoothing\n\n# Intermittent demand data\ndemand = [0, 0, 5, 0, 0, 3, 0, 0, 0, 7, 0, 0, 4, 0, 0]\n\n# Fit Croston model with automatic parameter optimization\ncroston_model = croston(demand)\n\n# Fit with fixed smoothing parameter\nfit_fixed = croston(demand, alpha=0.1)","category":"section"},{"location":"intermittent/#forecast(object::CrostonFit,-h::Int)","page":"Intermittent Demand","title":"forecast(object::CrostonFit, h::Int)","text":"Generates forecasts from a fitted Croston model.\n\nArguments:\n\nobject::CrostonFit: Fitted Croston model\nh::Int: Forecast horizon (number of periods ahead)\n\nReturns:\n\nCrostonForecast: Forecast object containing mean forecasts\n\nExample:\n\n# Generate 12-period-ahead forecast\nfc = forecast(fit, 12)\nprintln(fc.mean)  # Access forecast values","category":"section"},{"location":"intermittent/#fitted(object::CrostonFit)","page":"Intermittent Demand","title":"fitted(object::CrostonFit)","text":"Computes in-sample fitted values using one-step-ahead forecasts.\n\nArguments:\n\nobject::CrostonFit: Fitted Croston model\n\nReturns:\n\nVector: Fitted values (same length as original series)\n\nNote: The first value is NaN as no forecast is available for the first observation.\n\nExample:\n\nfitted_vals = fitted(fit)\nresiduals = demand .- fitted_vals","category":"section"},{"location":"intermittent/#plot(forecast::CrostonForecast;-show_fittedfalse)","page":"Intermittent Demand","title":"plot(forecast::CrostonForecast; show_fitted=false)","text":"Visualizes the forecast with optional fitted values.\n\nArguments:\n\nforecast::CrostonForecast: Forecast object to plot\nshow_fitted::Bool: Whether to display in-sample fitted values (default: false)\n\nReturns:\n\nPlots.jl plot object\n\nExample:\n\n# Plot forecast only\nplot(fc)\n\n# Plot forecast with fitted values\nplot(fc, show_fitted=true)","category":"section"},{"location":"intermittent/#Syntetos-Boylan-Approximation-(SBA)","page":"Intermittent Demand","title":"Syntetos-Boylan Approximation (SBA)","text":"The SBA method applies a bias correction to Croston's forecast. Syntetos and Boylan (2005) showed that Croston's method produces biased forecasts and proposed the following correction:\n\nhaty_t+h = left(1 - fracalpha_x2right) frachatz_qhatx_q\n\nThis correction reduces the upward bias inherent in the original Croston method.","category":"section"},{"location":"intermittent/#Teunter-Syntetos-Babai-(TSB)-Method","page":"Intermittent Demand","title":"Teunter-Syntetos-Babai (TSB) Method","text":"The TSB method reformulates the intermittent demand problem by modeling demand occurrence probability p_t and demand size z_t separately. This method provides an alternative theoretical framework but is not currently implemented in the IntermittentDemand module.","category":"section"},{"location":"intermittent/#Theoretical-Framework","page":"Intermittent Demand","title":"Theoretical Framework","text":"The probability of demand is updated every period:\n\nhatp_t = alpha_p d_t + (1-alpha_p)hatp_t-1\n\nwhere d_t = 1 if y_t  0, and d_t = 0 otherwise.\n\nThe demand size is updated only when demand occurs:\n\nhatz_q = alpha_z z_t + (1-alpha_z)hatz_q-1","category":"section"},{"location":"intermittent/#Forecast-2","page":"Intermittent Demand","title":"Forecast","text":"The TSB forecast is:\n\nhaty_t+h = hatp_t cdot hatz_q","category":"section"},{"location":"intermittent/#Shale-Boylan-Johnston-(SBJ)-Method","page":"Intermittent Demand","title":"Shale-Boylan-Johnston (SBJ) Method","text":"The SBJ method provides an alternative bias correction to Croston's method, particularly suited for Poisson demand arrivals. The correction factor is derived from the theoretical properties of the demand process.","category":"section"},{"location":"intermittent/#Optimization-and-Loss-Functions","page":"Intermittent Demand","title":"Optimization and Loss Functions","text":"","category":"section"},{"location":"intermittent/#Traditional-Loss-Functions","page":"Intermittent Demand","title":"Traditional Loss Functions","text":"Classical forecasting metrics often perform poorly for intermittent demand:\n\nMean Squared Error (MSE): displaystyle frac1nsum_t=1^n(y_t-haty_t)^2\nMean Absolute Error (MAE): displaystyle frac1nsum_t=1^ny_t-haty_t\n\nThese metrics compare forecasts against predominantly zero actual values, leading to downward-biased parameter estimates.","category":"section"},{"location":"intermittent/#Rate-Based-Loss-Functions","page":"Intermittent Demand","title":"Rate-Based Loss Functions","text":"Since Croston-type methods produce rate forecasts (expected demand per period), Kourentzes (2014) demonstrated that rate-based loss functions yield superior results:\n\nRate residual at time t:\n\nr_t = haty_t - frac1tsum_j=1^t y_j\n\nMean Absolute Rate error (MAR):\n\ntextMAR = frac1nsum_t=1^n r_t\n\nMean Squared Rate error (MSR):\n\ntextMSR = frac1nsum_t=1^n r_t^2","category":"section"},{"location":"intermittent/#Empirical-Findings","page":"Intermittent Demand","title":"Empirical Findings","text":"Kourentzes (2014) established through extensive simulation that:\n\nMAR and MSR perform equivalently and both substantially outperform MSE/MAE\nSeparate smoothing parameters (alpha_z neq alpha_x) improve performance for Croston variants\nInitial state optimization enhances accuracy, particularly for short series\nParameter bounds should allow values up to 1.0; restrictive upper bounds (e.g., 0.3) can degrade performance\nSmall smoothing parameters (typically 0.05-0.2) emerge naturally with proper optimization","category":"section"},{"location":"intermittent/#Model-Selection","page":"Intermittent Demand","title":"Model Selection","text":"Kourentzes (2014) found that:\n\nSimple Croston variants often perform competitively with complex model selection schemes\nBias-corrected methods (SBA, SBJ) generally outperform the classical Croston method\nFocus on proper optimization (using rate-based losses with separate parameters) matters more than complex model selection","category":"section"},{"location":"intermittent/#Implementation-Details","page":"Intermittent Demand","title":"Implementation Details","text":"The IntermittentDemand module provides three main functions that implement the Kourentzes (2014) recommendations:","category":"section"},{"location":"intermittent/#Available-Methods","page":"Intermittent Demand","title":"Available Methods","text":"croston_classic(): Classical Croston method\ncroston_sba(): Syntetos-Boylan Approximation\ncroston_sbj(): Shale-Boylan-Johnston bias correction","category":"section"},{"location":"intermittent/#Key-Parameters","page":"Intermittent Demand","title":"Key Parameters","text":"All methods support the following parameters aligned with Kourentzes (2014) findings:\n\ncost_metric: Loss function for optimization\n\"mar\" (recommended): Mean Absolute Rate error\n\"msr\" (recommended): Mean Squared Rate error\n\"mae\": Mean Absolute Error (classical)\n\"mse\": Mean Squared Error (classical)\nnumber_of_params: Number of smoothing parameters\n1: Single parameter for both size and interval\n2 (recommended): Separate parameters (alpha_z neq alpha_x)\noptimize_init: Whether to optimize initial states\ntrue (recommended): Optimize initial values\nfalse: Use heuristic initialization\ninit_strategy: Initialization method\n\"mean\" (default): Use mean of non-zero values and intervals\n\"naive\": Use first observed values","category":"section"},{"location":"intermittent/#Implementation-Notes","page":"Intermittent Demand","title":"Implementation Notes","text":"Rate-based optimization: MAR and MSR losses are implemented as recommended\nSeparate smoothing parameters: Supported via number_of_params = 2\nParameter bounds: Allow values up to 1.0 (no restrictive caps)\nBias corrections: SBA and SBJ corrections are built into the respective methods","category":"section"},{"location":"intermittent/#Forecasting-in-Julia","page":"Intermittent Demand","title":"Forecasting in Julia","text":"using Durbyn\nusing Durbyn.IntermittentDemand\n\n# Intermittent demand data\ndata = [6, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0,\n        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n        0, 0, 0, 0, 0]\n\n# Classical Croston method (using recommended MAR cost metric)\nfit_croston = croston_classic(data, cost_metric = \"mar\")\nfc_croston = forecast(fit_croston, h = 12)\n\n# Syntetos-Boylan Approximation with separate smoothing parameters\nfit_sba = croston_sba(data, cost_metric = \"mar\", number_of_params = 2)\nfc_sba = forecast(fit_sba, h = 12)\n\n# Shale-Boylan-Johnston method with initial state optimization\nfit_sbj = croston_sbj(data, cost_metric = \"mar\", optimize_init = true)\nfc_sbj = forecast(fit_sbj, h = 12)\n\n# Alternative cost metrics (classical - use with caution)\nfit_mse = croston_classic(data, cost_metric = \"mse\")  # Traditional MSE\nfit_mae = croston_classic(data, cost_metric = \"mae\")  # Traditional MAE\nfit_msr = croston_classic(data, cost_metric = \"msr\")  # Mean Squared Rate\n\n# Visualization\nplot(fc_croston, show_fitted = true)\nplot(fc_sba, show_fitted = true)\nplot(fc_sbj, show_fitted = true)\n\n# Model diagnostics\nresiduals(fit_croston)\nfitted(fit_croston)\n\n# Model comparison\nprintln(\"Croston weights: \", fit_croston.weights)\nprintln(\"SBA weights: \", fit_sba.weights)\nprintln(\"SBJ weights: \", fit_sbj.weights)","category":"section"},{"location":"intermittent/#Reference","page":"Intermittent Demand","title":"Reference","text":"Kourentzes, N. (2014). On Intermittent Demand Model Optimisation and Selection. International Journal of Production Economics, 156: 180-190.","category":"section"},{"location":"quickstart/#Quick-Start","page":"Quick Start","title":"Quick Start","text":"","category":"section"},{"location":"quickstart/#Installation","page":"Quick Start","title":"Installation","text":"Install the development version:\n\nusing Pkg\nPkg.add(url=\"https://github.com/taf-society/Durbyn.jl\")","category":"section"},{"location":"quickstart/#Formula-Interface-(Recommended)","page":"Quick Start","title":"Formula Interface (Recommended)","text":"Durbyn provides a modern, declarative interface for model specification with full support for tables, regressors (features in ML terminology), model comparison, and panel data.","category":"section"},{"location":"quickstart/#Example-1:-Single-Time-Series","page":"Quick Start","title":"Example 1: Single Time Series","text":"using Durbyn\n\n# Load data\ndata = (sales = [120, 135, 148, 152, 141, 158, 170, 165, 180, 195],)\n\n# ARIMA with automatic order selection\nspec = ArimaSpec(@formula(sales = p() + q() + P() + Q()))\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)\nplot(fc)\n\n\n# Load data another data\ndata = (passengers = air_passengers(),)\n\n# ARIMA with automatic order selection\nspec = ArimaSpec(@formula(passengers = p() + q() + P() + Q()))\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)\nplot(fc)\n","category":"section"},{"location":"quickstart/#Example-2:-With-Regressors-(Features)","page":"Quick Start","title":"Example 2: With Regressors (Features)","text":"using Durbyn\nusing Random\nRandom.seed!(123)\n\n# Simulate data \nn = 120\nidx = 1:n\ntemperature = -(15 .+ 8 .* sin.(2π .* idx ./ 12) .+ 0.5 .* randn(n))\nmarketing = -(0.3 .+ 0.15 .* sin.(2π .* idx ./ 6) .+ 0.05 .* randn(n))\nsales = 120 .+ 1.5 .* temperature .+ 30 .* marketing .+ randn(n)\ndata = (sales = sales, temperature = temperature, marketing = marketing)\n\nspec = ArimaSpec(@formula(sales = temperature + marketing + p() + d() + q() + P() + D() + Q()))\n# spec = ArimaSpec(@formula(sales = auto()))\nfitted = fit(spec, data, m = 12)\n\n# Simulate future data\nn_ahead = 24\nfuture_idx = (n + 1):(n + n_ahead)\nfuture_temp = -(15 .+ 8 .* sin.(2π .* future_idx ./ 12))\nfuture_marketing = -(0.3 .+ 0.15 .* sin.(2π .* future_idx ./ 6))\nnewdata = (temperature = future_temp, marketing = future_marketing)\n\n\nfc = forecast(fitted, h = n_ahead, newdata = newdata)\n\nplot(fc)\n","category":"section"},{"location":"quickstart/#Example-3:-Fitting-Multiple-Models-Together","page":"Quick Start","title":"Example 3: Fitting Multiple Models Together","text":"# Fit multiple model specifications at once\nmodels = model(\n    ArimaSpec(@formula(sales = p() + q())),\n    EtsSpec(@formula(sales = e(\"A\") + t(\"A\") + s(\"A\"))),\n    ArimaSpec(@formula(sales = p(2) + d(1) + q(2))),\n    names = [\"auto_arima\", \"ets_aaa\", \"arima_212\"]\n)\n\n# Fit all models\nfitted = fit(models, data, m = 12)\n\n# Forecast with all models\nfc = forecast(fitted, h = 12)\n\n# Compare results\nfor (name, model_result) in zip(models.names, fitted.models)\n    println(\"$name: AIC = $(round(model_result.fit.aic, digits=2)), BIC = $(round(model_result.fit.bic, digits=2))\")\nend","category":"section"},{"location":"quickstart/#Example-4:-Panel-Data-(Multiple-Series)","page":"Quick Start","title":"Example 4: Panel Data (Multiple Series)","text":"note: Optional Dependencies\nThis example requires CSV and Downloads:using Pkg\nPkg.add([\"CSV\", \"Downloads\"])\n\nusing Durbyn, Durbyn.TableOps\nusing CSV, Downloads, Tables\n\n# Load and reshape data\npath = Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/refs/heads/main/Data/retail.csv\")\nwide = Tables.columntable(CSV.File(path))\n\nlong = pivot_longer(wide; id_cols = :date, names_to = :series, values_to = :value)\npanel = PanelData(long; groupby = :series, date = :date, m = 12)\nglimpse(panel)\n# Fit model to all series at once\nspec = ArimaSpec(@formula(value = p() + q()))\nfitted = fit(spec, panel)\n\n# Forecast all series\nfc = forecast(fitted, h = 12)\n\n# Get tidy forecast table\ntbl = as_table(fc)\nglimpse(tbl)","category":"section"},{"location":"quickstart/#Example-5:-ETS-Models-with-Formula","page":"Quick Start","title":"Example 5: ETS Models with Formula","text":"\nusing Durbyn.Grammar\n\n# Automatic ETS model selection\nspec_ets = EtsSpec(@formula(sales = e(\"Z\") + t(\"Z\") + s(\"Z\")))\nfitted = fit(spec_ets, data, m = 12)\nfc = forecast(fitted, h = 12)\nplot(fc)\n# Specialized ETS specifications\nspec_ses = SesSpec(@formula(sales = ses()))\nspec_holt = HoltSpec(@formula(sales = holt(damped=true)))\nspec_hw = HoltWintersSpec(@formula(sales = hw(seasonal=:multiplicative)))\n\n# Fit and forecast\nfitted_ses = fit(spec_ses, data)\nfc_ses = forecast(fitted_ses, h = 12)\n\nplot(fc_ses)\n\n\n","category":"section"},{"location":"quickstart/#Base-Models-(Array-Interface)","page":"Quick Start","title":"Base Models (Array Interface)","text":"The array interface provides direct access to forecasting engines for working with numeric vectors.","category":"section"},{"location":"quickstart/#Exponential-Smoothing-(ETS)","page":"Quick Start","title":"Exponential Smoothing (ETS)","text":"using Durbyn\nusing Durbyn.ExponentialSmoothing\n\nap = air_passengers()\n\n# Automatic ETS model selection\nfit_ets = ets(ap, 12, \"ZZZ\")\nfc_ets  = forecast(fit_ets, h = 12)\nplot(fc_ets)\n\n# Simple exponential smoothing\nses_fit = ses(ap)\nses_fc  = forecast(ses_fit, h = 12)\nplot(ses_fc)\n\n# Holt's linear trend method\nholt_fit = holt(ap)\nholt_fc  = forecast(holt_fit, h = 12)\nplot(holt_fc)\n\n# Holt-Winters seasonal method\nhw_fit = holt_winters(ap, 12)\nhw_fc  = forecast(hw_fit, h = 12)\nplot(hw_fc)","category":"section"},{"location":"quickstart/#ARIMA","page":"Quick Start","title":"ARIMA","text":"using Durbyn.Arima\n\nap = air_passengers()\n\n# Manual ARIMA specification\narima_model = arima(ap, 12, order = PDQ(2,1,1), seasonal = PDQ(0,1,0))\nfc  = forecast(arima_model, h = 12)\nplot(fc)\n\n# Automatic ARIMA selection\nauto_arima_model = auto_arima(ap, 12)\nfc_auto  = forecast(auto_arima_model, h = 12)\nplot(fc_auto)\n\n","category":"section"},{"location":"quickstart/#Performance:-Multi-Threading-for-Parallel-Computing","page":"Quick Start","title":"Performance: Multi-Threading for Parallel Computing","text":"Durbyn's fit function automatically leverages Julia's multi-threading for massive parallel computing when fitting models to panel data (multiple time series) or comparing multiple model specifications. Performance scales nearly linearly with CPU cores—from laptops to large cloud instances with 96+ cores.","category":"section"},{"location":"quickstart/#When-Does-Multi-Threading-Help?","page":"Quick Start","title":"When Does Multi-Threading Help?","text":"Multi-threading provides dramatic performance improvements when:\n\nFitting models to panel data with many series (e.g., 40+ series)\nComparing multiple models across series simultaneously\nRunning ensemble methods or model selection procedures\n\nExample: Without multi-threading (1 thread)\n\nFitting 6 models to 42 series = 252 individual fits\nTime: ~5-10 minutes (sequential processing)\n\nWith multi-threading\n\nSame 252 fits processed in parallel\n8 cores (laptop): ~60-90 seconds (5-8x faster)\n16 cores (workstation): ~30-45 seconds (10-15x faster)\n32+ cores (cloud): ~15-30 seconds (15-20x faster)","category":"section"},{"location":"quickstart/#How-to-Enable-Multi-Threading","page":"Quick Start","title":"How to Enable Multi-Threading","text":"Julia must be started with multiple threads to enable parallel processing. Here are all the methods:","category":"section"},{"location":"quickstart/#Method-1:-VS-Code-Julia-Extension-Settings-(Recommended-for-VS-Code-Users)","page":"Quick Start","title":"Method 1: VS Code Julia Extension Settings (Recommended for VS Code Users)","text":"Add to your VS Code settings.json (File → Preferences → Settings → Open Settings JSON):\n\n{\n    \"julia.additionalArgs\": [\n        \"-t\",\n        \"auto\"\n    ]\n}\n\nOptions:\n\n\"auto\" — Use all available CPU cores (recommended)\nSpecific number (e.g., \"8\", \"12\") — Limit threads if you want to reserve cores for other tasks\n\nTo apply:\n\nSave settings.json\nRestart Julia REPL in VS Code (click trash icon in Julia REPL, then restart)\nVerify with Threads.nthreads()","category":"section"},{"location":"quickstart/#Method-2:-Command-Line","page":"Quick Start","title":"Method 2: Command Line","text":"# Use all available cores (recommended)\njulia -t auto\n\n# Use specific number of threads (e.g., 8 threads)\njulia -t 8\n\n# Alternative syntax\njulia --threads=auto","category":"section"},{"location":"quickstart/#Method-3:-Environment-Variable-(Persistent)","page":"Quick Start","title":"Method 3: Environment Variable (Persistent)","text":"Set once and applies to all Julia sessions.\n\nLinux/macOS — Add to ~/.bashrc, ~/.zshrc, or ~/.profile:\n\nexport JULIA_NUM_THREADS=auto\n\nWindows (PowerShell) — Add to PowerShell profile:\n\n$env:JULIA_NUM_THREADS = \"auto\"\n\nWindows (System Environment Variables):\n\nSearch \"Environment Variables\" in Windows\nAdd new system variable: JULIA_NUM_THREADS = auto\n\nAfter setting, restart terminal/IDE for changes to take effect.","category":"section"},{"location":"quickstart/#Method-4:-Julia-Startup-File","page":"Quick Start","title":"Method 4: Julia Startup File","text":"Create/edit ~/.julia/config/startup.jl (Linux/macOS) or %USERPROFILE%\\.julia\\config\\startup.jl (Windows):\n\n# Set before Julia starts - less reliable, use environment variable instead\nENV[\"JULIA_NUM_THREADS\"] = \"auto\"\n\nNote: This method is less reliable. Prefer environment variable or command-line methods.","category":"section"},{"location":"quickstart/#Verifying-Multi-Threading-Is-Active","page":"Quick Start","title":"Verifying Multi-Threading Is Active","text":"julia> Threads.nthreads()\n8  # Number of threads available (depends on your CPU and settings)\n\njulia> Threads.threadpoolsize()\n8  # Confirms thread pool size","category":"section"},{"location":"quickstart/#Real-World-Example:-Panel-Data-Model-Comparison","page":"Quick Start","title":"Real-World Example: Panel Data Model Comparison","text":"Here's how multi-threading accelerates fitting multiple models to panel data:\n\nusing Durbyn, Durbyn.TableOps, Durbyn.Grammar\nusing CSV, Downloads, Tables\n\n# Load panel data (42 retail series)\npath = Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/refs/heads/main/Data/retail.csv\")\nwide = Tables.columntable(CSV.File(path))\nlong = pivot_longer(wide; id_cols=:date, names_to=:series, values_to=:value)\npanel = PanelData(long; groupby=:series, date=:date, m=12)\n\n# Define multiple models for comparison\nmodels = model(\n    ArarSpec(@formula(value = arar())),                                # ARAR\n    ArimaSpec(@formula(value = p() + q())),                              # Auto ARIMA\n    EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\") + drift(:auto))),  # Auto ETS with drift\n    SesSpec(@formula(value = ses())),                                    # Simple exponential smoothing\n    HoltSpec(@formula(value = holt(damped=true))),                       # Damped Holt\n    HoltWintersSpec(@formula(value = hw(seasonal=:multiplicative))),     # Holt-Winters multiplicative\n    CrostonSpec(@formula(value = croston())),                            # Croston's method\n    names=[\"arar\", \"arima\", \"ets_auto\", \"ses\", \"holt_damped\", \"hw_mul\", \"croston\"]\n)\n\n# Fit all models to all series IN PARALLEL\n# Automatically uses available threads - no code changes needed!\nfitted = fit(models, panel)\n# Performance scales with cores:\n#   1 thread:    ~400-500 seconds (baseline)\n#   8 threads:   ~60-90 seconds (laptop/desktop)\n#   16 threads:  ~30-45 seconds (workstation)\n#   32+ threads: ~15-30 seconds (cloud/HPC)\n\n# Generate forecasts (also parallelized)\nfc = forecast(fitted, h=12)\n\n# Convert to tidy table format\nfc_tbl = as_table(fc)\nglimpse(fc_tbl)\n\nWhat's happening under the hood:\n\n42 series × 7 models = 294 model fits\nWith multiple threads: Fits are distributed across available cores\nEach thread handles a series/model combination independently\nNo code changes needed — parallelization is automatic!","category":"section"},{"location":"quickstart/#Troubleshooting","page":"Quick Start","title":"Troubleshooting","text":"Problem: Threads.nthreads() returns 1\n\nSolution: Julia was started without -t flag. Restart Julia with multi-threading enabled.\n\nProblem: VS Code settings not working\n\nSolution: Fully restart VS Code (not just Julia REPL). Settings only apply on fresh start.\n\nProblem: Performance not improving\n\nSolution: Check you have enough series/models. Small datasets (< 10 series) may not show speedup due to threading overhead.","category":"section"},{"location":"quickstart/#Recommended-Settings","page":"Quick Start","title":"Recommended Settings","text":"Development/Interactive: julia -t auto or VS Code settings with \"auto\"\nProduction/Scripts: export JULIA_NUM_THREADS=auto in environment\nCloud/HPC Systems: julia -t auto to leverage all available cores (e.g., 32, 64, 128+ threads)\nShared Systems: Use specific number (e.g., -t 8) to avoid consuming all resources and leave cores for other users\n\n","category":"section"},{"location":"quickstart/#Next-Steps","page":"Quick Start","title":"Next Steps","text":"tip: Complete End-to-End Example\nWant to see a comprehensive workflow with train/test split, model comparison, accuracy evaluation, and visualization? Check out the Complete End-to-End Example in Grammar Guide — demonstrates fitting 6 models to panel data with full evaluation pipeline.\n\nDocumentation:\n\nGrammar Guide — Complete formula interface documentation for ARIMA and ETS\nTable Operations — Data wrangling for time series and panel data\nARIMA — Formula interface and base models (ARIMA, SARIMA, Auto ARIMA)\nExponential Smoothing — Formula interface and base models (SES, Holt, Holt-Winters, ETS)\nIntermittent Demand — Croston methods for sparse/intermittent data\nARAR/ARARMA — Memory-shortening algorithms","category":"section"},{"location":"arima/#Forecasting-Using-ARIMA,-SARIMA,-ARIMAX,-SARIMAX,-and-Auto-ARIMA","page":"ARIMA","title":"Forecasting Using ARIMA, SARIMA, ARIMAX, SARIMAX, and Auto ARIMA","text":"tip: Formula Interface is the Recommended Approach\nThis page starts with the formula interface (recommended for most users), which provides declarative model specification with support for regressors, panel data, and model comparison. The array interface (base models) is covered later. See the Grammar Guide for complete documentation.","category":"section"},{"location":"arima/#1.-ARIMA-(AutoRegressive-Integrated-Moving-Average)","page":"ARIMA","title":"1. ARIMA (AutoRegressive Integrated Moving Average)","text":"","category":"section"},{"location":"arima/#Definition","page":"ARIMA","title":"Definition","text":"An ARIMA model is denoted as ARIMA(p, d, q), where:\n\np: order of the autoregressive (AR) part\nd: degree of differencing needed to achieve stationarity\nq: order of the moving average (MA) part\n\nFormally, the model is written as:\n\nPhi(B) Delta^d X_t = Theta(B) varepsilon_t\n\nwhere:\n\nB is the backshift operator (BX_t = X_t-1),\nPhi(B) = 1 - phi_1B - cdots - phi_pB^p,\nTheta(B) = 1 + theta_1B + cdots + theta_qB^q,\nDelta^d = (1 - B)^d is the differencing operator,\nvarepsilon_t is white noise.\n\nIf d = 0, the model reduces to ARMA(p, q).","category":"section"},{"location":"arima/#Key-Features","page":"ARIMA","title":"Key Features","text":"Handles non-stationary time series via differencing.\nShocks (innovations) have permanent effects for d  0.\nCommonly used for macroeconomic and financial data.\n\n","category":"section"},{"location":"arima/#2.-SARIMA-(Seasonal-ARIMA)","page":"ARIMA","title":"2. SARIMA (Seasonal ARIMA)","text":"","category":"section"},{"location":"arima/#Definition-2","page":"ARIMA","title":"Definition","text":"Seasonal ARIMA extends ARIMA to account for seasonality. It is denoted as:\n\nARIMA(p d q)(P D Q)_m\n\nwhere:\n\nP D Q are the seasonal AR, differencing, and MA orders,\nm is the seasonal period (e.g., 12 for monthly data with yearly seasonality).","category":"section"},{"location":"arima/#Model-Form","page":"ARIMA","title":"Model Form","text":"Phi(B)Phi_s(B^m) Delta^d Delta_m^D X_t = Theta(B)Theta_s(B^m)varepsilon_t\n\nwhere:\n\nPhi_s(B^m) and Theta_s(B^m) capture seasonal AR and MA terms,\nDelta_m^D = (1 - B^m)^D applies seasonal differencing.","category":"section"},{"location":"arima/#Key-Features-2","page":"ARIMA","title":"Key Features","text":"Captures both short-term dynamics (p, d, q) and seasonal effects (P, D, Q).\nWidely applied to monthly or quarterly economic indicators, sales, or climate data.\n\n","category":"section"},{"location":"arima/#3.-ARIMAX-(ARIMA-with-Exogenous-Variables)","page":"ARIMA","title":"3. ARIMAX (ARIMA with Exogenous Variables)","text":"","category":"section"},{"location":"arima/#Definition-3","page":"ARIMA","title":"Definition","text":"An ARIMAX model incorporates external regressors (covariates) into the ARIMA framework:\n\nPhi(B) Delta^d X_t = beta Z_t + Theta(B) varepsilon_t\n\nwhere:\n\nZ_t is a vector of exogenous predictors,\nbeta are their coefficients.","category":"section"},{"location":"arima/#Key-Features-3","page":"ARIMA","title":"Key Features","text":"Useful when external factors (e.g., interest rates, marketing spend, policy variables) explain additional variance beyond past values of the series.\nRequires careful checking of exogeneity assumptions.\n\n","category":"section"},{"location":"arima/#4.-SARIMAX-(Seasonal-ARIMAX)","page":"ARIMA","title":"4. SARIMAX (Seasonal ARIMAX)","text":"","category":"section"},{"location":"arima/#Definition-4","page":"ARIMA","title":"Definition","text":"SARIMAX generalizes SARIMA by including exogenous regressors:\n\nPhi(B)Phi_s(B^m) Delta^d Delta_m^D X_t = beta Z_t + Theta(B)Theta_s(B^m)varepsilon_t","category":"section"},{"location":"arima/#Key-Features-4","page":"ARIMA","title":"Key Features","text":"Combines seasonality and exogenous influences.\nPowerful for real-world applications such as:\nForecasting retail sales with promotions (exogenous variable) and seasonal cycles.\nModeling energy demand with weather as an exogenous driver.\n\n","category":"section"},{"location":"arima/#5.-Auto-ARIMA","page":"ARIMA","title":"5. Auto ARIMA","text":"","category":"section"},{"location":"arima/#Definition-5","page":"ARIMA","title":"Definition","text":"Auto ARIMA automates the process of identifying the best ARIMA/SARIMA model by searching across possible values of (p, d, q) and seasonal (P, D, Q), selecting the model that minimizes an information criterion such as AIC, AICc, or BIC.","category":"section"},{"location":"arima/#Algorithm-(Hyndman-and-Khandakar,-2008)","page":"ARIMA","title":"Algorithm (Hyndman & Khandakar, 2008)","text":"Unit root tests (ADF, KPSS, or combinations) to determine differencing orders ( d ) and ( D ).\nInitial model selection based on heuristics.  \nStepwise search over (p, q, P, Q) with bounds (e.g., up to 5 for non-seasonal and 2 for seasonal).  \nEvaluate models by likelihood and information criteria.  \nRefit the best model with full maximum likelihood.  ","category":"section"},{"location":"arima/#Advantages","page":"ARIMA","title":"Advantages","text":"Removes the manual effort of model identification.  \nScales well to large numbers of series.  \nEnsures differencing is tested systematically (avoids over-differencing).","category":"section"},{"location":"arima/#Limitations","page":"ARIMA","title":"Limitations","text":"Stepwise search may not find the global optimum.  \nComputationally expensive for very large seasonal periods.  \nStill requires diagnostic checking of residuals.  \n\n","category":"section"},{"location":"arima/#6.-Model-Selection-and-Diagnostics","page":"ARIMA","title":"6. Model Selection & Diagnostics","text":"","category":"section"},{"location":"arima/#Identification","page":"ARIMA","title":"Identification","text":"Use ACF/PACF plots and unit root tests (ADF, PP, KPSS) to choose orders manually (or confirm Auto ARIMA results).\nDifferencing ensures stationarity (d D).","category":"section"},{"location":"arima/#Estimation","page":"ARIMA","title":"Estimation","text":"Maximum Likelihood Estimation (MLE) or Conditional Sum of Squares.","category":"section"},{"location":"arima/#Diagnostics","page":"ARIMA","title":"Diagnostics","text":"Residual analysis: check for white noise.\nInformation criteria: AIC, BIC, AICc.  \nOut-of-sample forecast validation.\n\n","category":"section"},{"location":"arima/#Formula-Interface-(Primary-Usage)","page":"ARIMA","title":"Formula Interface (Primary Usage)","text":"The formula interface provides a modern, declarative way to specify ARIMA models with full support for single series, regressors, model comparison, and panel data.","category":"section"},{"location":"arima/#Example-1:-Single-ARIMA-Model","page":"ARIMA","title":"Example 1: Single ARIMA Model","text":"using Durbyn\n\n# Load data\ndata = (sales = [120, 135, 148, 152, 141, 158, 170, 165, 180, 195],)\n\n# Specify model with automatic order selection\nspec = ArimaSpec(@formula(sales = p() + q() + P() + Q() + d() + D()))\nfitted_model = fit(spec, data, m = 12)\nfc = forecast(fitted_model, h = 12)\n\n# Check model summary\nprintln(fitted_model)\n\n# Access fitted values and residuals\nfitted_values = fitted(fitted_model)\nresids = residuals(fitted_model)\n\nKey features:\n\np(), q(), P(), Q(), d() and D() with no arguments triggers automatic order selection\nm = 12 specifies monthly seasonality\nFormula syntax clearly shows response variable (sales)","category":"section"},{"location":"arima/#Example-2:-ARIMA-with-Regressors","page":"ARIMA","title":"Example 2: ARIMA with Regressors","text":"When you have external variables that influence the response, include them as regressors:\n\n# Model with exogenous regressors\ndata = (\n    sales = rand(100),\n    temperature = rand(100),\n    promotion = rand(0:1, 100)\n)\n\n# Specify model with regressors\nspec = ArimaSpec(@formula(sales = p(1,3) + q(1,3) + temperature + promotion))\nfitted = fit(spec, data, m = 7)\n\n# Forecast requires future regressor values\nnewdata = (temperature = rand(7), promotion = rand(0:1, 7))\nfc = forecast(fitted, h = 7, newdata = newdata)\n\nTerminology:\n\nResponse variable: The variable being forecasted (sales)\nRegressors: External predictors (temperature, promotion)\n\nKey features:\n\np(1,3) starts searching for best AR order between 1 and 3\nRegressors are simply added to the formula\nFuture regressor values must be provided via newdata","category":"section"},{"location":"arima/#Example-3:-Manual-ARIMA-Specification","page":"ARIMA","title":"Example 3: Manual ARIMA Specification","text":"For full control over model orders:\n\n# Specify exact orders for SARIMA model\nspec = ArimaSpec(@formula(sales = p(2) + d(1) + q(1) + P(1) + D(1) + Q(1)))\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)\n\n# Or use specific values with regressors\nspec = ArimaSpec(@formula(sales = p(1) + d(1) + q(1) + temperature + promotion))\nfitted = fit(spec, data, m = 12)\n\nARIMA order specification:\n\np(k): AR order = k\nd(k): Differencing order = k\nq(k): MA order = k\nP(k): Seasonal AR order = k\nD(k): Seasonal differencing = k\nQ(k): Seasonal MA order = k","category":"section"},{"location":"arima/#Example-4:-Fitting-Multiple-Models-Together","page":"ARIMA","title":"Example 4: Fitting Multiple Models Together","text":"Fit different model specifications and manually compare results:\n\n# Define multiple candidate models\nmodels = model(\n    ArimaSpec(@formula(sales = p() + q())),                    # Auto ARIMA\n    ArimaSpec(@formula(sales = p(2) + d(1) + q(2))),          # ARIMA(2,1,2)\n    ArimaSpec(@formula(sales = p(1) + d(1) + q(1) + P(1) + D(1) + Q(1))),  # SARIMA\n    names = [\"auto_arima\", \"arima_212\", \"sarima_111_111\"]\n)\n\n# Fit all models\nfitted = fit(models, data, m = 12)\n\n# Forecast with all models\nfc = forecast(fitted, h = 12)\n\n# Check forecast accuracy\naccuracy(fc, test)\n\nKey features:\n\nFit multiple specifications at once\nMix different model types (ARIMA, ETS, etc.)\nCheck model accuracy\nForecasts generated for all models","category":"section"},{"location":"arima/#Example-5:-Panel-Data-(Multiple-Time-Series)","page":"ARIMA","title":"Example 5: Panel Data (Multiple Time Series)","text":"Fit the same model specification to many series efficiently:\n\nusing Durbyn.TableOps\nusing CSV, Downloads\n\n# Load panel data\npath = Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/refs/heads/main/Data/retail.csv\")\nwide = Tables.columntable(CSV.File(path))\n\n# Reshape to long format\nlong = pivot_longer(wide; id_cols = :date, names_to = :series, values_to = :value)\n\n# Create panel data wrapper\npanel = PanelData(long; groupby = :series, date = :date, m = 12)\n\n# Fit model to all series at once\nspec = ArimaSpec(@formula(value = p() + q()))\nfitted = fit(spec, panel)\n\n# Forecast all series\nfc = forecast(fitted, h = 12)\n\n# Get tidy forecast table\ntbl = as_table(fc)\n\n# Optional: Save forecasts to CSV\n# CSV.write(\"forecasts.csv\", tbl)\n\n# Calculate accuracy metrics\n# Method 1: Using ForecastModelCollection directly\nacc_results = accuracy(fc, test)\n\nprintln(\"\\nAccuracy by Series and Model:\")\nglimpse(acc_results)\n\nlist_series(fc)  # See what's available\nplot(fc)  # Quick look at first series\nplot(fc, series=:all, facet=true, n_cols=4)  # Overview\n\n# Detailed inspection\nplot(fc, series=\"series_1\", actual=test)\n\n# Calculate accuracy\nacc = accuracy(fc, test)\n\n# Find and plot interesting cases\nbest = acc.series[argmin(acc.MAPE)]\nworst = acc.series[argmax(acc.MAPE)]\n\nplot(fc, series=[best, worst], facet=true, actual=test)\n\n\nPanel data features:\n\nFits model separately to each group\nReturns structured output for all series\nas_table creates tidy format for analysis\nEfficient for hundreds or thousands of series","category":"section"},{"location":"arima/#Example-6:-Panel-Data-with-Grouping-Variables","page":"ARIMA","title":"Example 6: Panel Data with Grouping Variables","text":"For complex panel structures:\n\n\n# Use PanelData interface\npanel = PanelData(train; groupby=[:product, :location, :product_line], date=:date, m=7);\n\nspec = ArimaSpec(@formula(sales = p() + q()))\nfitted = fit(spec, panel)\nfc = forecast(fitted, h = 14)\n\n# Data with multiple grouping variables\nspec = ArimaSpec(@formula(sales = p() + q()))\nfitted = fit(spec, data,\n             groupby = [:product, :location, :product_line],\n             m = 7)\nfc = forecast(fitted, h = 7)\n\n# Filter forecasts for specific groups\ntbl = as_table(fc)\n\n\n","category":"section"},{"location":"arima/#Array-Interface-(Base-Models)","page":"ARIMA","title":"Array Interface (Base Models)","text":"The array interface provides direct access to ARIMA estimation for numeric vectors. This is useful for quick analyses or integration with existing code for example using Durbyn base models as backend for Python or R packages.","category":"section"},{"location":"arima/#Forecasting-Using-Seasonal-ARIMA-Model","page":"ARIMA","title":"Forecasting Using Seasonal ARIMA Model","text":"using Durbyn\nusing Durbyn.Arima\n\nap  = air_passengers()\narima_model = arima(ap, 12, order = PDQ(2,1,1), seasonal = PDQ(0,1,0))\nfc  = forecast(arima_model, h = 12)\nplot(fc)\n","category":"section"},{"location":"arima/#Forecasting-Using-Auto-ARIMA-Model","page":"ARIMA","title":"Forecasting Using Auto-ARIMA Model","text":"auto_arima_model = auto_arima(ap, 12)\nfc2  = forecast(auto_arima_model, h = 12)\nplot(fc2)","category":"section"},{"location":"arima/#References","page":"ARIMA","title":"References","text":"Kunst, R. (2011). Applied Time Series Analysis — Part II. University of Vienna.  \nHyndman, R.J., & Khandakar, Y. (2008). Automatic Time Series Forecasting: The forecast Package for R. Journal of Statistical Software, 27(3).  \nBox, G.E.P., Jenkins, G.M., & Reinsel, G.C. (1994). Time Series Analysis, Forecasting and Control.  \nHamilton, J.D. (1994). Time Series Analysis.  ","category":"section"},{"location":"#Durbyn.jl","page":"Home","title":"Durbyn.jl","text":"(Image: Durbyn.jl logo)\n\n(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)\n\nDurbyn is a Julia package for time-series forecasting, inspired by the R forecast and fable packages. While drawing on their methodology, Durbyn is a native Julia implementation featuring its own unique formula-based grammar for declarative model specification.\n\nDurbyn — Kurdish for “binoculars” (Dur, far + Byn, to see), embodies foresight through science. Like Hari Seldon’s psychohistory in Asimov’s Foundation, we seek to glimpse the shape of tomorrow through the disciplined clarity of mathematics.\n\nThis site documents the development version. After your first tagged release, see stable docs for the latest release.\n\n","category":"section"},{"location":"#About-TAFS","page":"Home","title":"About TAFS","text":"TAFS (Time Series Analysis and Forecasting Society) is a non-profit association (“Verein”) in Vienna, Austria. It connects academics, experts, practitioners, and students focused on time-series, forecasting, and decision science. Contributions remain fully open source.   Learn more at taf-society.org.\n\n","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"Durbyn is under active development. For the latest dev version:\n\nusing Pkg\nPkg.add(url=\"https://github.com/taf-society/Durbyn.jl\")\n\ntip: Performance: Multi-Threading\nDurbyn automatically uses parallel computing when fitting models to panel data. Start Julia with multiple threads for massive speedups that scale with your CPU cores:julia -t autoSee Performance Guide for all setup methods including VS Code configuration.\n\n","category":"section"},{"location":"#Formula-Interface-(Recommended)","page":"Home","title":"Formula Interface (Recommended)","text":"Durbyn provides a modern, declarative interface for model specification using @formula. This is the recommended approach for most users, supporting single series, model comparison, and panel data forecasting.\n\nThe PanelData interface follows the tidy forecasting workflow (Hyndman & Athanasopoulos, 2021), providing a structured approach:\n\nData Preparation — Load, reshape, and clean data using TableOps\nVisualization — Explore patterns with plot() and glimpse()\nModel Specification — Define models using the formula interface (@formula)\nModel Training — Fit models with fit(), producing fitted model objects\nPerformance Evaluation — Assess accuracy with accuracy() and diagnostics\nForecasting — Generate predictions with forecast(), returning tidy forecast tables\n\nnote: Optional Dependencies\nPanel data examples require CSV and Downloads packages:using Pkg\nPkg.add([\"CSV\", \"Downloads\"])","category":"section"},{"location":"#Complete-Workflow:-Model-Comparison-with-Panel-Data","page":"Home","title":"Complete Workflow: Model Comparison with Panel Data","text":"using Durbyn, Durbyn.TableOps, Durbyn.Grammar\nusing CSV, Downloads, Tables\n\n# 1. Load and prepare data\npath = Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/refs/heads/main/Data/retail.csv\")\nwide = Tables.columntable(CSV.File(path))\n\n# Reshape to long format\ntbl = pivot_longer(wide; id_cols=:date, names_to=:series, values_to=:value)\nglimpse(tbl)\n\n# 2. Split into train and test sets\nall_dates = unique(tbl.date)\nsplit_date = all_dates[end-11]  # Hold out last 12 periods for testing\n\ntrain = query(tbl, row -> row.date <= split_date)\ntest = query(tbl, row -> row.date > split_date)\n\nprintln(\"Training data:\")\nglimpse(train)\nprintln(\"\\nTest data:\")\nglimpse(test)\n\n# 3. Create panel data wrapper\npanel = PanelData(train; groupby=:series, date=:date, m=12)\nglimpse(panel)\n\n# 4. Define multiple models for comparison\nmodels = model(\n    ArarSpec(@formula(value = arar())),                                  # ARAR\n    BatsSpec(@formula(value = bats(seasonal_periods=12))),               # BATS with seasonality\n    ArimaSpec(@formula(value = p() + q())),                              # Auto ARIMA\n    EtsSpec(@formula(value = e(\"Z\") + t(\"Z\") + s(\"Z\") + drift(:auto))),  # Auto ETS with drift\n    SesSpec(@formula(value = ses())),                                    # Simple exponential smoothing\n    HoltSpec(@formula(value = holt(damped=true))),                       # Damped Holt\n    HoltWintersSpec(@formula(value = hw(seasonal=:multiplicative))),     # Holt-Winters multiplicative\n    CrostonSpec(@formula(value = croston())),                            # Croston's method\n    names=[\"arar\", \"bats\", \"arima\", \"ets_auto\", \"ses\", \"holt_damped\", \"hw_mul\", \"croston\"]\n)\n\n# 5. Fit all models to all series\nfitted = fit(models, panel)\n\n# 6. Generate forecasts (h=12 to match test set)\nfc = forecast(fitted, h=12)\n\n# 7. Convert to tidy table format\nfc_tbl = as_table(fc)\nglimpse(fc_tbl)\n\n# 8. Calculate accuracy metrics across all models and series\nacc_results = accuracy(fc, test)\nprintln(\"\\nAccuracy by Series and Model:\")\nglimpse(acc_results)\n\n# 9. Visualization\nlist_series(fc)  # Show available series\n\n# Quick overview of all series for first model\nplot(fc, series=:all, facet=true, n_cols=4)\n\n# Detailed inspection with actual values from test set\nplot(fc, series=\"series_10\", actual=test)\n\n# 10. Find best and worst performing series\nbest_series = acc_results.series[argmin(acc_results.MAPE)]\nworst_series = acc_results.series[argmax(acc_results.MAPE)]\n\n# Compare best vs worst performers\nplot(fc, series=[best_series, worst_series], facet=true, actual=test)\n\nThis example demonstrates:\n\nData wrangling: Load, reshape, and split data using TableOps\nModel comparison: Fit 8 forecasting methods (ARAR, BATS, ARIMA, ETS variants, Croston)\nPanel forecasting: Automatic iteration over multiple time series\nOut-of-sample evaluation: Train/test split with accuracy metrics\nVisualization: Faceted plots, actual vs forecast comparison\nTidy output: Structured tables ready for further analysis","category":"section"},{"location":"#Quick-Examples","page":"Home","title":"Quick Examples","text":"","category":"section"},{"location":"#Single-Series-ARIMA","page":"Home","title":"Single Series ARIMA","text":"using Durbyn\n\ndata = (sales = [120, 135, 148, 152, 141, 158, 170, 165, 180, 195],)\n\nspec = ArimaSpec(@formula(sales = p() + q()))\nfitted = fit(spec, data, m = 12)\nfc = forecast(fitted, h = 12)","category":"section"},{"location":"#ARIMA-with-Regressors-(Features)","page":"Home","title":"ARIMA with Regressors (Features)","text":"data = (\n    sales = rand(100),\n    temperature = rand(100),\n    promotion = rand(0:1, 100)\n)\n\nspec = ArimaSpec(@formula(sales = p(1,3) + q(1,3) + temperature + promotion))\nfitted = fit(spec, data, m = 7)\n\n# Provide future values of regressors\nnewdata = (temperature = rand(7), promotion = rand(0:1, 7))\nfc = forecast(fitted, h = 7, newdata = newdata)","category":"section"},{"location":"#Automatic-ETS-Selection","page":"Home","title":"Automatic ETS Selection","text":"spec_ets = EtsSpec(@formula(sales = e(\"Z\") + t(\"Z\") + s(\"Z\")))\nfitted = fit(spec_ets, data, m = 12)\nfc = forecast(fitted, h = 12)\n\n","category":"section"},{"location":"#Base-Models-(Array-Interface)","page":"Home","title":"Base Models (Array Interface)","text":"using Durbyn\nusing Durbyn.ExponentialSmoothing\n\nap = air_passengers()\n\nfit_ets = ets(ap, 12, \"ZZZ\")\nfc_ets  = forecast(fit_ets, h = 12)\nplot(fc_ets)\n\nses_fit = ses(ap, 12)\nses_fc  = forecast(ses_fit, h = 12)\nplot(ses_fc)\n\nholt_fit = holt(ap, 12)\nholt_fc  = forecast(holt_fit, h = 12)\nplot(holt_fc)\n\nhw_fit = holt_winters(ap, 12)\nhw_fc  = forecast(hw_fit, h = 12)\nplot(hw_fc)\n\n","category":"section"},{"location":"#Intermittent-demand-(Croston-variants)","page":"Home","title":"Intermittent demand (Croston variants)","text":"data = [6, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0,\n0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, \n0, 0, 0, 0, 0];\n\n# Based on Shenstone & Hyndman (2005)\nm = 1\nfit_crst = croston(data, m)\nfc_crst  = forecast(fit_crst, 12)\nplot(fc_crst)\n\nusing Durbyn.IntermittentDemand\n\n# Classical Croston (Croston, 1972)\ncrst1 = croston_classic(data)\nfc1   = forecast(crst1, h = 12)\n\nresiduals(crst1); residuals(fc1);\nfitted(crst1);    fitted(fc1);\nplot(fc1, show_fitted = true)\n\n# Croston + SBA correction\ncrst2 = croston_sba(data)\nfc2   = forecast(crst2, h = 12)\nplot(fc2, show_fitted = true)\n\n# Croston + SBJ correction\ncrst3 = croston_sbj(data)\nfc3   = forecast(crst3, h = 12)\nplot(fc3, show_fitted = true)\n\n","category":"section"},{"location":"#ARIMA","page":"Home","title":"ARIMA","text":"using Durbyn.Arima\n\nap  = air_passengers()\n\n# manual ARIMA\narima_model = arima(ap, 12, order = PDQ(2,1,1), seasonal = PDQ(0,1,0))\nfc  = forecast(arima_model, h = 12)\n\n# auto ARIMA\nauto_arima_model = auto_arima(ap, 12, d = 1, D = 1)\nfc2  = forecast(auto_arima_model, h = 12)\nplot(fc2)\n\n","category":"section"},{"location":"#ARAR","page":"Home","title":"ARAR","text":"using Durbyn\nusing Durbyn.Ararma\n\nap = air_passengers()\n\narar_model = arar(ap, max_ar_depth = 13)\nfc = forecast(arar_model, h = 12)\nplot(fc)\n\n","category":"section"},{"location":"#ARARMA","page":"Home","title":"ARARMA","text":"using Durbyn\nusing Durbyn.Ararma\n\nap = air_passengers()\n\nararma_model = ararma(ap, p = 0, q = 1)\nfc = forecast(ararma_model, h = 12)\nplot(fc)\n\nauto_ararma_model = auto_ararma(ap)\nfc2 = forecast(auto_ararma_model, h = 12)\nplot(fc2)\n\n","category":"section"},{"location":"#BATS-/-TBATS","page":"Home","title":"BATS / TBATS","text":"BATS (Box-Cox, ARMA errors, Trend, Seasonal) and TBATS (Trigonometric BATS) handle complex seasonal patterns with multiple seasonal periods.","category":"section"},{"location":"#Formula-Interface-(Recommended)-2","page":"Home","title":"Formula Interface (Recommended)","text":"using Durbyn\nusing Durbyn.ModelSpecs\n\n# BATS with monthly seasonality\ndata = (sales = randn(120) .+ 10,)\nspec = BatsSpec(@formula(sales = bats(seasonal_periods=12)))\nfitted = fit(spec, data)\nfc = forecast(fitted, h = 12)\nplot(fc)\n# BATS with multiple seasonal periods (hourly data with daily and weekly patterns)\nspec_multi = BatsSpec(@formula(sales = bats(seasonal_periods=[24, 168])))\nfitted_multi = fit(spec_multi, data)\nfc_multi = forecast(fitted_multi, h = 24)\n\n# BATS with specific components\nspec_custom = BatsSpec(@formula(sales = bats(\n    seasonal_periods=12,\n    use_box_cox=true,\n    use_trend=true,\n    use_damped_trend=true,\n    use_arma_errors=true\n)))\nfitted_custom = fit(spec_custom, data)\nfc_custom = forecast(fitted_custom, h = 12)","category":"section"},{"location":"#Base-API","page":"Home","title":"Base API","text":"using Durbyn.Bats\n\ny = randn(120)\n\n# BATS with automatic component selection\nbats_model = bats(y, 12)\nfc = forecast(bats_model, h = 12)\nplot(fc)\n\n# BATS with multiple seasonal periods\nbats_multi = bats(y, [24, 168]; use_box_cox=true, use_arma_errors=true)\nfc_multi = forecast(bats_multi, h = 24)\n\n# TBATS for non-integer seasonality\ntbats_model = tbats(y, [12.5, 52.18])  # Non-integer periods\nfc_tbats = forecast(tbats_model, h = 12)\n\n","category":"section"},{"location":"#License","page":"Home","title":"License","text":"Apache License 2.0.\n\n","category":"section"},{"location":"#What's-next","page":"Home","title":"What's next","text":"Grammar Guide (Recommended) — Learn the complete formula interface for ARIMA, BATS, and ETS\nQuick Start — Get started quickly with formula and base models\nUser Guide pages:\nTable Operations — Data wrangling with Tables.jl for panel data\nARIMA — Formula interface and base models (ARIMA, SARIMA, Auto ARIMA)\nExponential Smoothing — Formula interface and base models (SES, Holt, Holt-Winters, ETS)\nBATS — Box-Cox, ARMA errors, Trend, Seasonal models for complex seasonality\nTBATS — Trigonometric BATS for non-integer and very long seasonal periods\nIntermittent Demand — Croston methods\nARAR — Memory-shortening AR model (Brockwell & Davis)\nARARMA — Memory-shortening with ARMA fitting (Parzen)\nStatistics — Box-Cox transformations, decomposition (STL, MSTL), unit root tests, ACF/PACF\nOptimization — Nelder-Mead, BFGS, L-BFGS-B, and Brent optimization algorithms\nUtilities — Example datasets, data manipulation helpers, and other utilities\nAPI Reference — Complete API documentation","category":"section"}]
}
