var documenterSearchIndex = {"docs":
[{"location":"ararma/#ARAR-and-ARARMA-Models","page":"ARAR/ARARMA","title":"ARAR and ARARMA Models","text":"","category":"section"},{"location":"ararma/#ARAR-Model","page":"ARAR/ARARMA","title":"ARAR Model","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"The ARAR model applies a memory-shortening transformation; if the underlying process of a time series Y_t t=12ldotsn is “long-memory”, it then fits an autoregressive model.","category":"page"},{"location":"ararma/#Memory-Shortening","page":"ARAR/ARARMA","title":"Memory Shortening","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"The model follows five steps to classify Y_t and take one of three actions:","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"L: declare Y_t as long memory and form tilde Y_t = Y_t - hatphi Y_t-hattau\nM: declare Y_t as moderately long memory and form tilde Y_t = Y_t - hatphi_1 Y_t-1 - hatphi_2 Y_t-2\nS: declare Y_t as short memory.","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"If Y_t is declared L or M, the series is transformed again until the transformed series is classified as short memory. (At most three transformations are applied; in practice, more than two is rare.)","category":"page"},{"location":"ararma/#Steps","page":"ARAR/ARARMA","title":"Steps","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"For each tau=12ldots15, find hatphi(tau) that minimizes  \nmathrmERR(phitau) = \nfracdisplaystylesum_t=tau+1^nbig(Y_t-phiY_t-taubig)^2\n     displaystylesum_t=tau+1^nY_t^2\nthen set mathrmErr(tau)=mathrmERRbig(hatphi(tau)taubig) and choose   hattau=argmin_taumathrmErr(tau).\nIf mathrmErr(hattau)le 8n, then Y_t is a long-memory series.\nIf hatphi(hattau)ge 093 and hattau2, then Y_t is a long-memory series.\nIf hatphi(hattau)ge 093 and hattauin12, then Y_t is a long-memory series.\nIf hatphi(hattau)093, then Y_t is a short-memory series.","category":"page"},{"location":"ararma/#Subset-Autoregressive-Model","page":"ARAR/ARARMA","title":"Subset Autoregressive Model","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"We now describe how ARAR fits an autoregression to the mean-corrected series   X_t=S_t-bar S, t=k+1ldotsn, where S_t is the memory-shortened version of Y_t obtained above and bar S is the sample mean of S_k+1ldotsS_n.","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"The fitted model has the form","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"X_t = phi_1 X_t-1 + phi_l_1 X_t-l_1 + phi_l_2 X_t-l_2 + phi_l_3 X_t-l_3 + Z_t\nqquad Z_t sim mathrmWN(0sigma^2)","category":"page"},{"location":"ararma/#Yule–Walker-Equations","page":"ARAR/ARARMA","title":"Yule–Walker Equations","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"The coefficients phi_j and the noise variance sigma^2 follow from the Yule–Walker equations for given lags l_1l_2l_3:","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"beginbmatrix\n1  hatrho(l_1-1)  hatrho(l_2-1)  hatrho(l_3-1)\nhatrho(l_1-1)  1  hatrho(l_2-l_1)  hatrho(l_3-l_1)\nhatrho(l_2-1)  hatrho(l_2-l_1)  1  hatrho(l_3-l_2)\nhatrho(l_3-1)  hatrho(l_3-l_1)  hatrho(l_3-l_2)  1\nendbmatrix\nbeginbmatrix\nphi_12pt\nphi_l_12pt\nphi_l_22pt\nphi_l_3\nendbmatrix\n=\nbeginbmatrix\nhatrho(1)2pt\nhatrho(l_1)2pt\nhatrho(l_2)2pt\nhatrho(l_3)\nendbmatrix","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"sigma^2 = hatgamma(0)Big( 1 - phi_1hatrho(1) - phi_l_1hatrho(l_1) - phi_l_2hatrho(l_2) - phi_l_3hatrho(l_3) Big)","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"where hatgamma(j) and hatrho(j), j=012ldots, are the sample autocovariances and autocorrelations of X_t.   The algorithm computes phi(cdot) for each set of lags with 1l_1l_2l_3le m (m typically 13 or 26) and selects the model with minimal Yule–Walker estimate of sigma^2.","category":"page"},{"location":"ararma/#Forecasting","page":"ARAR/ARARMA","title":"Forecasting","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"If the short-memory filter found in the first step has coefficients Psi_0Psi_1ldotsPsi_k (kge0, Psi_0=1), then","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"S_t = Psi(B)Y_t = Y_t + Psi_1 Y_t-1 + cdots + Psi_k Y_t-k\nqquad\nPsi(B) = 1 + Psi_1 B + cdots + Psi_k B^k ","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"If the subset AR coefficients are phi_1phi_l_1phi_l_2phi_l_3 then, for X_t=S_t-bar S, ","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"phi(B)X_t = Z_t qquad \nphi(B) = 1 - phi_1 B - phi_l_1 B^l_1 - phi_l_2 B^l_2 - phi_l_3 B^l_3","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"From the two displays above,","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"xi(B)Y_t = phi(1)bar S + Z_t \nqquad xi(B) = Psi(B)phi(B)","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Assuming this model is appropriate and Z_t is uncorrelated with Y_j for jt, the minimum-MSE linear predictors P_n Y_n+h of Y_n+h (for nk+l_3) satisfy the recursion","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"P_n Y_n+h = - sum_j=1^k+l_3 xi_j  P_n Y_n+h-j + phi(1)bar S qquad hge 1","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"with initial conditions P_n Y_n+h=Y_n+h for hle 0.","category":"page"},{"location":"ararma/#Reference","page":"ARAR/ARARMA","title":"Reference","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Brockwell, Peter J., and Richard A. Davis. Introduction to Time Series and Forecasting. Springer (2016)","category":"page"},{"location":"ararma/#Forecasing-in-Julia-using-Arar-Model","page":"ARAR/ARARMA","title":"Forecasing in Julia using Arar Model","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"using Durbyn\nusing Durbyn.Ararma\n\nap = air_passengers()\n\nfit = arar(ap, max_ar_depth = 13)\nfc  = forecast(fit, h = 12)\nplot(fc)\n","category":"page"},{"location":"ararma/#ARARMA-Model","page":"ARAR/ARARMA","title":"ARARMA Model","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"ARARMA extends the ARAR approach by first applying an adaptive AR prefilter to shorten memory (the ARAR stage), and then fitting a short-memory ARMA(p, q) model on the prefiltered residuals. The goal is to capture long/persistent structure via a composed AR filter Psi(B) and the remaining short-term dynamics via an ARMA kernel.","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Given a univariate series Y_t t=12ldotsn, ARARMA produces a fitted model and forecasting mechanism that combine both stages.","category":"page"},{"location":"ararma/#Stage-1-—-Memory-Shortening-(ARAR)","page":"ARAR/ARARMA","title":"Stage 1 — Memory Shortening (ARAR)","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"As in ARAR, we iteratively test for long memory and, if detected, apply a memory-shortening AR filter. At iteration r, the procedure evaluates delays tau=1ldots15 by ordinary least squares and scores each delay by a relative error measure:","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"mathrmERR(phitau)\n=\nfracdisplaystylesum_t=tau+1^nbig(Y_t-phiY_t-taubig)^2\n     displaystylesum_t=tau+1^nY_t^2\nqquad\nhatphi(tau)inargmin_phi mathrmERR(phitau)","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Let mathrmErr(tau) = mathrmERRbig(hatphi(tau)taubig) and hattau=argmin_tau mathrmErr(tau). Then:","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"If mathrmErr(hattau)le 8n or if hatphi(hattau)ge 093 with hattau2, declare long memory and filter:\ntilde Y_t = Y_t - hatphiY_t-hattau\nIf hatphi(hattau)ge 093 with hattauin12, fit an AR(2) at lags 1 and 2 by normal equations and filter:\ntilde Y_t = Y_t - hatphi_1 Y_t-1 - hatphi_2 Y_t-2\nOtherwise, stop.","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Each successful reduction composes the prefilter polynomial Psi(B) (with Psi_0=1):","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"S_t = Psi(B)Y_t = Y_t + Psi_1 Y_t-1 + cdots + Psi_k Y_t-k\nqquad\nPsi(B) = 1 + Psi_1 B + cdots + Psi_k B^k","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"The reduction loop terminates when short memory is reached or after three passes (rarely more than two are needed in practice).","category":"page"},{"location":"ararma/#Stage-2-—-Best-Lag-Subset-AR-(on-the-prefiltered-series)","page":"ARAR/ARARMA","title":"Stage 2 — Best-Lag Subset AR (on the prefiltered series)","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Let X_t = S_t - bar S with bar S the sample mean of the final prefiltered series. Over 4-term lag tuples (1ijk) satisfying 1ijkle m (with m typically 13 or 26), we fit the subset AR:","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"X_t = phi_1 X_t-1 + phi_i X_t-i + phi_j X_t-j + phi_k X_t-k + Z_t\nqquad Z_tsim mathrmWN(0sigma^2)","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Yule–Walker equations (using sample autocorrelations hatrho(cdot) of X_t) yield the coefficients:","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"beginbmatrix\n1  hatrho(i-1)  hatrho(j-1)  hatrho(k-1)\nhatrho(i-1)  1  hatrho(j-i)  hatrho(k-i)\nhatrho(j-1)  hatrho(j-i)  1  hatrho(k-j)\nhatrho(k-1)  hatrho(k-i)  hatrho(k-j)  1\nendbmatrix\n\nbeginbmatrix\nphi_12ptphi_i2ptphi_j2ptphi_k\nendbmatrix\n=\nbeginbmatrix\nhatrho(1)2pthatrho(i)2pthatrho(j)2pthatrho(k)\nendbmatrix","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"The implied variance is","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"sigma^2 = hatgamma(0)Big(1 - phi_1 hatrho(1) - phi_i hatrho(i) - phi_j hatrho(j) - phi_k hatrho(k)Big)","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"where hatgamma(cdot) are sample autocovariances of X_t. The algorithm selects (1ijk) minimizing this sigma^2.","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Define the composite AR kernel by convolving the prefilter with the selected subset AR:","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"phi(B) = 1 - phi_1 B - phi_i B^i - phi_j B^j - phi_k B^k\nqquad\nxi(B) = Psi(B)phi(B)","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Let c = big(1-phi_1-phi_i-phi_j-phi_kbig)bar S be the AR intercept.","category":"page"},{"location":"ararma/#Stage-3-—-Short-Memory-ARMA(p,-q)-on-AR-Residuals","page":"ARAR/ARARMA","title":"Stage 3 — Short-Memory ARMA(p, q) on AR Residuals","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Using the AR-only fit implied by xi(B) and c, compute residuals and fit an ARMA(p, q) by maximizing the conditional Gaussian likelihood. Denote the ARMA polynomials","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Phi(B) = 1 - varphi_1 B - cdots - varphi_p B^p\nqquad\nTheta(B) = 1 + theta_1 B + cdots + theta_q B^q","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"The ARMA stage estimates (varphi_1ldotsvarphi_ptheta_1ldotstheta_qsigma^2) via Nelder–Mead. The code log-parameterizes the variance for numerical stability.","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Information criteria. With effective residual length n_texteff and k=p+q parameters (variance excluded), the log-likelihood ell yields","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"mathrmAIC=2k-2ell qquad mathrmBIC=(log n_texteff)k-2ell","category":"page"},{"location":"ararma/#Forecasting-2","page":"ARAR/ARARMA","title":"Forecasting","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"With the composite kernel xi(B) and intercept c from Stage 2, and the ARMA(p,q) layer from Stage 3, h-step-ahead forecasts are formed recursively. Let P_n Y_n+h denote the minimum-MSE predictor of Y_n+h. Writing xi(B)=1+xi_1 B+cdots+xi_K B^K,","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"P_n Y_n+h\n=\n- sum_j=1^K xi_j  P_n Y_n+h-j\n+ c + text(MA terms from  Theta(B)text)\nqquad hge 1","category":"page"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"with initialization P_n Y_n+h=Y_n+h for hle 0 and future shocks set to zero for the MA recursion. Forecast standard errors follow from the MA representation and the estimated innovation variance sigma^2.","category":"page"},{"location":"ararma/#References","page":"ARAR/ARARMA","title":"References","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"Parzen, E. (1985). ARARMA Models for Time Series Analysis and Forecasting. Journal of Forecasting, 1(1), 67–82.","category":"page"},{"location":"ararma/#Forecasing-in-Julia-using-Ararma-Model","page":"ARAR/ARARMA","title":"Forecasing in Julia using Ararma Model","text":"","category":"section"},{"location":"ararma/","page":"ARAR/ARARMA","title":"ARAR/ARARMA","text":"using Durbyn\nusing Durbyn.Ararma\n\nap = air_passengers()\n\nfit1 = ararma(ap, p = 0, q = 1)\nfc1  = forecast(fit1, h = 12)\nplot(fc1)\n\nfit2 = auto_ararma(ap)\nfc2  = forecast(fit2, h = 12)\nplot(fc2)","category":"page"},{"location":"expsmoothing/#Exponential-Smoothing-(ETS):-State-Space-Form,-Additive-and-Multiplicative-Models","page":"Exponential Smoothing","title":"Exponential Smoothing (ETS): State-Space Form, Additive & Multiplicative Models","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"This page summarizes the ETS state-space framework wich is implemented in Durbyn.jl as ets() for automatic forecasting, and the admissible parameter regions for stability/forecastability.   It includes both additive and multiplicative error models, following Hyndman et al. (2002, 2008).","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"page"},{"location":"expsmoothing/#Model-taxonomy-and-notation","page":"Exponential Smoothing","title":"Model taxonomy and notation","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"ETS models are categorized by (Error, Trend, Seasonality):","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"ANN / MNN — simple exponential smoothing (additive vs multiplicative error)  \nAAN / MAN — additive trend (Holt, with additive vs multiplicative error)  \nADN — damped additive trend (only additive error common in practice)  \nAAA / MAM — additive trend + additive/multiplicative seasonality  \nANA / AAA / ADA — seasonal additive-error forms (with no / additive / damped trend)  \nOther hybrids (e.g. multiplicative seasonality with additive error, damped multiplicative trend) can be defined analogously.","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"We use smoothing parameters alphabetagamma and damping phi (if present).   Additive vs multiplicative error models give the same point forecasts but different likelihoods and intervals.","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"page"},{"location":"expsmoothing/#Additive-error-state-space-form","page":"Exponential Smoothing","title":"Additive error state-space form","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"beginaligned\ntextbfObservationquad\n Y_t = Hx_t-1 + varepsilon_t \ntextbfStatequad\n x_t = Fx_t-1 + Gvarepsilon_t qquad varepsilon_t sim WN(0sigma^2)\nendaligned","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Forecast mean and variance at horizon h:","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"mu_n(h) = H F^h-1 x_n qquad\nv_n(h) = sigma^2left(1 + sum_j=1^h-1 (HF^j-1G)^2right)","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"page"},{"location":"expsmoothing/#Multiplicative-error-form","page":"Exponential Smoothing","title":"Multiplicative error form","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"For multiplicative error models:","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Observation:  \nY_t = hatY_t (1+varepsilon_t)\nwhere varepsilon_t sim WN(0sigma^2).\nKey property: Point forecasts are the same as additive-error models, but prediction intervals scale with the level.","category":"page"},{"location":"expsmoothing/#Examples","page":"Exponential Smoothing","title":"Examples","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"MNN (no trend, no seasonality):\nY_t = ell_t-1(1+varepsilon_t) qquad\nell_t = ell_t-1(1+alphavarepsilon_t)\nMAN (additive trend):\nY_t = (ell_t-1+b_t-1)(1+varepsilon_t) \nell_t = (ell_t-1+b_t-1)(1+alphavarepsilon_t) \nb_t = b_t-1 + beta(ell_t-1+b_t-1)varepsilon_t\nMAM (additive trend + multiplicative seasonality):\nY_t = (ell_t-1+b_t-1)s_t-m(1+varepsilon_t) \nell_t = (ell_t-1+b_t-1)(1+alphavarepsilon_t) \nb_t = b_t-1+beta(ell_t-1+b_t-1)varepsilon_t \ns_t = s_t-m(1+gammavarepsilon_t)","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Other multiplicative combinations (e.g. damped trend, hybrid seasonality) follow analogously.","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"page"},{"location":"expsmoothing/#Model-properties","page":"Exponential Smoothing","title":"Model properties","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Let M = F-GH.","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Observability: operatornamerank(H^top(F^top)H^topdots(F^top)^p-1H^top)=p  \nReachability: operatornamerank(GFGdotsF^p-1G)=p  \nStability: eigenvalues of M lie inside the unit circle  \nForecastability: weaker notion, unstable modes do not affect forecasts if orthogonal to forecast functional","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Non-seasonal additive/multiplicative ETS are minimal (reachable & observable).   Standard seasonal ETS are not (contain redundant seasonal states).","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"page"},{"location":"expsmoothing/#Admissible-regions-(non-seasonal,-additive-and-multiplicative)","page":"Exponential Smoothing","title":"Admissible regions (non-seasonal, additive & multiplicative)","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"For ANN/AAN/ADN (and their multiplicative analogues), the admissible stability regions are identical:","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"ANN / MNN\n0  alpha  2\nAAN / MAN\n0  alpha  2 qquad 0  beta  4-2alpha\nADN (damped additive trend)\n0  phi le 1 qquad\n1-tfrac1phi  alpha  1+tfrac1phi qquad\nalpha(phi-1)  beta  (1+phi)(2-alpha)","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Thus, admissible regions do not depend on whether errors are additive or multiplicative.","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"page"},{"location":"expsmoothing/#Seasonal-ETS","page":"Exponential Smoothing","title":"Seasonal ETS","text":"","category":"section"},{"location":"expsmoothing/#Standard-Holt–Winters-seasonal-form","page":"Exponential Smoothing","title":"Standard Holt–Winters seasonal form","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"In ANA/AAA/ADA with recursion s_t=s_t-m+gammavarepsilon_t,   M has a unit eigenvalue → unstable, non-minimal.   Forecasts can remain valid (forecastable) but states are corrupted.","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Characteristic polynomial factorization (ADA case):","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"f(lambda) = (1-lambda)P(lambda)","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"with forecastability polynomial P(lambda) whose roots must lie inside the unit circle.   AAA is the special case phi=1.","category":"page"},{"location":"expsmoothing/#Normalized-seasonal-ETS","page":"Exponential Smoothing","title":"Normalized seasonal ETS","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Fix instability by imposing a sum-to-zero seasonal constraint each period:","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"S(B)s_t = theta(B)gammavarepsilon_t","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"where S(B)=1+B+cdots+B^m-1,   theta(B)=tfrac1m(m-1)+(m-2)B+cdots+B^m-2.","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Operationally: after updating seasonals, subtract the average of last m shocks.   This normalization restores stability.","category":"page"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"page"},{"location":"expsmoothing/#References","page":"Exponential Smoothing","title":"References","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Hyndman, Koehler, Snyder & Grose (2002). A state space framework for automatic forecasting using exponential smoothing methods.  \nHyndman, Akram & Archibald (2006). The admissible parameter space for exponential smoothing models.","category":"page"},{"location":"expsmoothing/#Forecast-with-Automatic-ETS-model","page":"Exponential Smoothing","title":"Forecast with Automatic ETS model","text":"","category":"section"},{"location":"expsmoothing/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"using Durbyn\nusing Durbyn.ExponentialSmoothing\n# Fit automatically selected ETS model to a monthly series (m = 12)\nap = air_passengers()\nfit = ets(ap(), 12, \"ZZZ\")\n\n# Specify a particular structure (multiplicative seasonality, additive trend, additive errors)\nfit2 = ets(ap, 12, \"AAM\")\nfc2 = forecast(fit2, h=12)\nplot(fc2)\n\n# Use a damped trend search and automatic Box–Cox selection\nfit3 = ets(ap, 12, \"ZZZ\"; damped=nothing, lambda=\"auto\", biasadj=true)\nfc3 = forecast(fit3, h=12)\nplot(fc3)","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"quickstart/#Quick-Start","page":"Quick Start","title":"Quick Start","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Install (dev version):","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"using Pkg\nPkg.add(url=\"https://github.com/taf-society/Durbyn.jl\")","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Basic forecasting with Exponential Smoothing (ETS):","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"using Durbyn\nusing Durbyn.ExponentialSmoothing\n\nap = air_passengers()\nfit_ets = ets(ap, 12, \"ZZZ\")\nfc_ets  = forecast(fit_ets, h = 12)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Plot (example with Plots.jl):","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"using Plots\nplot(fc_ets)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Next: Explore Intermittent Demand, ARIMA, and ARAR/ARARMA.","category":"page"},{"location":"arima/#Forecasting-Using-ARIMA,-SARIMA,-ARIMAX,-SARIMAX,-and-Auto-ARIMA","page":"ARIMA","title":"Forecasting Using ARIMA, SARIMA, ARIMAX, SARIMAX, and Auto ARIMA","text":"","category":"section"},{"location":"arima/#1.-ARIMA-(AutoRegressive-Integrated-Moving-Average)","page":"ARIMA","title":"1. ARIMA (AutoRegressive Integrated Moving Average)","text":"","category":"section"},{"location":"arima/#Definition","page":"ARIMA","title":"Definition","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"An ARIMA model is denoted as ARIMA(p, d, q), where:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"p: order of the autoregressive (AR) part\nd: degree of differencing needed to achieve stationarity\nq: order of the moving average (MA) part","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Formally, the model is written as:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Phi(B) Delta^d X_t = Theta(B) varepsilon_t","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"where:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"B is the backshift operator (BX_t = X_t-1),\nPhi(B) = 1 - phi_1B - cdots - phi_pB^p,\nTheta(B) = 1 + theta_1B + cdots + theta_qB^q,\nDelta^d = (1 - B)^d is the differencing operator,\nvarepsilon_t is white noise.","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"If d = 0, the model reduces to ARMA(p, q).","category":"page"},{"location":"arima/#Key-Features","page":"ARIMA","title":"Key Features","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Handles non-stationary time series via differencing.\nShocks (innovations) have permanent effects for d  0.\nCommonly used for macroeconomic and financial data.","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"","category":"page"},{"location":"arima/#2.-SARIMA-(Seasonal-ARIMA)","page":"ARIMA","title":"2. SARIMA (Seasonal ARIMA)","text":"","category":"section"},{"location":"arima/#Definition-2","page":"ARIMA","title":"Definition","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Seasonal ARIMA extends ARIMA to account for seasonality. It is denoted as:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"ARIMA(p d q)(P D Q)_m","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"where:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"P D Q are the seasonal AR, differencing, and MA orders,\nm is the seasonal period (e.g., 12 for monthly data with yearly seasonality).","category":"page"},{"location":"arima/#Model-Form","page":"ARIMA","title":"Model Form","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Phi(B)Phi_s(B^m) Delta^d Delta_m^D X_t = Theta(B)Theta_s(B^m)varepsilon_t","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"where:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Phi_s(B^m) and Theta_s(B^m) capture seasonal AR and MA terms,\nDelta_m^D = (1 - B^m)^D applies seasonal differencing.","category":"page"},{"location":"arima/#Key-Features-2","page":"ARIMA","title":"Key Features","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Captures both short-term dynamics (p, d, q) and seasonal effects (P, D, Q).\nWidely applied to monthly or quarterly economic indicators, sales, or climate data.","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"","category":"page"},{"location":"arima/#3.-ARIMAX-(ARIMA-with-Exogenous-Variables)","page":"ARIMA","title":"3. ARIMAX (ARIMA with Exogenous Variables)","text":"","category":"section"},{"location":"arima/#Definition-3","page":"ARIMA","title":"Definition","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"An ARIMAX model incorporates external regressors (covariates) into the ARIMA framework:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Phi(B) Delta^d X_t = beta Z_t + Theta(B) varepsilon_t","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"where:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Z_t is a vector of exogenous predictors,\nbeta are their coefficients.","category":"page"},{"location":"arima/#Key-Features-3","page":"ARIMA","title":"Key Features","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Useful when external factors (e.g., interest rates, marketing spend, policy variables) explain additional variance beyond past values of the series.\nRequires careful checking of exogeneity assumptions.","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"","category":"page"},{"location":"arima/#4.-SARIMAX-(Seasonal-ARIMAX)","page":"ARIMA","title":"4. SARIMAX (Seasonal ARIMAX)","text":"","category":"section"},{"location":"arima/#Definition-4","page":"ARIMA","title":"Definition","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"SARIMAX generalizes SARIMA by including exogenous regressors:","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Phi(B)Phi_s(B^m) Delta^d Delta_m^D X_t = beta Z_t + Theta(B)Theta_s(B^m)varepsilon_t","category":"page"},{"location":"arima/#Key-Features-4","page":"ARIMA","title":"Key Features","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Combines seasonality and exogenous influences.\nPowerful for real-world applications such as:\nForecasting retail sales with promotions (exogenous variable) and seasonal cycles.\nModeling energy demand with weather as an exogenous driver.","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"","category":"page"},{"location":"arima/#5.-Auto-ARIMA","page":"ARIMA","title":"5. Auto ARIMA","text":"","category":"section"},{"location":"arima/#Definition-5","page":"ARIMA","title":"Definition","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Auto ARIMA automates the process of identifying the best ARIMA/SARIMA model by searching across possible values of (p, d, q) and seasonal (P, D, Q), selecting the model that minimizes an information criterion such as AIC, AICc, or BIC.","category":"page"},{"location":"arima/#Algorithm-(Hyndman-and-Khandakar,-2008)","page":"ARIMA","title":"Algorithm (Hyndman & Khandakar, 2008)","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Unit root tests (ADF, KPSS, or combinations) to determine differencing orders ( d ) and ( D ).\nInitial model selection based on heuristics.  \nStepwise search over (p, q, P, Q) with bounds (e.g., up to 5 for non-seasonal and 2 for seasonal).  \nEvaluate models by likelihood and information criteria.  \nRefit the best model with full maximum likelihood.  ","category":"page"},{"location":"arima/#Advantages","page":"ARIMA","title":"Advantages","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Removes the manual effort of model identification.  \nScales well to large numbers of series.  \nEnsures differencing is tested systematically (avoids over-differencing).","category":"page"},{"location":"arima/#Limitations","page":"ARIMA","title":"Limitations","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Stepwise search may not find the global optimum.  \nComputationally expensive for very large seasonal periods.  \nStill requires diagnostic checking of residuals.  ","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"","category":"page"},{"location":"arima/#6.-Model-Selection-and-Diagnostics","page":"ARIMA","title":"6. Model Selection & Diagnostics","text":"","category":"section"},{"location":"arima/#Identification","page":"ARIMA","title":"Identification","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Use ACF/PACF plots and unit root tests (ADF, PP, KPSS) to choose orders manually (or confirm Auto ARIMA results).\nDifferencing ensures stationarity (d D).","category":"page"},{"location":"arima/#Estimation","page":"ARIMA","title":"Estimation","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Maximum Likelihood Estimation (MLE) or Conditional Sum of Squares.","category":"page"},{"location":"arima/#Diagnostics","page":"ARIMA","title":"Diagnostics","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Residual analysis: check for white noise.\nInformation criteria: AIC, BIC, AICc.  \nOut-of-sample forecast validation.","category":"page"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"","category":"page"},{"location":"arima/#Forecasing-in-Julia-Using-Seasonal-Arima-Model","page":"ARIMA","title":"Forecasing in Julia Using Seasonal Arima Model","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"using Durbyn\nusing Durbyn.Arima\n\nap  = air_passengers()\nfit = arima(ap, 12, order = PDQ(2,1,1), seasonal = PDQ(0,1,0))\nfc  = forecast(fit, h = 12)\nplot(fc)\n","category":"page"},{"location":"arima/#Forecasing-in-Julia-Using-Auto-Arima-Model","page":"ARIMA","title":"Forecasing in Julia Using Auto-Arima Model","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"fit2 = auto_arima(ap, 12)\nfc2  = forecast(fit2, h = 12)\nplot(fc)","category":"page"},{"location":"arima/#References","page":"ARIMA","title":"References","text":"","category":"section"},{"location":"arima/","page":"ARIMA","title":"ARIMA","text":"Kunst, R. (2011). Applied Time Series Analysis — Part II. University of Vienna.  \nHyndman, R.J., & Khandakar, Y. (2008). Automatic Time Series Forecasting: The forecast Package for R. Journal of Statistical Software, 27(3).  \nBox, G.E.P., Jenkins, G.M., & Reinsel, G.C. (1994). Time Series Analysis, Forecasting and Control.  \nHamilton, J.D. (1994). Time Series Analysis.  ","category":"page"},{"location":"#Durbyn.jl","page":"Home","title":"Durbyn.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Durbyn.jl logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Durbyn is a Julia package that implements functionality of the R forecast package, providing tools for time-series forecasting.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The name Durbyn traces back to Kurdish, where Dur means “far” and Byn means “to see,” together signifying binoculars, which is why the package logo features them.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This site documents the development version. After your first tagged release, see stable docs for the latest release.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#About-TAFS","page":"Home","title":"About TAFS","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TAFS (Time Series Analysis and Forecasting Society) is a non-profit association (“Verein”) in Vienna, Austria. It connects academics, experts, practitioners, and students focused on time-series, forecasting, and decision science. Contributions remain fully open source.   Learn more at taf-society.org.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Durbyn is under active development. For the latest dev version:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/taf-society/Durbyn.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quick-peek-(ETS)","page":"Home","title":"Quick peek (ETS)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Durbyn\nusing Durbyn.ExponentialSmoothing\n\nap = air_passengers()\n\nfit_ets = ets(ap, 12, \"ZZZ\")\nfc_ets  = forecast(fit_ets, h = 12)\nplot(fc_ets)\n\nses_fit = ses(ap, 12)\nses_fc  = forecast(ses_fit, h = 12)\nplot(ses_fc)\n\nholt_fit = holt(ap, 12)\nholt_fc  = forecast(holt_fit, h = 12)\nplot(holt_fc)\n\nhw_fit = holt_winters(ap, 12)\nhw_fc  = forecast(hw_fit, h = 12)\nplot(hw_fc)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Intermittent-demand-(Croston-variants)","page":"Home","title":"Intermittent demand (Croston variants)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"data = [6, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0,\n0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, \n0, 0, 0, 0, 0];\n\n# Based on Shenstone & Hyndman (2005)\nm = 1\nfit_crst = croston(data, m)\nfc_crst  = forecast(fit_crst, 12)\nplot(fc_crst)\n\nusing Durbyn.IntermittentDemand\n\n# Classical Croston (Croston, 1972)\ncrst1 = croston_classic(data)\nfc1   = forecast(crst1, h = 12)\n\nresiduals(crst1); residuals(fc1);\nfitted(crst1);    fitted(fc1);\nplot(fc1, show_fitted = true)\n\n# Croston + SBA correction\ncrst2 = croston_sba(data)\nfc2   = forecast(crst2, h = 12)\nplot(fc2, show_fitted = true)\n\n# Croston + SBJ correction\ncrst3 = croston_sbj(data)\nfc3   = forecast(crst3, h = 12)\nplot(fc3, show_fitted = true)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#ARIMA","page":"Home","title":"ARIMA","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Durbyn.Arima\n\nap  = air_passengers()\n\n# manual ARIMA\nfit = arima(ap, 12, order = PDQ(2,1,1), seasonal = PDQ(0,1,0))\nfc  = forecast(fit, h = 12)\n\n# auto ARIMA\nfit2 = auto_arima(ap, 12, d = 1, D = 1)\nfc2  = forecast(fit2, h = 12)\nplot(fc2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#ARAR-/-ARARMA","page":"Home","title":"ARAR / ARARMA","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Durbyn\nusing Durbyn.Ararma\n\nap = air_passengers()\n\nfit  = arar(ap, max_ar_depth = 13)\nfc   = forecast(fit, h = 12)\nplot(fc)\n\nfit2 = ararma(ap, p = 0, q = 1)\nfc2  = forecast(fit2, h = 12)\nplot(fc2)\n\nfit3 = auto_ararma(ap)\nfc3  = forecast(fit3, h = 12)\nplot(fc3)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MIT License.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#What’s-next","page":"Home","title":"What’s next","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Read the Quick Start (left sidebar).\nExplore User Guide pages (ETS, Intermittent Demand, ARIMA, ARAR/ARARMA).\nSee the API Reference for full docs.","category":"page"},{"location":"intermittent/#Intermittent-Demand","page":"Intermittent Demand","title":"Intermittent Demand","text":"","category":"section"},{"location":"intermittent/","page":"Intermittent Demand","title":"Intermittent Demand","text":"Tools for sparse series with many zeros, including Croston variants.","category":"page"},{"location":"intermittent/","page":"Intermittent Demand","title":"Intermittent Demand","text":"using Durbyn\nusing Durbyn.IntermittentDemand\n\ndata = [6, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0,\n0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, \n0, 0, 0, 0, 0];\n\n# Classical Croston method\ncrst = croston_classic(data)\nfc   = forecast(crst, h = 12)\nplot(fc, show_fitted = true)\n\n# SBA bias-corrected\ncrst2 = croston_sba(data)\nfc2   = forecast(crst2, h = 12)\n\n# SBJ correction\ncrst3 = croston_sbj(data)\nfc3   = forecast(crst3, h = 12)","category":"page"},{"location":"intermittent/","page":"Intermittent Demand","title":"Intermittent Demand","text":"Inspection","category":"page"},{"location":"intermittent/","page":"Intermittent Demand","title":"Intermittent Demand","text":"residuals(crst); residuals(fc);\nfitted(crst);    fitted(fc);","category":"page"}]
}
